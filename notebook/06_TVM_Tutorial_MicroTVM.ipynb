{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "06_TVM_Tutorial_MicroTVM.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/uwsampl/tutorial/blob/master/notebook/06_TVM_Tutorial_MicroTVM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZ_LgheWhovH",
        "colab_type": "text"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/uwsampl/tutorial/blob/master/notebook/06_TVM_Tutorial_MicroTVM.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xFoF4FhyYcwE",
        "colab_type": "text"
      },
      "source": [
        "Please run the following block to ensure TVM is setup for *this notebook*.  Each notebook may have its own runtime."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ce5eshWvUiyN",
        "colab_type": "code",
        "outputId": "92084372-901a-458d-bb20-23e58e355328",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1567
        }
      },
      "source": [
        "try:\n",
        "  import google.colab\n",
        "  IN_COLAB = True\n",
        "except:\n",
        "  IN_COLAB = False\n",
        "\n",
        "if IN_COLAB:\n",
        "    ! gsutil cp \"gs://tvm-fcrc-binariesd5fce43e-8373-11e9-bfb6-0242ac1c0002/tvm.tar.gz\" /tmp/tvm.tar.gz\n",
        "    ! mkdir -p /tvm\n",
        "    ! tar -xf /tmp/tvm.tar.gz --strip-components=4 --directory /tvm\n",
        "    ! ls -la /tvm\n",
        "    ! bash /tvm/package.sh\n",
        "    # Add TVM to the Python path.\n",
        "    import sys\n",
        "    sys.path.append('/tvm/python')\n",
        "    sys.path.append('/tvm/topi/python')\n",
        "    ! pip install mxnet\n",
        "else:\n",
        "    print(\"Notebook executing locally, skipping Colab setup ...\")\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Copying gs://tvm-fcrc-binariesd5fce43e-8373-11e9-bfb6-0242ac1c0002/tvm.tar.gz...\n",
            "- [1 files][114.4 MiB/114.4 MiB]                                                \n",
            "Operation completed over 1 objects/114.4 MiB.                                    \n",
            "total 164\n",
            "drwxr-xr-x 21 root root  4096 Jun 14 22:03 .\n",
            "drwxr-xr-x  1 root root  4096 Jun 14 21:59 ..\n",
            "drwx------  8 root root  4096 May 31 08:14 3rdparty\n",
            "drwx------ 12 root root  4096 Jun 13 23:39 apps\n",
            "drwx------  3 root root  4096 Jun 14 00:04 build\n",
            "drwx------  4 root root  4096 Jun 13 23:39 cmake\n",
            "-rw-------  1 root root 10778 Jun 13 23:39 CMakeLists.txt\n",
            "drwx------  6 root root  4096 Jun 13 23:39 conda\n",
            "-rw-------  1 root root  5736 Jun 13 23:39 CONTRIBUTORS.md\n",
            "drwx------  3 root root  4096 Jun 13 23:39 docker\n",
            "drwx------ 11 root root  4096 Jun 13 23:39 docs\n",
            "drwx------  4 root root  4096 Jun 13 23:39 golang\n",
            "drwx------  3 root root  4096 May 31 08:14 include\n",
            "-rw-------  1 root root 10542 Jun 13 23:39 Jenkinsfile\n",
            "drwx------  6 root root  4096 Jun 13 23:39 jvm\n",
            "-rw-------  1 root root 11357 Jun 13 23:39 LICENSE\n",
            "-rw-------  1 root root  4267 Jun 13 23:39 Makefile\n",
            "-rw-------  1 root root 10476 Jun 13 23:39 NEWS.md\n",
            "drwx------  9 root root  4096 Jun 13 23:39 nnvm\n",
            "-rw-------  1 root root    61 Jun 13 23:39 NOTICE\n",
            "-rwx------  1 root root   374 Jun 13 23:39 package.sh\n",
            "drwx------  3 root root  4096 Jun 13 23:39 python\n",
            "-rw-------  1 root root  2705 Jun 13 23:39 README.md\n",
            "drwx------  6 root root  4096 Jun 13 23:39 rust\n",
            "drwx------ 14 root root  4096 Jun 13 23:39 src\n",
            "drwx------  9 root root  4096 May 31 08:14 tests\n",
            "drwx------  7 root root  4096 Jun 13 23:39 topi\n",
            "drwx------  8 root root  4096 Jun 13 23:39 tutorials\n",
            "-rw-------  1 root root  2902 Jun 13 23:39 version.py\n",
            "drwx------ 10 root root  4096 Jun 13 23:39 vta\n",
            "drwx------  2 root root  4096 Jun 13 23:39 web\n",
            "Installing Dependencies ...\n",
            "deb https://dl.bintray.com/sbt/debian /\n",
            "Executing: /tmp/apt-key-gpghome.6D2bbtV3Ci/gpg.1.sh --keyserver hkp://keyserver.ubuntu.com:80 --recv 2EE0EA64E40A89B84B2DF73499E82A75642AC823\n",
            "gpg: key 99E82A75642AC823: \"sbt build tool <scalasbt@gmail.com>\" not changed\n",
            "gpg: Total number processed: 1\n",
            "gpg:              unchanged: 1\n",
            "Hit:1 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Hit:2 http://archive.ubuntu.com/ubuntu bionic-updates InRelease\n",
            "Hit:3 http://archive.ubuntu.com/ubuntu bionic-backports InRelease\n",
            "Ign:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:5 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Hit:6 http://security.ubuntu.com/ubuntu bionic-security InRelease\n",
            "Ign:7 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:8 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
            "Hit:9 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Hit:10 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran35/ InRelease\n",
            "Ign:11 https://dl.bintray.com/sbt/debian  InRelease\n",
            "Hit:12 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic InRelease\n",
            "Get:13 https://dl.bintray.com/sbt/debian  Release [815 B]\n",
            "Fetched 815 B in 1s (842 B/s)\n",
            "Reading package lists... Done\n",
            "W: Target Packages (Packages) is configured multiple times in /etc/apt/sources.list.d/sbt.list:1 and /etc/apt/sources.list.d/sbt.list:2\n",
            "W: Target Packages (Packages) is configured multiple times in /etc/apt/sources.list.d/sbt.list:1 and /etc/apt/sources.list.d/sbt.list:2\n",
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "libffi-dev is already the newest version (3.2.1-8).\n",
            "llvm-6.0 is already the newest version (1:6.0-1ubuntu2).\n",
            "zlib1g-dev is already the newest version (1:1.2.11.dfsg-0ubuntu2).\n",
            "clinfo is already the newest version (2.2.18.03.26-1).\n",
            "tree is already the newest version (1.7.0-5).\n",
            "libtinfo-dev is already the newest version (6.1-1ubuntu1.18.04).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-410\n",
            "Use 'sudo apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 120 not upgraded.\n",
            "W: Target Packages (Packages) is configured multiple times in /etc/apt/sources.list.d/sbt.list:1 and /etc/apt/sources.list.d/sbt.list:2\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "verilator is already the newest version (3.916-1build1).\n",
            "sbt is already the newest version (1.2.8).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-410\n",
            "Use 'sudo apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 120 not upgraded.\n",
            "W: Target Packages (Packages) is configured multiple times in /etc/apt/sources.list.d/sbt.list:1 and /etc/apt/sources.list.d/sbt.list:2\n",
            "Requirement already satisfied: mxnet in /usr/local/lib/python3.6/dist-packages (1.4.1)\n",
            "Requirement already satisfied: numpy<1.15.0,>=1.8.2 in /usr/local/lib/python3.6/dist-packages (from mxnet) (1.14.6)\n",
            "Requirement already satisfied: requests>=2.20.0 in /usr/local/lib/python3.6/dist-packages (from mxnet) (2.21.0)\n",
            "Requirement already satisfied: graphviz<0.9.0,>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from mxnet) (0.8.4)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20.0->mxnet) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20.0->mxnet) (2019.3.9)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20.0->mxnet) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20.0->mxnet) (1.24.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hHFxmiOzjFF2",
        "colab_type": "text"
      },
      "source": [
        "# MicroTVM\n",
        "\n",
        "In this notebook, we're going to cover:\n",
        "\n",
        "*   the C code generation backend\n",
        "*   the graph runtime\n",
        "*   a demo of ResNeT on the graph runtime\n",
        "*   the low-level device interface\n",
        "\n",
        "TODO: Do we even show the low-level device interface?\n",
        "\n",
        "First, we run some necessary imports."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RHHOBXwRuJGl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import tvm\n",
        "from tvm.contrib import graph_runtime, util\n",
        "from tvm import relay\n",
        "import tvm.micro as micro"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ERvCOpVjsn_U",
        "colab_type": "text"
      },
      "source": [
        "## C Codegen\n",
        "\n",
        "Let's take a look at the C codegen backend for a simple function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fAozo7Slus7I",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2315
        },
        "outputId": "efde3d31-4c9d-4dd7-a430-3d43943d4290"
      },
      "source": [
        "# First, we construct the function.\n",
        "ty = relay.TensorType(shape=(1024,), dtype=\"float32\")\n",
        "x = relay.var(\"x\", ty)\n",
        "y = relay.var(\"y\", ty)\n",
        "func = relay.Function([x, y], relay.add(x, y))\n",
        "\n",
        "# Then build it with the appropriate configuration.\n",
        "with tvm.build_config(disable_vectorize=True):\n",
        "  _, src_mod, _ = relay.build(func, target=\"c\", params={})\n",
        "\n",
        "# Now, we can see the source that was generated.\n",
        "print(src_mod.get_source())"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "#include \"tvm/runtime/c_runtime_api.h\"\n",
            "#include \"tvm/runtime/c_backend_api.h\"\n",
            "#include \"tvm/runtime/micro/utvm_device_lib.h\"\n",
            "extern void* __tvm_module_ctx = NULL;\n",
            "#ifdef __cplusplus\n",
            "extern \"C\"\n",
            "#endif\n",
            "TVM_DLL int32_t fused_add( void* args,  void* arg_type_ids, int32_t num_args) {\n",
            "  if (!((num_args == 3))) {\n",
            "    TVMAPISetLastError(\"fused_add: num_args should be 3\");\n",
            "    return -1;\n",
            "  }\n",
            "  void* arg0 = (((TVMValue*)args)[0].v_handle);\n",
            "  int32_t arg0_code = (( int32_t*)arg_type_ids)[0];\n",
            "  void* arg1 = (((TVMValue*)args)[1].v_handle);\n",
            "  int32_t arg1_code = (( int32_t*)arg_type_ids)[1];\n",
            "  void* arg2 = (((TVMValue*)args)[2].v_handle);\n",
            "  int32_t arg2_code = (( int32_t*)arg_type_ids)[2];\n",
            "  float* placeholder = (float*)(((TVMArray*)arg0)[0].data);\n",
            "  int64_t* arg0_shape = (int64_t*)(((TVMArray*)arg0)[0].shape);\n",
            "  int64_t* arg0_strides = (int64_t*)(((TVMArray*)arg0)[0].strides);\n",
            "  if (!(arg0_strides == NULL)) {\n",
            "    if (!((1 == ((int32_t)arg0_strides[0])))) {\n",
            "      TVMAPISetLastError(\"arg0.strides: expected to be compact array\");\n",
            "      return -1;\n",
            "    }\n",
            "  }\n",
            "  int32_t dev_type = (((TVMArray*)arg0)[0].ctx.device_type);\n",
            "  int32_t dev_id = (((TVMArray*)arg0)[0].ctx.device_id);\n",
            "  float* placeholder1 = (float*)(((TVMArray*)arg1)[0].data);\n",
            "  int64_t* arg1_shape = (int64_t*)(((TVMArray*)arg1)[0].shape);\n",
            "  int64_t* arg1_strides = (int64_t*)(((TVMArray*)arg1)[0].strides);\n",
            "  if (!(arg1_strides == NULL)) {\n",
            "    if (!((1 == ((int32_t)arg1_strides[0])))) {\n",
            "      TVMAPISetLastError(\"arg1.strides: expected to be compact array\");\n",
            "      return -1;\n",
            "    }\n",
            "  }\n",
            "  float* T_add = (float*)(((TVMArray*)arg2)[0].data);\n",
            "  int64_t* arg2_shape = (int64_t*)(((TVMArray*)arg2)[0].shape);\n",
            "  int64_t* arg2_strides = (int64_t*)(((TVMArray*)arg2)[0].strides);\n",
            "  if (!(arg2_strides == NULL)) {\n",
            "    if (!((1 == ((int32_t)arg2_strides[0])))) {\n",
            "      TVMAPISetLastError(\"arg2.strides: expected to be compact array\");\n",
            "      return -1;\n",
            "    }\n",
            "  }\n",
            "  if (!(((((arg0_code == 3) || (arg0_code == 13)) || (arg0_code == 7)) || (arg0_code == 4)))) {\n",
            "    TVMAPISetLastError(\"fused_add: Expect arg[0] to be pointer\");\n",
            "    return -1;\n",
            "  }\n",
            "  if (!(((((arg1_code == 3) || (arg1_code == 13)) || (arg1_code == 7)) || (arg1_code == 4)))) {\n",
            "    TVMAPISetLastError(\"fused_add: Expect arg[1] to be pointer\");\n",
            "    return -1;\n",
            "  }\n",
            "  if (!(((((arg2_code == 3) || (arg2_code == 13)) || (arg2_code == 7)) || (arg2_code == 4)))) {\n",
            "    TVMAPISetLastError(\"fused_add: Expect arg[2] to be pointer\");\n",
            "    return -1;\n",
            "  }\n",
            "  if (!((dev_type == 1))) {\n",
            "    TVMAPISetLastError(\"device_type need to be 1\");\n",
            "    return -1;\n",
            "  }\n",
            "  if (!((1 == (((TVMArray*)arg0)[0].ndim)))) {\n",
            "    TVMAPISetLastError(\"arg0.ndim is expected to equal 1\");\n",
            "    return -1;\n",
            "  }\n",
            "  if (!(((((((TVMArray*)arg0)[0].dtype.code) == (uint8_t)2) && ((((TVMArray*)arg0)[0].dtype.bits) == (uint8_t)32)) && ((((TVMArray*)arg0)[0].dtype.lanes) == (uint16_t)1)))) {\n",
            "    TVMAPISetLastError(\"arg0.dtype is expected to be float32\");\n",
            "    return -1;\n",
            "  }\n",
            "  if (!((((int32_t)arg0_shape[0]) == 1024))) {\n",
            "    TVMAPISetLastError(\"Argument arg0.shape[0] has an unsatisfied constraint\");\n",
            "    return -1;\n",
            "  }\n",
            "  if (!(((((TVMArray*)arg0)[0].byte_offset) == (uint64_t)0))) {\n",
            "    TVMAPISetLastError(\"Argument arg0.byte_offset has an unsatisfied constraint\");\n",
            "    return -1;\n",
            "  }\n",
            "  if (!((1 == (((TVMArray*)arg1)[0].ndim)))) {\n",
            "    TVMAPISetLastError(\"arg1.ndim is expected to equal 1\");\n",
            "    return -1;\n",
            "  }\n",
            "  if (!(((((((TVMArray*)arg1)[0].dtype.code) == (uint8_t)2) && ((((TVMArray*)arg1)[0].dtype.bits) == (uint8_t)32)) && ((((TVMArray*)arg1)[0].dtype.lanes) == (uint16_t)1)))) {\n",
            "    TVMAPISetLastError(\"arg1.dtype is expected to be float32\");\n",
            "    return -1;\n",
            "  }\n",
            "  if (!((((int32_t)arg1_shape[0]) == 1024))) {\n",
            "    TVMAPISetLastError(\"Argument arg1.shape[0] has an unsatisfied constraint\");\n",
            "    return -1;\n",
            "  }\n",
            "  if (!(((((TVMArray*)arg1)[0].byte_offset) == (uint64_t)0))) {\n",
            "    TVMAPISetLastError(\"Argument arg1.byte_offset has an unsatisfied constraint\");\n",
            "    return -1;\n",
            "  }\n",
            "  if (!((1 == (((TVMArray*)arg1)[0].ctx.device_type)))) {\n",
            "    TVMAPISetLastError(\"Argument arg1.device_type has an unsatisfied constraint\");\n",
            "    return -1;\n",
            "  }\n",
            "  if (!((dev_id == (((TVMArray*)arg1)[0].ctx.device_id)))) {\n",
            "    TVMAPISetLastError(\"Argument arg1.device_id has an unsatisfied constraint\");\n",
            "    return -1;\n",
            "  }\n",
            "  if (!((1 == (((TVMArray*)arg2)[0].ndim)))) {\n",
            "    TVMAPISetLastError(\"arg2.ndim is expected to equal 1\");\n",
            "    return -1;\n",
            "  }\n",
            "  if (!(((((((TVMArray*)arg2)[0].dtype.code) == (uint8_t)2) && ((((TVMArray*)arg2)[0].dtype.bits) == (uint8_t)32)) && ((((TVMArray*)arg2)[0].dtype.lanes) == (uint16_t)1)))) {\n",
            "    TVMAPISetLastError(\"arg2.dtype is expected to be float32\");\n",
            "    return -1;\n",
            "  }\n",
            "  if (!((((int32_t)arg2_shape[0]) == 1024))) {\n",
            "    TVMAPISetLastError(\"Argument arg2.shape[0] has an unsatisfied constraint\");\n",
            "    return -1;\n",
            "  }\n",
            "  if (!(((((TVMArray*)arg2)[0].byte_offset) == (uint64_t)0))) {\n",
            "    TVMAPISetLastError(\"Argument arg2.byte_offset has an unsatisfied constraint\");\n",
            "    return -1;\n",
            "  }\n",
            "  if (!((1 == (((TVMArray*)arg2)[0].ctx.device_type)))) {\n",
            "    TVMAPISetLastError(\"Argument arg2.device_type has an unsatisfied constraint\");\n",
            "    return -1;\n",
            "  }\n",
            "  if (!((dev_id == (((TVMArray*)arg2)[0].ctx.device_id)))) {\n",
            "    TVMAPISetLastError(\"Argument arg2.device_id has an unsatisfied constraint\");\n",
            "    return -1;\n",
            "  }\n",
            "  for (int32_t ax0 = 0; ax0 < 1024; ++ax0) {\n",
            "    T_add[ax0] = (placeholder[ax0] + placeholder1[ax0]);\n",
            "  }\n",
            "  return 0;\n",
            "}\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aEGvx5OVymcD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_c_module(func):\n",
        "  with tvm.build_config(disable_vectorize=True):\n",
        "    _, src_mod, _ = relay.build(func, target=\"c\", params={})\n",
        "  return src_mod"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QSJcyuD-0AJa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 531
        },
        "outputId": "3402e226-33bb-4928-d6b5-be11a1cc38f2"
      },
      "source": [
        "func = relay.Function([], relay.const(1.0))\n",
        "print(build_c_module(func).get_source())"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TVMError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTVMError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-7dca14713a62>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrelay\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrelay\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconst\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuild_c_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_source\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-5-019b33d98b56>\u001b[0m in \u001b[0;36mbuild_c_module\u001b[0;34m(func)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mbuild_c_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mtvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdisable_vectorize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrelay\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0msrc_mod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tvm/python/tvm/relay/build_module.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(func, target, target_host, params)\u001b[0m\n\u001b[1;32m    194\u001b[0m         \u001b[0mbld_mod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBuildModule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m         graph_json, mod, params = bld_mod.build(func, target, target_host,\n\u001b[0;32m--> 196\u001b[0;31m                                                 params)\n\u001b[0m\u001b[1;32m    197\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_json\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tvm/python/tvm/relay/build_module.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, func, target, target_host, params)\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0;31m# Build the function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_host\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m         \u001b[0;31m# Get artifacts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0mgraph_json\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tvm/python/tvm/_ffi/_ctypes/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    207\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtcodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m                 ctypes.byref(ret_val), ctypes.byref(ret_tcode)) != 0:\n\u001b[0;32m--> 209\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mget_last_ffi_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m         \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m         \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTVMError\u001b[0m: Traceback (most recent call last):\n  [bt] (8) /tvm/build/libtvm.so(+0x4bbad7) [0x7f8930fa1ad7]\n  [bt] (7) /tvm/build/libtvm.so(+0x4bb8ff) [0x7f8930fa18ff]\n  [bt] (6) /tvm/build/libtvm.so(+0x4baef4) [0x7f8930fa0ef4]\n  [bt] (5) /tvm/build/libtvm.so(tvm::build(tvm::Map<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::Array<tvm::LoweredFunc, void>, void, void> const&, tvm::Target const&, tvm::BuildConfig const&)+0x309) [0x7f8930d397e9]\n  [bt] (4) /tvm/build/libtvm.so(tvm::build(tvm::Map<tvm::Target, tvm::Array<tvm::LoweredFunc, void>, void, void> const&, tvm::Target const&, tvm::BuildConfig const&)+0x620) [0x7f8930d38ec0]\n  [bt] (3) /tvm/build/libtvm.so(+0x25a2b7) [0x7f8930d402b7]\n  [bt] (2) /tvm/build/libtvm.so(+0x948b73) [0x7f893142eb73]\n  [bt] (1) /tvm/build/libtvm.so(+0x94aaab) [0x7f8931430aab]\n  [bt] (0) /tvm/build/libtvm.so(+0x154973) [0x7f8930c3a973]\n  File \"/content/gdrive/My Drive/tvm/src/codegen/llvm/llvm_module.cc\", line 180\nTVMError: Check failed: funcs.size() != 0U (0 vs. 0) : "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CjayGmPKynSH",
        "colab_type": "text"
      },
      "source": [
        "Try building some other functions and see what source is generated."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HJbJDEaqtv5t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: Try another function.\n",
        "func = ...\n",
        "print(build_c_module(func).get_source())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqGxOMFp0NsB",
        "colab_type": "text"
      },
      "source": [
        "## Graph Runtime Execution\n",
        "\n",
        "The primary execution strategy for running models with MicroTVM is the graph runtime.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "looZwQcl2D9H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 351
        },
        "outputId": "63fab03c-ae13-47b4-d7c1-9325063a7842"
      },
      "source": [
        "DEVICE_TYPE = \"host\"\n",
        "# TODO: Explain this variable.\n",
        "BINUTIL_PREFIX = \"\"\n",
        "\n",
        "shape = (1024,)\n",
        "dtype = \"float32\"\n",
        "    \n",
        "# Construct Relay program.\n",
        "x = relay.var(\"x\", relay.TensorType(shape=shape, dtype=dtype))\n",
        "xx = relay.multiply(x, x)\n",
        "z = relay.add(xx, relay.const(1.0))\n",
        "func = relay.Function([x], z)\n",
        "\n",
        "with micro.Session(DEVICE_TYPE, BINUTIL_PREFIX) as sess:\n",
        "    mod, params = relay_micro_build(func, BINUTIL_PREFIX)\n",
        "\n",
        "    mod.set_input(**params)\n",
        "    x_in = np.random.uniform(size=shape[0]).astype(dtype)\n",
        "    mod.run(x=x_in)\n",
        "    result = mod.get_output(0).asnumpy()\n",
        "\n",
        "    tvm.testing.assert_allclose(\n",
        "            result, x_in * x_in + 1.0)\n",
        "\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-953565aa87a7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrelay\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mmicro\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE_TYPE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBINUTIL_PREFIX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mmod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrelay_micro_build\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBINUTIL_PREFIX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tvm/python/tvm/micro/base.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;31m# Then, initialize the session (includes loading the compiled runtime lib).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m         \u001b[0m_InitSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mruntime_lib_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0;31m# Return `self` to bind the session as a variable in the `with` block.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name '_InitSession' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITy9wnt1WsN_",
        "colab_type": "text"
      },
      "source": [
        "## ResNet-18 Demo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZAxIkBheYrOp",
        "colab_type": "text"
      },
      "source": [
        "Before we get into the mechanics of MicroTVM, we start with a motivating example.  In the code block below, we perform image recognition on a picture of a cat using ResNet-18.\n",
        "\n",
        "First, we import the model from MxNet's model zoo and use TVM's MxNet model importer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FrTSYq4wqc_N",
        "colab_type": "code",
        "outputId": "c9035faa-76de-46ac-9e06-f478b0bc8405",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        }
      },
      "source": [
        "import mxnet as mx\n",
        "from mxnet.gluon.model_zoo.vision import get_model\n",
        "\n",
        "# Fetch a mapping from class IDs to human-readable labels.\n",
        "synset_url = \"\".join([\"https://gist.githubusercontent.com/zhreshold/\",\n",
        "                      \"4d0b62f3d01426887599d4f7ede23ee5/raw/\",\n",
        "                      \"596b27d23537e5a1b5751d2b0481ef172f58b539/\",\n",
        "                      \"imagenet1000_clsid_to_human.txt\"])\n",
        "synset_name = \"synset.txt\"\n",
        "download(synset_url, synset_name)\n",
        "with open(synset_name) as f:\n",
        "    synset = eval(f.read())\n",
        "    \n",
        "block = get_model(\"resnet18_v1\", pretrained=True)\n",
        "func, params = relay.frontend.from_mxnet(\n",
        "    block, shape={\"data\": image.shape})"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-7eb2f755b910>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mmxnet\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmxnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgluon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_zoo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvision\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Fetch a mapping from class IDs to human-readable labels.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m synset_url = \"\".join([\"https://gist.githubusercontent.com/zhreshold/\",\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'mxnet'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1OjQ4rdBtSer",
        "colab_type": "text"
      },
      "source": [
        "Now, we download a picture of a cat and convert it to a format the model can understand."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iKH4Hmp6WwPD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from mxnet.gluon.utils import download\n",
        "from PIL import Image\n",
        "\n",
        "dtype = \"float32\"\n",
        "\n",
        "img_name = \"cat.png\"\n",
        "download(\"https://github.com/dmlc/mxnet.js/blob/master/data/cat.png?raw=true\",\n",
        "         img_name)\n",
        "image = Image.open(img_name).resize((224, 224))\n",
        "image = np.array(image) - np.array([123., 117., 104.])\n",
        "image /= np.array([58.395, 57.12, 57.375])\n",
        "image = image.transpose((2, 0, 1))\n",
        "image = image[np.newaxis, :]\n",
        "image = tvm.nd.array(image.astype(dtype))\n",
        "\n",
        "# TODO: print image"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2EEuEtrjtcRj",
        "colab_type": "text"
      },
      "source": [
        "With that, we can create a MicroTVM session, build a graph runtime module, then run the model.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CqmbMcmga4Vp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DEVICE_TYPE = \"host\"\n",
        "# TODO: Explain this variable.\n",
        "BINUTIL_PREFIX = \"\"\n",
        "\n",
        "with micro.Session(DEVICE_TYPE, BINUTIL_PREFIX) as sess:\n",
        "    mod, params = relay_micro_build(func, BINUTIL_PREFIX, params=params)\n",
        "    # Set model weights.\n",
        "    mod.set_input(**params)\n",
        "    # Execute with `image` as the input.\n",
        "    mod.run(data=image)\n",
        "    # Get outputs.\n",
        "    tvm_output = mod.get_output(0)\n",
        "\n",
        "    prediction_idx = np.argmax(tvm_output.asnumpy()[0])\n",
        "    prediction = synset[prediction_idx]\n",
        "    print(prediction)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJJZRRnr0gGw",
        "colab_type": "text"
      },
      "source": [
        "## The Low-Level Device Interface\n",
        "\n",
        "The low-level device interface is used by TVM to communicate with MicroDevices. It exposes three important functions to interact with the MicroDevice: \n",
        "- Read from device memory.\n",
        "- Write to device memory.\n",
        "- Start function execution.\n",
        "\n",
        "\n",
        "Below is an example which uses an allocated region of memory on the host machine to simulate a MicroDevice. This HostLowLevelDevice provides MicroTVM users with an easy test setup to experiment with, without having to communicate and debug external MicroDevices. However, the LowLevelDevice interface can be implemented for any MicroDevice, for example over JTAG or a network connection, which will enable you to run TVM on your own MicroDevice.\n",
        "\n",
        "TODO: Add prints that are only on the `tutorial` branch?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dm1fZjgi66te",
        "colab_type": "text"
      },
      "source": [
        "```\n",
        "#@title \n",
        "#include <sys/mman.h>\n",
        "#include <cstring>\n",
        "#include \"low_level_device.h\"\n",
        "#include \"micro_common.h\"\n",
        "\n",
        "namespace tvm {\n",
        "namespace runtime {\n",
        "/*!\n",
        " * \\brief emulated low-level device on host machine\n",
        " */\n",
        "class HostLowLevelDevice final : public LowLevelDevice {\n",
        " public:\n",
        "  /*!\n",
        "   * \\brief constructor to initialize on-host memory region to act as device\n",
        "   * \\param num_bytes size of the emulated on-device memory region\n",
        "   */\n",
        "  explicit HostLowLevelDevice(size_t num_bytes)\n",
        "    : size_(num_bytes) {\n",
        "    size_t size_in_pages = (num_bytes + kPageSize - 1) / kPageSize;\n",
        "    int mmap_prot = PROT_READ | PROT_WRITE | PROT_EXEC;\n",
        "    int mmap_flags = MAP_ANONYMOUS | MAP_PRIVATE;\n",
        "    base_addr_ = DevBaseAddr(\n",
        "      (reinterpret_cast<std::uintptr_t>(\n",
        "        mmap(nullptr, size_in_pages * kPageSize, mmap_prot, mmap_flags, -1, 0))));\n",
        "  }\n",
        "\n",
        "  /*!\n",
        "   * \\brief destructor to deallocate on-host device region\n",
        "   */\n",
        "  ~HostLowLevelDevice() {\n",
        "    munmap(base_addr_.cast_to<void*>(), size_);\n",
        "  }\n",
        "\n",
        "  void Write(DevBaseOffset offset,\n",
        "             void* buf,\n",
        "             size_t num_bytes) final {\n",
        "    void* addr = (offset + base_addr_).cast_to<void*>();\n",
        "    std::memcpy(addr, buf, num_bytes);\n",
        "  }\n",
        "\n",
        "  void Read(DevBaseOffset offset,\n",
        "            void* buf,\n",
        "            size_t num_bytes) final {\n",
        "    void* addr = (offset + base_addr_).cast_to<void*>();\n",
        "    std::memcpy(buf, addr, num_bytes);\n",
        "  }\n",
        "\n",
        "  void Execute(DevBaseOffset func_offset, DevBaseOffset breakpoint) final {\n",
        "    DevAddr func_addr = func_offset + base_addr_;\n",
        "    reinterpret_cast<void (*)(void)>(func_addr.value())();\n",
        "  }\n",
        "\n",
        "  DevBaseAddr base_addr() const final {\n",
        "    return base_addr_;\n",
        "  }\n",
        "\n",
        "  const char* device_type() const final {\n",
        "    return \"host\";\n",
        "  }\n",
        "\n",
        " private:\n",
        "  /*! \\brief base address of the micro device memory region */\n",
        "  DevBaseAddr base_addr_;\n",
        "  /*! \\brief size of memory region */\n",
        "  size_t size_;\n",
        "};\n",
        "\n",
        "const std::shared_ptr<LowLevelDevice> HostLowLevelDeviceCreate(size_t num_bytes) {\n",
        "  std::shared_ptr<LowLevelDevice> lld =\n",
        "      std::make_shared<HostLowLevelDevice>(num_bytes);\n",
        "  return lld;\n",
        "}\n",
        "}  // namespace runtime\n",
        "}  // namespace tvm\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g8dibeuWH8Nt",
        "colab_type": "text"
      },
      "source": [
        "# Homework: RISC-V\n",
        "\n",
        "To use RISC-V as a target device, there are some extra steps that can't be done within this notebook.\n",
        "\n",
        "First, you will need to download and compile [TVM](https://github.com/dmlc/tvm) on your own machine.\n",
        "\n",
        "Next, you will need [Spike](https://github.com/riscv/riscv-isa-sim) (a RISC-V ISA simulator) and [OpenOCD](https://github.com/ntfreak/openocd) (provides a high-level debugging interface to compatible devices).\n",
        "\n",
        "TODO: Flesh out."
      ]
    }
  ]
}