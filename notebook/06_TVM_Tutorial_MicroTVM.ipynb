{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MicroTVM iPython Notebook",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZ_LgheWhovH",
        "colab_type": "text"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/uwsampl/tutorial/blob/master/notebook/06_TVM_Tutorial_MicroTVM.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xFoF4FhyYcwE",
        "colab_type": "text"
      },
      "source": [
        "Please run the following block to ensure TVM is setup for *this notebook*.  Each notebook may have its own runtime."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ce5eshWvUiyN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1462
        },
        "outputId": "25fc3df3-739a-4716-bdf9-ab08be1fc424"
      },
      "source": [
        "try:\n",
        "  import google.colab\n",
        "  IN_COLAB = True\n",
        "except:\n",
        "  IN_COLAB = False\n",
        "\n",
        "if IN_COLAB:\n",
        "    ! gsutil cp \"gs://tvm-fcrc-binariesd5fce43e-8373-11e9-bfb6-0242ac1c0002/tvm.tar.gz\" /tmp/tvm.tar.gz\n",
        "    ! mkdir -p /tvm\n",
        "    ! tar -xf /tmp/tvm.tar.gz --strip-components=4 --directory /tvm\n",
        "    ! ls -la /tvm\n",
        "    ! bash /tvm/package.sh\n",
        "    # Add TVM to the Python path.\n",
        "    import sys\n",
        "    sys.path.append('/tvm/python')\n",
        "    sys.path.append('/tvm/topi/python')\n",
        "    ! pip install mxnet\n",
        "else:\n",
        "    print(\"Notebook executing locally, skipping Colab setup ...\")\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Copying gs://tvm-fcrc-binariesd5fce43e-8373-11e9-bfb6-0242ac1c0002/tvm.tar.gz...\n",
            "- [1 files][112.9 MiB/112.9 MiB]                                                \n",
            "Operation completed over 1 objects/112.9 MiB.                                    \n",
            "total 164\n",
            "drwxr-xr-x 21 root root  4096 Jun 10 19:41 .\n",
            "drwxr-xr-x  1 root root  4096 Jun 10 19:40 ..\n",
            "drwx------  8 root root  4096 May 31 08:14 3rdparty\n",
            "drwx------ 12 root root  4096 May 31 08:14 apps\n",
            "drwx------  3 root root  4096 Jun  4 09:46 build\n",
            "drwx------  4 root root  4096 May 31 08:14 cmake\n",
            "-rw-------  1 root root 10406 May 31 08:14 CMakeLists.txt\n",
            "drwx------  6 root root  4096 May 31 08:14 conda\n",
            "-rw-------  1 root root  5673 May 31 08:14 CONTRIBUTORS.md\n",
            "drwx------  3 root root  4096 May 31 08:14 docker\n",
            "drwx------ 11 root root  4096 May 31 08:14 docs\n",
            "drwx------  4 root root  4096 May 31 08:14 golang\n",
            "drwx------  3 root root  4096 May 31 08:14 include\n",
            "-rw-------  1 root root 10027 May 31 08:14 Jenkinsfile\n",
            "drwx------  6 root root  4096 May 31 08:14 jvm\n",
            "-rw-------  1 root root 11357 May 31 08:14 LICENSE\n",
            "-rw-------  1 root root  4267 May 31 08:14 Makefile\n",
            "-rw-------  1 root root 10476 May 31 08:14 NEWS.md\n",
            "drwx------  9 root root  4096 May 31 08:14 nnvm\n",
            "-rw-------  1 root root    61 May 31 08:14 NOTICE\n",
            "-rw-------  1 root root   369 Jun  3 22:49 package.sh\n",
            "drwx------  3 root root  4096 May 31 08:14 python\n",
            "-rw-------  1 root root  2705 May 31 08:14 README.md\n",
            "drwx------  5 root root  4096 May 31 08:14 rust\n",
            "drwx------ 14 root root  4096 May 31 08:14 src\n",
            "drwx------  9 root root  4096 May 31 08:14 tests\n",
            "drwx------  7 root root  4096 May 31 08:14 topi\n",
            "drwx------  8 root root  4096 May 31 08:14 tutorials\n",
            "-rw-------  1 root root  2902 May 31 08:14 version.py\n",
            "drwx------ 10 root root  4096 May 31 08:14 vta\n",
            "drwx------  2 root root  4096 May 31 08:14 web\n",
            "Installing Dependencies ...\n",
            "deb https://dl.bintray.com/sbt/debian /\n",
            "Executing: /tmp/apt-key-gpghome.AN9TDEGt5u/gpg.1.sh --keyserver hkp://keyserver.ubuntu.com:80 --recv 2EE0EA64E40A89B84B2DF73499E82A75642AC823\n",
            "gpg: key 99E82A75642AC823: \"sbt build tool <scalasbt@gmail.com>\" not changed\n",
            "gpg: Total number processed: 1\n",
            "gpg:              unchanged: 1\n",
            "Get:1 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Hit:2 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Ign:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:5 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu bionic-backports InRelease\n",
            "Ign:7 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:8 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
            "Hit:9 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Hit:10 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran35/ InRelease\n",
            "Get:11 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [324 kB]\n",
            "Hit:12 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic InRelease\n",
            "Ign:13 https://dl.bintray.com/sbt/debian  InRelease\n",
            "Get:14 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [490 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [1,217 kB]\n",
            "Get:16 https://dl.bintray.com/sbt/debian  Release [815 B]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [834 kB]\n",
            "Fetched 3,042 kB in 1s (2,489 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Target Packages (Packages) is configured multiple times in /etc/apt/sources.list.d/sbt.list:1 and /etc/apt/sources.list.d/sbt.list:2\n",
            "W: Target Packages (Packages) is configured multiple times in /etc/apt/sources.list.d/sbt.list:1 and /etc/apt/sources.list.d/sbt.list:2\n",
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "libffi-dev is already the newest version (3.2.1-8).\n",
            "llvm-6.0 is already the newest version (1:6.0-1ubuntu2).\n",
            "zlib1g-dev is already the newest version (1:1.2.11.dfsg-0ubuntu2).\n",
            "clinfo is already the newest version (2.2.18.03.26-1).\n",
            "libtinfo-dev is already the newest version (6.1-1ubuntu1.18.04).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-410\n",
            "Use 'sudo apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 80 not upgraded.\n",
            "W: Target Packages (Packages) is configured multiple times in /etc/apt/sources.list.d/sbt.list:1 and /etc/apt/sources.list.d/sbt.list:2\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "verilator is already the newest version (3.916-1build1).\n",
            "sbt is already the newest version (1.2.8).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-410\n",
            "Use 'sudo apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 80 not upgraded.\n",
            "W: Target Packages (Packages) is configured multiple times in /etc/apt/sources.list.d/sbt.list:1 and /etc/apt/sources.list.d/sbt.list:2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hHFxmiOzjFF2",
        "colab_type": "text"
      },
      "source": [
        "# MicroTVM\n",
        "\n",
        "In this notebook, we're going to cover:\n",
        "\n",
        "*   the C code generation backend\n",
        "*   the graph runtime\n",
        "*   a demo of ResNeT on the graph runtime\n",
        "*   the low-level device interface\n",
        "\n",
        "\n",
        "First, we run some necessary imports."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RHHOBXwRuJGl",
        "colab_type": "code",
        "outputId": "11cfa43b-1f7c-49d6-e1fc-8b122cca04e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        }
      },
      "source": [
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import tvm\n",
        "from tvm.contrib import graph_runtime, util\n",
        "from tvm import relay\n",
        "import tvm.micro as micro"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-4e67d16c2d7d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgraph_runtime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtvm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrelay\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmicro\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmicro\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tvm.micro'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ERvCOpVjsn_U",
        "colab_type": "text"
      },
      "source": [
        "## C Codegen\n",
        "\n",
        "Let's take a look at the C codegen backend for a simple function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fAozo7Slus7I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# First, we construct the function.\n",
        "ty = relay.TensorType(shape=(1024,), dtype=\"float32\")\n",
        "x = relay.var(\"x\", ty)\n",
        "y = relay.var(\"y\", ty)\n",
        "func = relay.Function([x], relay.add(x, y))\n",
        "\n",
        "# Then build it with the appropriate configuration.\n",
        "with tvm.build_config(disable_vectorize=True):\n",
        "  _, src_mod, _ = relay.build(func, target=\"c\", params={})\n",
        "\n",
        "# Now, we can see the source that was generated.\n",
        "print(src_mod.get_source())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aEGvx5OVymcD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_c_module(func):\n",
        "  with tvm.build_config(disable_vectorize=True):\n",
        "    _, src_mod, _ = relay.build(func, target=\"c\", params={})\n",
        "  return src_mod"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QSJcyuD-0AJa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "func = relay.Function([], relay.const(1.0))\n",
        "print(build_c_module(func).get_source())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CjayGmPKynSH",
        "colab_type": "text"
      },
      "source": [
        "Try building some other functions and see what source is generated."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HJbJDEaqtv5t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: Try another function.\n",
        "func = ...\n",
        "print(build_c_module(func).get_source())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqGxOMFp0NsB",
        "colab_type": "text"
      },
      "source": [
        "## Graph Runtime Execution\n",
        "\n",
        "The primary execution strategy for running models with MicroTVM is the graph runtime.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "looZwQcl2D9H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DEVICE_TYPE = \"host\"\n",
        "# TODO: Explain this variable.\n",
        "BINUTIL_PREFIX = \"\"\n",
        "\n",
        "shape = (1024,)\n",
        "dtype = \"float32\"\n",
        "    \n",
        "# Construct Relay program.\n",
        "x = relay.var(\"x\", relay.TensorType(shape=shape, dtype=dtype))\n",
        "xx = relay.multiply(x, x)\n",
        "z = relay.add(xx, relay.const(1.0))\n",
        "func = relay.Function([x], z)\n",
        "\n",
        "with micro.Session(DEVICE_TYPE, BINUTIL_PREFIX) as sess:\n",
        "    mod, params = relay_micro_build(func, BINUTIL_PREFIX)\n",
        "\n",
        "    mod.set_input(**params)\n",
        "    x_in = np.random.uniform(size=shape[0]).astype(dtype)\n",
        "    mod.run(x=x_in)\n",
        "    result = mod.get_output(0).asnumpy()\n",
        "\n",
        "    tvm.testing.assert_allclose(\n",
        "            result, x_in * x_in + 1.0)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITy9wnt1WsN_",
        "colab_type": "text"
      },
      "source": [
        "## ResNet-18 Demo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZAxIkBheYrOp",
        "colab_type": "text"
      },
      "source": [
        "Before we get into the mechanics of MicroTVM, we start with a motivating example.  In the code block below, we perform image recognition on a picture of a cat using ResNet-18.\n",
        "\n",
        "First, we import the model from MxNet's model zoo and use TVM's MxNet model importer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FrTSYq4wqc_N",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        },
        "outputId": "c9035faa-76de-46ac-9e06-f478b0bc8405"
      },
      "source": [
        "import mxnet as mx\n",
        "from mxnet.gluon.model_zoo.vision import get_model\n",
        "\n",
        "# Fetch a mapping from class IDs to human-readable labels.\n",
        "synset_url = \"\".join([\"https://gist.githubusercontent.com/zhreshold/\",\n",
        "                      \"4d0b62f3d01426887599d4f7ede23ee5/raw/\",\n",
        "                      \"596b27d23537e5a1b5751d2b0481ef172f58b539/\",\n",
        "                      \"imagenet1000_clsid_to_human.txt\"])\n",
        "synset_name = \"synset.txt\"\n",
        "download(synset_url, synset_name)\n",
        "with open(synset_name) as f:\n",
        "    synset = eval(f.read())\n",
        "    \n",
        "block = get_model(\"resnet18_v1\", pretrained=True)\n",
        "func, params = relay.frontend.from_mxnet(\n",
        "    block, shape={\"data\": image.shape})"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-7eb2f755b910>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mmxnet\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmxnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgluon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_zoo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvision\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Fetch a mapping from class IDs to human-readable labels.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m synset_url = \"\".join([\"https://gist.githubusercontent.com/zhreshold/\",\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'mxnet'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1OjQ4rdBtSer",
        "colab_type": "text"
      },
      "source": [
        "Now, we download a picture of a cat and convert it to a format the model can understand."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iKH4Hmp6WwPD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from mxnet.gluon.utils import download\n",
        "from PIL import Image\n",
        "\n",
        "dtype = \"float32\"\n",
        "\n",
        "img_name = \"cat.png\"\n",
        "download(\"https://github.com/dmlc/mxnet.js/blob/master/data/cat.png?raw=true\",\n",
        "         img_name)\n",
        "image = Image.open(img_name).resize((224, 224))\n",
        "image = np.array(image) - np.array([123., 117., 104.])\n",
        "image /= np.array([58.395, 57.12, 57.375])\n",
        "image = image.transpose((2, 0, 1))\n",
        "image = image[np.newaxis, :]\n",
        "image = tvm.nd.array(image.astype(dtype))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2EEuEtrjtcRj",
        "colab_type": "text"
      },
      "source": [
        "With that, we can create a MicroTVM session, build a graph runtime module, then run the model.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CqmbMcmga4Vp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DEVICE_TYPE = \"host\"\n",
        "# TODO: Explain this variable.\n",
        "BINUTIL_PREFIX = \"\"\n",
        "\n",
        "with micro.Session(DEVICE_TYPE, BINUTIL_PREFIX) as sess:\n",
        "    mod, params = relay_micro_build(func, BINUTIL_PREFIX, params=params)\n",
        "    # Set model weights.\n",
        "    mod.set_input(**params)\n",
        "    # Execute with `image` as the input.\n",
        "    mod.run(data=image)\n",
        "    # Get outputs.\n",
        "    tvm_output = mod.get_output(0)\n",
        "\n",
        "    prediction_idx = np.argmax(tvm_output.asnumpy()[0])\n",
        "    prediction = synset[prediction_idx]\n",
        "    assert prediction == \"tiger cat\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJJZRRnr0gGw",
        "colab_type": "text"
      },
      "source": [
        "## The Low-Level Device Interface\n",
        "\n",
        "The low-level device interface is used by TVM to communicate with MicroDevices. It exposes three important functions to interact with the MicroDevice: \n",
        "- Read from device memory.\n",
        "- Write to device memory.\n",
        "- Start function execution.\n",
        "\n",
        "\n",
        "Below is an example which uses an allocated region of memory on the host machine to simulate a MicroDevice. This HostLowLevelDevice provides MicroTVM users with an easy test setup to experiment with, without having to communicate and debug external MicroDevices. However, the LowLevelDevice interface can be implemented for any MicroDevice, for example over JTAG or a network connection, which will enable you to run TVM on your own MicroDevice.\n",
        "\n",
        "TODO: Should we implement this as a Python example?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dm1fZjgi66te",
        "colab_type": "text"
      },
      "source": [
        "```\n",
        "#@title \n",
        "#include <sys/mman.h>\n",
        "#include <cstring>\n",
        "#include \"low_level_device.h\"\n",
        "#include \"micro_common.h\"\n",
        "\n",
        "namespace tvm {\n",
        "namespace runtime {\n",
        "/*!\n",
        " * \\brief emulated low-level device on host machine\n",
        " */\n",
        "class HostLowLevelDevice final : public LowLevelDevice {\n",
        " public:\n",
        "  /*!\n",
        "   * \\brief constructor to initialize on-host memory region to act as device\n",
        "   * \\param num_bytes size of the emulated on-device memory region\n",
        "   */\n",
        "  explicit HostLowLevelDevice(size_t num_bytes)\n",
        "    : size_(num_bytes) {\n",
        "    size_t size_in_pages = (num_bytes + kPageSize - 1) / kPageSize;\n",
        "    int mmap_prot = PROT_READ | PROT_WRITE | PROT_EXEC;\n",
        "    int mmap_flags = MAP_ANONYMOUS | MAP_PRIVATE;\n",
        "    base_addr_ = DevBaseAddr(\n",
        "      (reinterpret_cast<std::uintptr_t>(\n",
        "        mmap(nullptr, size_in_pages * kPageSize, mmap_prot, mmap_flags, -1, 0))));\n",
        "  }\n",
        "\n",
        "  /*!\n",
        "   * \\brief destructor to deallocate on-host device region\n",
        "   */\n",
        "  ~HostLowLevelDevice() {\n",
        "    munmap(base_addr_.cast_to<void*>(), size_);\n",
        "  }\n",
        "\n",
        "  void Write(DevBaseOffset offset,\n",
        "             void* buf,\n",
        "             size_t num_bytes) final {\n",
        "    void* addr = (offset + base_addr_).cast_to<void*>();\n",
        "    std::memcpy(addr, buf, num_bytes);\n",
        "  }\n",
        "\n",
        "  void Read(DevBaseOffset offset,\n",
        "            void* buf,\n",
        "            size_t num_bytes) final {\n",
        "    void* addr = (offset + base_addr_).cast_to<void*>();\n",
        "    std::memcpy(buf, addr, num_bytes);\n",
        "  }\n",
        "\n",
        "  void Execute(DevBaseOffset func_offset, DevBaseOffset breakpoint) final {\n",
        "    DevAddr func_addr = func_offset + base_addr_;\n",
        "    reinterpret_cast<void (*)(void)>(func_addr.value())();\n",
        "  }\n",
        "\n",
        "  DevBaseAddr base_addr() const final {\n",
        "    return base_addr_;\n",
        "  }\n",
        "\n",
        "  const char* device_type() const final {\n",
        "    return \"host\";\n",
        "  }\n",
        "\n",
        " private:\n",
        "  /*! \\brief base address of the micro device memory region */\n",
        "  DevBaseAddr base_addr_;\n",
        "  /*! \\brief size of memory region */\n",
        "  size_t size_;\n",
        "};\n",
        "\n",
        "const std::shared_ptr<LowLevelDevice> HostLowLevelDeviceCreate(size_t num_bytes) {\n",
        "  std::shared_ptr<LowLevelDevice> lld =\n",
        "      std::make_shared<HostLowLevelDevice>(num_bytes);\n",
        "  return lld;\n",
        "}\n",
        "}  // namespace runtime\n",
        "}  // namespace tvm\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g8dibeuWH8Nt",
        "colab_type": "text"
      },
      "source": [
        "# Homework: RISC-V\n",
        "\n",
        "To use RISC-V as a target device, there are some extra steps that can't be done within this notebook.\n",
        "\n",
        "First, you will need to download and compile [TVM](https://github.com/dmlc/tvm) on your own machine.\n",
        "\n",
        "Next, you will need [Spike](https://github.com/riscv/riscv-isa-sim) (a RISC-V ISA simulator) and [OpenOCD](https://github.com/ntfreak/openocd) (provides a high-level debugging interface to compatible devices).\n",
        "\n",
        "TODO: Flesh out."
      ]
    }
  ]
}