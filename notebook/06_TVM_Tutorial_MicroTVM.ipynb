{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "06_TVM_Tutorial_MicroTVM.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/uwsampl/tutorial/blob/master/notebook/06_TVM_Tutorial_MicroTVM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xFoF4FhyYcwE",
        "colab_type": "text"
      },
      "source": [
        "Please run the following block to ensure TVM is setup for *this notebook*.  Each notebook may have its own runtime."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ce5eshWvUiyN",
        "colab_type": "code",
        "outputId": "218ff877-13e8-47d6-a96e-101698c28edd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3336
        }
      },
      "source": [
        "try:\n",
        "  import google.colab\n",
        "  IN_COLAB = True\n",
        "except:\n",
        "  IN_COLAB = False\n",
        "\n",
        "if IN_COLAB:\n",
        "    ! gsutil cp \"gs://tvm-fcrc-binariesd5fce43e-8373-11e9-bfb6-0242ac1c0002/tvm.tar.gz\" /tmp/tvm.tar.gz\n",
        "    ! mkdir -p /tvm\n",
        "    ! tar -xf /tmp/tvm.tar.gz --strip-components=4 --directory /tvm\n",
        "    ! ls -la /tvm\n",
        "    ! bash /tvm/package.sh\n",
        "    # Add TVM to the Python path.\n",
        "    import sys\n",
        "    sys.path.append('/tvm/python')\n",
        "    sys.path.append('/tvm/topi/python')\n",
        "    ! pip install mxnet\n",
        "else:\n",
        "    print('Notebook executing locally, skipping Colab setup ...')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Copying gs://tvm-fcrc-binariesd5fce43e-8373-11e9-bfb6-0242ac1c0002/tvm.tar.gz...\n",
            "- [1 files][119.5 MiB/119.5 MiB]                                                \n",
            "Operation completed over 1 objects/119.5 MiB.                                    \n",
            "total 164\n",
            "drwxr-xr-x 21 root root  4096 Jun 22 07:30 .\n",
            "drwxr-xr-x  1 root root  4096 Jun 22 07:30 ..\n",
            "drwx------  8 root root  4096 May 31 08:14 3rdparty\n",
            "drwx------ 12 root root  4096 May 31 08:14 apps\n",
            "drwx------  3 root root  4096 Jun 19 07:58 build\n",
            "drwx------  4 root root  4096 May 31 08:14 cmake\n",
            "-rw-------  1 root root 11053 Jun 19 04:54 CMakeLists.txt\n",
            "drwx------  6 root root  4096 May 31 08:14 conda\n",
            "-rw-------  1 root root  5736 Jun 19 04:54 CONTRIBUTORS.md\n",
            "drwx------  3 root root  4096 May 31 08:14 docker\n",
            "drwx------ 11 root root  4096 May 31 08:14 docs\n",
            "drwx------  4 root root  4096 May 31 08:14 golang\n",
            "drwx------  3 root root  4096 May 31 08:14 include\n",
            "-rw-------  1 root root 10607 Jun 19 04:54 Jenkinsfile\n",
            "drwx------  6 root root  4096 May 31 08:14 jvm\n",
            "-rw-------  1 root root 11357 Jun 19 04:54 LICENSE\n",
            "-rw-------  1 root root  4267 Jun 19 04:54 Makefile\n",
            "-rw-------  1 root root 10476 Jun 19 04:54 NEWS.md\n",
            "drwx------  9 root root  4096 May 31 08:14 nnvm\n",
            "-rw-------  1 root root    61 Jun 19 04:54 NOTICE\n",
            "-rwx------  1 root root   374 Jun 19 04:57 package.sh\n",
            "drwx------  3 root root  4096 May 31 08:14 python\n",
            "-rw-------  1 root root  2705 Jun 19 04:54 README.md\n",
            "drwx------  6 root root  4096 May 31 08:14 rust\n",
            "drwx------ 14 root root  4096 May 31 08:14 src\n",
            "drwx------  9 root root  4096 May 31 08:14 tests\n",
            "drwx------  7 root root  4096 May 31 08:14 topi\n",
            "drwx------  8 root root  4096 May 31 08:14 tutorials\n",
            "-rw-------  1 root root  2902 Jun 19 05:04 version.py\n",
            "drwx------ 10 root root  4096 May 31 08:14 vta\n",
            "drwx------  2 root root  4096 May 31 08:14 web\n",
            "Installing Dependencies ...\n",
            "deb https://dl.bintray.com/sbt/debian /\n",
            "Executing: /tmp/apt-key-gpghome.32cT2E05Jg/gpg.1.sh --keyserver hkp://keyserver.ubuntu.com:80 --recv 2EE0EA64E40A89B84B2DF73499E82A75642AC823\n",
            "gpg: key 99E82A75642AC823: public key \"sbt build tool <scalasbt@gmail.com>\" imported\n",
            "gpg: Total number processed: 1\n",
            "gpg:               imported: 1\n",
            "Ign:1 https://dl.bintray.com/sbt/debian  InRelease\n",
            "Ign:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Get:3 https://dl.bintray.com/sbt/debian  Release [815 B]\n",
            "Get:4 https://dl.bintray.com/sbt/debian  Release.gpg [821 B]\n",
            "Ign:5 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
            "Hit:7 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Get:8 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Hit:9 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Hit:10 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Get:11 https://dl.bintray.com/sbt/debian  Packages [3,424 B]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Hit:15 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic InRelease\n",
            "Get:16 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran35/ InRelease [3,626 B]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Get:18 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [553 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [1,225 kB]\n",
            "Get:20 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [719 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [856 kB]\n",
            "Get:22 http://archive.ubuntu.com/ubuntu bionic-backports/universe amd64 Packages [3,927 B]\n",
            "Fetched 3,617 kB in 2s (1,686 kB/s)\n",
            "Reading package lists... Done\n",
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "zlib1g-dev is already the newest version (1:1.2.11.dfsg-0ubuntu2).\n",
            "zlib1g-dev set to manually installed.\n",
            "clinfo is already the newest version (2.2.18.03.26-1).\n",
            "libtinfo-dev is already the newest version (6.1-1ubuntu1.18.04).\n",
            "libtinfo-dev set to manually installed.\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-410\n",
            "Use 'sudo apt autoremove' to remove it.\n",
            "Suggested packages:\n",
            "  llvm-6.0-doc\n",
            "The following NEW packages will be installed:\n",
            "  binfmt-support libffi-dev llvm-6.0 llvm-6.0-dev llvm-6.0-runtime tree\n",
            "0 upgraded, 6 newly installed, 0 to remove and 26 not upgraded.\n",
            "Need to get 28.3 MB of archives.\n",
            "After this operation, 178 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/main amd64 binfmt-support amd64 2.1.8-2 [51.6 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/main amd64 llvm-6.0-runtime amd64 1:6.0-1ubuntu2 [200 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic/main amd64 llvm-6.0 amd64 1:6.0-1ubuntu2 [4,838 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic/main amd64 libffi-dev amd64 3.2.1-8 [156 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu bionic/main amd64 llvm-6.0-dev amd64 1:6.0-1ubuntu2 [23.0 MB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu bionic/universe amd64 tree amd64 1.7.0-5 [40.7 kB]\n",
            "Fetched 28.3 MB in 3s (10.3 MB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 6.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package binfmt-support.\n",
            "(Reading database ... 130942 files and directories currently installed.)\n",
            "Preparing to unpack .../0-binfmt-support_2.1.8-2_amd64.deb ...\n",
            "Unpacking binfmt-support (2.1.8-2) ...\n",
            "Selecting previously unselected package llvm-6.0-runtime.\n",
            "Preparing to unpack .../1-llvm-6.0-runtime_1%3a6.0-1ubuntu2_amd64.deb ...\n",
            "Unpacking llvm-6.0-runtime (1:6.0-1ubuntu2) ...\n",
            "Selecting previously unselected package llvm-6.0.\n",
            "Preparing to unpack .../2-llvm-6.0_1%3a6.0-1ubuntu2_amd64.deb ...\n",
            "Unpacking llvm-6.0 (1:6.0-1ubuntu2) ...\n",
            "Selecting previously unselected package libffi-dev:amd64.\n",
            "Preparing to unpack .../3-libffi-dev_3.2.1-8_amd64.deb ...\n",
            "Unpacking libffi-dev:amd64 (3.2.1-8) ...\n",
            "Selecting previously unselected package llvm-6.0-dev.\n",
            "Preparing to unpack .../4-llvm-6.0-dev_1%3a6.0-1ubuntu2_amd64.deb ...\n",
            "Unpacking llvm-6.0-dev (1:6.0-1ubuntu2) ...\n",
            "Selecting previously unselected package tree.\n",
            "Preparing to unpack .../5-tree_1.7.0-5_amd64.deb ...\n",
            "Unpacking tree (1.7.0-5) ...\n",
            "Setting up binfmt-support (2.1.8-2) ...\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/binfmt-support.service → /lib/systemd/system/binfmt-support.service.\n",
            "invoke-rc.d: could not determine current runlevel\n",
            "invoke-rc.d: policy-rc.d denied execution of start.\n",
            "Setting up tree (1.7.0-5) ...\n",
            "Setting up libffi-dev:amd64 (3.2.1-8) ...\n",
            "Setting up llvm-6.0-runtime (1:6.0-1ubuntu2) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1) ...\n",
            "Setting up llvm-6.0 (1:6.0-1ubuntu2) ...\n",
            "Processing triggers for systemd (237-3ubuntu10.22) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Setting up llvm-6.0-dev (1:6.0-1ubuntu2) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1) ...\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-410\n",
            "Use 'sudo apt autoremove' to remove it.\n",
            "Suggested packages:\n",
            "  gtkwave systemc\n",
            "The following NEW packages will be installed:\n",
            "  sbt verilator\n",
            "0 upgraded, 2 newly installed, 0 to remove and 26 not upgraded.\n",
            "Need to get 4,005 kB of archives.\n",
            "After this operation, 14.4 MB of additional disk space will be used.\n",
            "Get:1 https://dl.bintray.com/sbt/debian  sbt 1.2.8 [1,126 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/universe amd64 verilator amd64 3.916-1build1 [2,878 kB]\n",
            "Fetched 4,005 kB in 1s (2,905 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 2.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package sbt.\n",
            "(Reading database ... 132583 files and directories currently installed.)\n",
            "Preparing to unpack .../apt/archives/sbt_1.2.8_all.deb ...\n",
            "Unpacking sbt (1.2.8) ...\n",
            "Selecting previously unselected package verilator.\n",
            "Preparing to unpack .../verilator_3.916-1build1_amd64.deb ...\n",
            "Unpacking verilator (3.916-1build1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Setting up verilator (3.916-1build1) ...\n",
            "Setting up sbt (1.2.8) ...\n",
            "Creating system group: sbt\n",
            "Creating system user: sbt in sbt with sbt daemon-user and shell /bin/false\n",
            "Collecting mxnet\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/58/f4/bc147a1ba7175f9890523ff8f1a928a43ac8a79d5897a067158cac4d092f/mxnet-1.4.1-py2.py3-none-manylinux1_x86_64.whl (28.4MB)\n",
            "\u001b[K     |████████████████████████████████| 28.4MB 46.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests>=2.20.0 in /usr/local/lib/python3.6/dist-packages (from mxnet) (2.21.0)\n",
            "Collecting graphviz<0.9.0,>=0.8.1 (from mxnet)\n",
            "  Downloading https://files.pythonhosted.org/packages/53/39/4ab213673844e0c004bed8a0781a0721a3f6bb23eb8854ee75c236428892/graphviz-0.8.4-py2.py3-none-any.whl\n",
            "Collecting numpy<1.15.0,>=1.8.2 (from mxnet)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/c4/395ebb218053ba44d64935b3729bc88241ec279915e72100c5979db10945/numpy-1.14.6-cp36-cp36m-manylinux1_x86_64.whl (13.8MB)\n",
            "\u001b[K     |████████████████████████████████| 13.8MB 42.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20.0->mxnet) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20.0->mxnet) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20.0->mxnet) (2019.3.9)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20.0->mxnet) (2.8)\n",
            "\u001b[31mERROR: spacy 2.1.4 has requirement numpy>=1.15.0, but you'll have numpy 1.14.6 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: imgaug 0.2.9 has requirement numpy>=1.15.0, but you'll have numpy 1.14.6 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: fastai 1.0.53.post2 has requirement numpy>=1.15, but you'll have numpy 1.14.6 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: blis 0.2.4 has requirement numpy>=1.15.0, but you'll have numpy 1.14.6 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: graphviz, numpy, mxnet\n",
            "  Found existing installation: graphviz 0.10.1\n",
            "    Uninstalling graphviz-0.10.1:\n",
            "      Successfully uninstalled graphviz-0.10.1\n",
            "  Found existing installation: numpy 1.16.4\n",
            "    Uninstalling numpy-1.16.4:\n",
            "      Successfully uninstalled numpy-1.16.4\n",
            "Successfully installed graphviz-0.8.4 mxnet-1.4.1 numpy-1.14.6\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hHFxmiOzjFF2",
        "colab_type": "text"
      },
      "source": [
        "# μTVM: Deep Learning on Bare-Metal Devices"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RHHOBXwRuJGl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import pprint\n",
        "\n",
        "import numpy as np\n",
        "import tvm\n",
        "from tvm.contrib import graph_runtime, util\n",
        "from tvm import relay\n",
        "import tvm.micro as micro"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CLTjoVHpW3ex",
        "colab_type": "text"
      },
      "source": [
        "The proliferation of low-cost, AI-powered IoT devices has led to widespread interest in running ML on \"bare-metal\" (lacking an operating system) devices among ML researchers and practitioners. On the other hand, computer architects building custom ML hardware need a way to rapidly experiment with and optimize an increasing variety of ML models on their hardware. μTVM brings rapid deployment and optimization of deep learning code generated by TVM to such bare-metal devices.\n",
        "\n",
        "This tutorial aims to provide a broad understanding of how μTVM works, and primarily focuses on the steps you need to follow to support μTVM on your own bare-metal device.\n",
        "\n",
        "In this notebook, we're going to cover:\n",
        "\n",
        "*   the C code generation backend\n",
        "*   the cross-compiler interface\n",
        "*   the low-level device interface\n",
        "*   the graph runtime\n",
        "*   a demo of ResNeT-18 using the graph runtime"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-jpZPaMcVX8",
        "colab_type": "text"
      },
      "source": [
        "**Note**: We have only tested μTVM using a Linux host machine.  It may or may not work on Mac.\n",
        "\n",
        "**Disclaimer**: μTVM is still experimental and its API may change in the future."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ERvCOpVjsn_U",
        "colab_type": "text"
      },
      "source": [
        "## C Codegen\n",
        "\n",
        "If we're going to work with bare-metal devices, we need some type of code generation backend that targets them.  At first glance, LLVM seems like a great option, but unfortunately, we can't use it because bare-metal devices often lack LLVM support.\n",
        "\n",
        "However, most bare-metal devices ***do*** support C.  We have taken advantage of this fact by developing a C code generation backend for TVM.  With this backend, any programs expressible in TVM's low-level IR can be compiled into C.\n",
        "\n",
        "Note that although we made this choice because most devices can run C code, it is still possible to use LLVM or any other code generation backend supported by TVM if your target bare-metal device supports it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aEGvx5OVymcD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Builds a C source module from a Relay function.\n",
        "def build_c_module(func):\n",
        "  \"\"\"Builds a C source module by compiling `func` with Relay.\"\"\"\n",
        "  # Note: μTVM currently does not support vectorized instructions.\n",
        "  with tvm.build_config(disable_vectorize=True):\n",
        "    # We set `target` to \"c\", and the build is now routed to the C backend.\n",
        "    _, src_mod, _ = relay.build(func, target='c', params={})\n",
        "  return src_mod"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uKCijhKQFFOi",
        "colab_type": "text"
      },
      "source": [
        "Now, let's write a simple Relay program to add two vectors and examine the generated C code.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fAozo7Slus7I",
        "colab_type": "code",
        "outputId": "7f445354-c07e-4762-fd0a-01f3eeeb05b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "# First, we use Relay to construct our function which computes `x + y`.\n",
        "ty = relay.TensorType(shape=(1024,), dtype='float32')\n",
        "x = relay.var('x', ty)\n",
        "y = relay.var('y', ty)\n",
        "func = relay.Function([x, y], relay.add(x, y))\n",
        "print(func)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "v0.0.1\n",
            "fn (%x: Tensor[(1024,), float32], %y: Tensor[(1024,), float32]) {\n",
            "  add(%x, %y)\n",
            "}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dWh8sKdU3ecf",
        "colab_type": "code",
        "outputId": "269755f7-69e3-4522-874f-4a9b40330bea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2315
        }
      },
      "source": [
        "# Then build it into a C source module.\n",
        "src_mod = build_c_module(func)\n",
        "# Now, we can see the source that was generated.\n",
        "print(src_mod.get_source())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "#include \"tvm/runtime/c_runtime_api.h\"\n",
            "#include \"tvm/runtime/c_backend_api.h\"\n",
            "#include \"tvm/runtime/micro/utvm_device_lib.h\"\n",
            "extern void* __tvm_module_ctx = NULL;\n",
            "#ifdef __cplusplus\n",
            "extern \"C\"\n",
            "#endif\n",
            "TVM_DLL int32_t fused_add( void* args,  void* arg_type_ids, int32_t num_args) {\n",
            "  if (!((num_args == 3))) {\n",
            "    TVMAPISetLastError(\"fused_add: num_args should be 3\");\n",
            "    return -1;\n",
            "  }\n",
            "  void* arg0 = (((TVMValue*)args)[0].v_handle);\n",
            "  int32_t arg0_code = (( int32_t*)arg_type_ids)[0];\n",
            "  void* arg1 = (((TVMValue*)args)[1].v_handle);\n",
            "  int32_t arg1_code = (( int32_t*)arg_type_ids)[1];\n",
            "  void* arg2 = (((TVMValue*)args)[2].v_handle);\n",
            "  int32_t arg2_code = (( int32_t*)arg_type_ids)[2];\n",
            "  float* placeholder = (float*)(((TVMArray*)arg0)[0].data);\n",
            "  int64_t* arg0_shape = (int64_t*)(((TVMArray*)arg0)[0].shape);\n",
            "  int64_t* arg0_strides = (int64_t*)(((TVMArray*)arg0)[0].strides);\n",
            "  if (!(arg0_strides == NULL)) {\n",
            "    if (!((1 == ((int32_t)arg0_strides[0])))) {\n",
            "      TVMAPISetLastError(\"arg0.strides: expected to be compact array\");\n",
            "      return -1;\n",
            "    }\n",
            "  }\n",
            "  int32_t dev_type = (((TVMArray*)arg0)[0].ctx.device_type);\n",
            "  int32_t dev_id = (((TVMArray*)arg0)[0].ctx.device_id);\n",
            "  float* placeholder1 = (float*)(((TVMArray*)arg1)[0].data);\n",
            "  int64_t* arg1_shape = (int64_t*)(((TVMArray*)arg1)[0].shape);\n",
            "  int64_t* arg1_strides = (int64_t*)(((TVMArray*)arg1)[0].strides);\n",
            "  if (!(arg1_strides == NULL)) {\n",
            "    if (!((1 == ((int32_t)arg1_strides[0])))) {\n",
            "      TVMAPISetLastError(\"arg1.strides: expected to be compact array\");\n",
            "      return -1;\n",
            "    }\n",
            "  }\n",
            "  float* T_add = (float*)(((TVMArray*)arg2)[0].data);\n",
            "  int64_t* arg2_shape = (int64_t*)(((TVMArray*)arg2)[0].shape);\n",
            "  int64_t* arg2_strides = (int64_t*)(((TVMArray*)arg2)[0].strides);\n",
            "  if (!(arg2_strides == NULL)) {\n",
            "    if (!((1 == ((int32_t)arg2_strides[0])))) {\n",
            "      TVMAPISetLastError(\"arg2.strides: expected to be compact array\");\n",
            "      return -1;\n",
            "    }\n",
            "  }\n",
            "  if (!(((((arg0_code == 3) || (arg0_code == 13)) || (arg0_code == 7)) || (arg0_code == 4)))) {\n",
            "    TVMAPISetLastError(\"fused_add: Expect arg[0] to be pointer\");\n",
            "    return -1;\n",
            "  }\n",
            "  if (!(((((arg1_code == 3) || (arg1_code == 13)) || (arg1_code == 7)) || (arg1_code == 4)))) {\n",
            "    TVMAPISetLastError(\"fused_add: Expect arg[1] to be pointer\");\n",
            "    return -1;\n",
            "  }\n",
            "  if (!(((((arg2_code == 3) || (arg2_code == 13)) || (arg2_code == 7)) || (arg2_code == 4)))) {\n",
            "    TVMAPISetLastError(\"fused_add: Expect arg[2] to be pointer\");\n",
            "    return -1;\n",
            "  }\n",
            "  if (!((dev_type == 1))) {\n",
            "    TVMAPISetLastError(\"device_type need to be 1\");\n",
            "    return -1;\n",
            "  }\n",
            "  if (!((1 == (((TVMArray*)arg0)[0].ndim)))) {\n",
            "    TVMAPISetLastError(\"arg0.ndim is expected to equal 1\");\n",
            "    return -1;\n",
            "  }\n",
            "  if (!(((((((TVMArray*)arg0)[0].dtype.code) == (uint8_t)2) && ((((TVMArray*)arg0)[0].dtype.bits) == (uint8_t)32)) && ((((TVMArray*)arg0)[0].dtype.lanes) == (uint16_t)1)))) {\n",
            "    TVMAPISetLastError(\"arg0.dtype is expected to be float32\");\n",
            "    return -1;\n",
            "  }\n",
            "  if (!((((int32_t)arg0_shape[0]) == 1024))) {\n",
            "    TVMAPISetLastError(\"Argument arg0.shape[0] has an unsatisfied constraint\");\n",
            "    return -1;\n",
            "  }\n",
            "  if (!(((((TVMArray*)arg0)[0].byte_offset) == (uint64_t)0))) {\n",
            "    TVMAPISetLastError(\"Argument arg0.byte_offset has an unsatisfied constraint\");\n",
            "    return -1;\n",
            "  }\n",
            "  if (!((1 == (((TVMArray*)arg1)[0].ndim)))) {\n",
            "    TVMAPISetLastError(\"arg1.ndim is expected to equal 1\");\n",
            "    return -1;\n",
            "  }\n",
            "  if (!(((((((TVMArray*)arg1)[0].dtype.code) == (uint8_t)2) && ((((TVMArray*)arg1)[0].dtype.bits) == (uint8_t)32)) && ((((TVMArray*)arg1)[0].dtype.lanes) == (uint16_t)1)))) {\n",
            "    TVMAPISetLastError(\"arg1.dtype is expected to be float32\");\n",
            "    return -1;\n",
            "  }\n",
            "  if (!((((int32_t)arg1_shape[0]) == 1024))) {\n",
            "    TVMAPISetLastError(\"Argument arg1.shape[0] has an unsatisfied constraint\");\n",
            "    return -1;\n",
            "  }\n",
            "  if (!(((((TVMArray*)arg1)[0].byte_offset) == (uint64_t)0))) {\n",
            "    TVMAPISetLastError(\"Argument arg1.byte_offset has an unsatisfied constraint\");\n",
            "    return -1;\n",
            "  }\n",
            "  if (!((1 == (((TVMArray*)arg1)[0].ctx.device_type)))) {\n",
            "    TVMAPISetLastError(\"Argument arg1.device_type has an unsatisfied constraint\");\n",
            "    return -1;\n",
            "  }\n",
            "  if (!((dev_id == (((TVMArray*)arg1)[0].ctx.device_id)))) {\n",
            "    TVMAPISetLastError(\"Argument arg1.device_id has an unsatisfied constraint\");\n",
            "    return -1;\n",
            "  }\n",
            "  if (!((1 == (((TVMArray*)arg2)[0].ndim)))) {\n",
            "    TVMAPISetLastError(\"arg2.ndim is expected to equal 1\");\n",
            "    return -1;\n",
            "  }\n",
            "  if (!(((((((TVMArray*)arg2)[0].dtype.code) == (uint8_t)2) && ((((TVMArray*)arg2)[0].dtype.bits) == (uint8_t)32)) && ((((TVMArray*)arg2)[0].dtype.lanes) == (uint16_t)1)))) {\n",
            "    TVMAPISetLastError(\"arg2.dtype is expected to be float32\");\n",
            "    return -1;\n",
            "  }\n",
            "  if (!((((int32_t)arg2_shape[0]) == 1024))) {\n",
            "    TVMAPISetLastError(\"Argument arg2.shape[0] has an unsatisfied constraint\");\n",
            "    return -1;\n",
            "  }\n",
            "  if (!(((((TVMArray*)arg2)[0].byte_offset) == (uint64_t)0))) {\n",
            "    TVMAPISetLastError(\"Argument arg2.byte_offset has an unsatisfied constraint\");\n",
            "    return -1;\n",
            "  }\n",
            "  if (!((1 == (((TVMArray*)arg2)[0].ctx.device_type)))) {\n",
            "    TVMAPISetLastError(\"Argument arg2.device_type has an unsatisfied constraint\");\n",
            "    return -1;\n",
            "  }\n",
            "  if (!((dev_id == (((TVMArray*)arg2)[0].ctx.device_id)))) {\n",
            "    TVMAPISetLastError(\"Argument arg2.device_id has an unsatisfied constraint\");\n",
            "    return -1;\n",
            "  }\n",
            "  for (int32_t ax0 = 0; ax0 < 1024; ++ax0) {\n",
            "    T_add[ax0] = (placeholder[ax0] + placeholder1[ax0]);\n",
            "  }\n",
            "  return 0;\n",
            "}\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26Bm6r-FfyMQ",
        "colab_type": "text"
      },
      "source": [
        "## The Cross-Compiler Interface\n",
        "With the ability to generate C, we need access to a cross-compiler toolchain for our device, in order to create binaries for it.\n",
        "\n",
        "How you obtain the toolchain will vary, depending on the device you're targetting.  Once you have a toolchain, you need to make sure it's accessible from a terminal by adding it to your system's PATH variable.\n",
        "\n",
        "All of the binaries in the toolchain should have a common prefix.  For example, if you're targetting RISC-V, you might want \"gcc\" to be named \"riscv-gcc\", \"ld\" to be named \"riscv-ld\", etc., and in this case, the prefix is \"riscv-\".  Having a common prefix is important, because we use this prefix to tell μTVM which binaries to use for binary compilation/manipulation.\n",
        "\n",
        "In the example below, we create a session using the host emulated device type, which creates a region of memory on the host machine to emulate device memory.  Because this device type is running on the host machine, we don't need a cross-compiler, and in fact, we can use plain old \"gcc\".  In this case, there is no prefix on the toolchain, so we use the empty string."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kGtxTXRYfGlB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device_type = 'host'\n",
        "toolchain_prefix = ''\n",
        "with micro.Session(device_type, toolchain_prefix) as sess:\n",
        "  # Do μTVM things...\n",
        "  pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJJZRRnr0gGw",
        "colab_type": "text"
      },
      "source": [
        "## The Low-Level Device Interface\n",
        "Now we have the ability to generate binaries for our device, but we don't yet have a method of loading and running these binaries onto the device.  And even if we could run it, we couldn't read the outputs.  \n",
        "\n",
        "To satisfy these requirements, we require an implementation of a low-level device interface for every new device in μTVM.  The low-level device interface exposes three important operations to interact with the bare-metal device: \n",
        "- Reading from device memory\n",
        "- Writing to device memory\n",
        "- Starting function execution\n",
        "\n",
        "This interface appears in C++ as\n",
        "\n",
        "<img src=\"https://i.imgur.com/beF6Rm3.png\" alt=\"low-level device interface\" width=\"500\"/>\n",
        "\n",
        "In the above interface, `DevBaseOffset` is a wrapper type for a `uintptr_t` that represents an offset from the base address of the device.  The full definition of the inteface can be found [here](https://github.com/uwsampl/tvm/blob/tutorial/src/runtime/micro/low_level_device.h).\n",
        "\n",
        "To add support for your own MicroDevice in μTVM, you simply need to implement the low-level device interface for your MicroDevice.  Doing so will require some form of communication with the device.  For instance, communication may occur over [JTAG](https://en.wikipedia.org/wiki/JTAG) or over a network connection.\n",
        "\n",
        "### Host Low-Level Device\n",
        "To give an example of an implementation of this interface, we will walk through the already-existing `HostLowLevelDevice` (found [here](https://github.com/uwsampl/tvm/blob/tutorial/src/runtime/micro/host_low_level_device.cc)), because it's implementation is relatively simple.\n",
        "\n",
        "This device uses an allocated region of memory on the host machine to simulate a bare-metal device. It provides μTVM users with an easy setup to experiment with, without having to communicate with and debug external MicroDevices.\n",
        "\n",
        "#### Constructor\n",
        "To create a host device, we call `mmap` to allocate a memory region that is at least as large as the requested device size, and we assign read/write/execute permissions to that region.  The base address of the device is just the start address of the region.\n",
        "\n",
        "<img src=\"https://i.imgur.com/FVNSVnV.png\" alt=\"constructor implementation\" width=\"700\"/>\n",
        "\n",
        "#### Destructor\n",
        "To destroy a host device, we simply unmap the `mmap`'d region.\n",
        "\n",
        "<img src=\"https://i.imgur.com/rthH1Cv.png\" alt=\"destructor implementation\" width=\"400\"/>\n",
        "\n",
        "#### Read\n",
        "To read from the host device, we convert the device offset into a host virtual address, then use `memcpy` to transfer from the mapped memory into the output buffer `buf`.\n",
        "\n",
        "<img src=\"https://i.imgur.com/77lRlzZ.png\" alt=\"read implementation\" width=\"500\"/>\n",
        "\n",
        "#### Write\n",
        "Writing to the host device is similar to reading, except now `buf` serves as an input buffer.\n",
        "\n",
        "<img src=\"https://i.imgur.com/SEm4UWQ.png\" alt=\"write implementation\" width=\"500\"/>\n",
        "\n",
        "#### Execute\n",
        "For function execution, we are given an offset to the target function, but also a breakpoint indicating the memory offset at which code execution should stop.\n",
        "\n",
        "We don't need to use `breakpoint` here, because with the host device, control flow never leaves the host machine, so we can simply call the function as we would any other function.\n",
        "\n",
        "For real devices, we may need to use `breakpoint` to stop execution, because otherwise, the device may continue executing indefinitely.\n",
        "\n",
        "<img src=\"https://i.imgur.com/psuYqrt.png\" alt=\"execute implementation\" width=\"500\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqGxOMFp0NsB",
        "colab_type": "text"
      },
      "source": [
        "## Graph Runtime Execution\n",
        "\n",
        "The primary execution strategy for running models with μTVM is TVM's graph runtime.\n",
        "\n",
        "When we use the graph runtime, the host drives the control flow of the model.  For each node in the graph, the host calls out to the device to execute the corresponding operator for that node. Such a split execution framework is useful because it allows you to optimize kernels for your target bare-metal device easily using AutoTVM.\n",
        "\n",
        "Note that this means models do not run entirely on the device (i.e., they aren't self-hosted).  Fully self-hosted models are not currently supported, as this would require using the Relay ahead-of-time compiler, which is still experimental.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "looZwQcl2D9H",
        "colab_type": "code",
        "outputId": "1e4a2bb2-0521-4afb-80e5-a507e8aaded2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "shape = (1024,)\n",
        "dtype = 'float32'\n",
        "    \n",
        "# Construct Relay program.\n",
        "x = relay.var('x', relay.TensorType(shape=shape, dtype=dtype))\n",
        "xx = relay.multiply(x, x)\n",
        "z = relay.add(xx, relay.const(1.0))\n",
        "func = relay.Function([x], z)\n",
        "print(func)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "v0.0.1\n",
            "fn (%x: Tensor[(1024,), float32]) {\n",
            "  %0 = multiply(%x, %x)\n",
            "  add(%0, 1f)\n",
            "}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kISYiPHYZWSh",
        "colab_type": "code",
        "outputId": "79a93324-3a38-46af-a6cc-f4ff2419998a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "with micro.Session('host', '') as sess:\n",
        "  mod, params = sess.build(func)\n",
        "  mod.set_input(**params)\n",
        "\n",
        "  x_in = np.random.uniform(size=shape[0]).astype(dtype)\n",
        "  print(f'input:\\t\\t\\t{x_in}')\n",
        "  # Run with `x_in` as the input. \n",
        "  mod.run(x=x_in)\n",
        "  result = mod.get_output(0).asnumpy()\n",
        "\n",
        "  expected = (x_in * x_in) + 1.0\n",
        "  print()\n",
        "  print(f'expected result:\\t{expected}')\n",
        "  print(f'μTVM result:\\t\\t{result}')\n",
        "  print()\n",
        "  if np.array_equal(expected, result):\n",
        "    print('Looks good!')\n",
        "  else:\n",
        "    print('Something\\'s wrong!')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input:\t\t\t[0.20322835 0.07885458 0.3271423  ... 0.56848824 0.3425446  0.7226836 ]\n",
            "\n",
            "expected result:\t[1.0413017 1.0062181 1.107022  ... 1.3231789 1.1173368 1.5222716]\n",
            "μTVM result:\t\t[1.0413017 1.0062181 1.107022  ... 1.3231789 1.1173368 1.5222716]\n",
            "\n",
            "Looks good!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITy9wnt1WsN_",
        "colab_type": "text"
      },
      "source": [
        "## ResNet-18 Demo\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gk61wn9jvJDJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import io\n",
        "import mxnet as mx\n",
        "from mxnet.gluon.model_zoo.vision import get_model\n",
        "from mxnet.gluon.utils import download\n",
        "from PIL import Image\n",
        "import IPython.display\n",
        "\n",
        "RESNET_INPUT_IMG_SHAPE = (1, 3, 224, 224)\n",
        "\n",
        "def get_image():\n",
        "  img_name = 'cat.png'\n",
        "  download('https://github.com/dmlc/mxnet.js/blob/master/data/cat.png?raw=true',\n",
        "           img_name)\n",
        "  image = Image.open(img_name)\n",
        "  return image\n",
        "\n",
        "\n",
        "def show_image(image):\n",
        "  img_byte_arr = io.BytesIO()\n",
        "  image.save(img_byte_arr, format='png')\n",
        "  img_byte_arr = img_byte_arr.getvalue()\n",
        "  IPython.display.display(IPython.display.Image(img_byte_arr))\n",
        "\n",
        "\n",
        "def convert_image(image):\n",
        "  dtype = 'float32'\n",
        "  image = image.resize(RESNET_INPUT_IMG_SHAPE[2:])\n",
        "  image = np.array(image) - np.array([123., 117., 104.])\n",
        "  image /= np.array([58.395, 57.12, 57.375])\n",
        "  image = image.transpose((2, 0, 1))\n",
        "  image = image[np.newaxis, :]\n",
        "  image = tvm.nd.array(image.astype(dtype))\n",
        "  return image\n",
        "\n",
        "\n",
        "def get_resnet():    \n",
        "  block = get_model('resnet18_v1', pretrained=True)\n",
        "  module, params = relay.frontend.from_mxnet(\n",
        "    block, shape={'data': RESNET_INPUT_IMG_SHAPE})\n",
        "  func = module['main']\n",
        "  return func, params\n",
        "  \n",
        "  \n",
        "# Fetch a mapping from class IDs to human-readable labels.\n",
        "synset_url = ''.join(['https://gist.githubusercontent.com/zhreshold/',\n",
        "                      '4d0b62f3d01426887599d4f7ede23ee5/raw/',\n",
        "                      '596b27d23537e5a1b5751d2b0481ef172f58b539/',\n",
        "                      'imagenet1000_clsid_to_human.txt'])\n",
        "synset_name = 'synset.txt'\n",
        "download(synset_url, synset_name)\n",
        "with open(synset_name) as f:\n",
        "  synset = eval(f.read())\n",
        "\n",
        "    \n",
        "def get_predictions(mod):\n",
        "  output = mod.get_output(0).asnumpy()[0]\n",
        "  top_k = output.argsort()[-3:][::-1]\n",
        "  predictions = list(map(lambda x: synset[x], top_k))\n",
        "  return predictions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZAxIkBheYrOp",
        "colab_type": "text"
      },
      "source": [
        "It's great that we can prototype models and immediately run them on a device, but the function we just ran was pretty simple.  Let's try a more interesting example.\n",
        "\n",
        "In this section, we will run ResNet-18 on μTVM to perform image recognition.  And our example input is..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FrTSYq4wqc_N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "image = get_image()\n",
        "show_image(image)\n",
        "# Convert image to a format the model can understand.\n",
        "image = convert_image(image)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2EEuEtrjtcRj",
        "colab_type": "text"
      },
      "source": [
        "...a cat!\n",
        "\n",
        "Luckily, MxNet is one of the many frameworks that TVM can import models from, so we can use a pretrained ResNet-18 model from the Gluon Model Zoo.\n",
        "\n",
        "Below, we grab a relay function that expresses our model, along with its set of pretrained parameters.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t52Cr6rd7n81",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "resnet, params = get_resnet()\n",
        "print(resnet)\n",
        "print()\n",
        "pprint.pprint(list(params.keys()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RIGStzjDx8bx",
        "colab_type": "text"
      },
      "source": [
        "With that, we can create a μTVM session, build a graph runtime module, then run the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lAEJ7ceR7aF1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with micro.Session('host', '') as sess:\n",
        "  print('Building...')\n",
        "  module, params = sess.build(resnet, params=params)\n",
        "  module.set_input(**params)\n",
        "  print()\n",
        "  print('Running...', end='')\n",
        "  # Execute with `image` as the input.\n",
        "  module.run(data=image)\n",
        "  print()\n",
        "  print('Predictions')\n",
        "  for i, prediction in enumerate(get_predictions(module)):\n",
        "    print(f'  {i + 1}: {prediction}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "azzGhNPY2QIJ",
        "colab_type": "text"
      },
      "source": [
        "# Additional Resources\n",
        "\n",
        "For more details on how μTVM is implemented, you can look into the following pull requests on the [TVM GitHub](https://github.com/dmlc/tvm) repository: \n",
        "\n",
        "- [C code generation backend](https://github.com/dmlc/tvm/pull/2161)\n",
        "- [μTVM RFC](https://github.com/dmlc/tvm/issues/2563) (somewhat outdated)\n",
        "- [μTVM on host-emulated device](https://github.com/dmlc/tvm/pull/3227)\n",
        "- μTVM on RISC-V: Will be upstreamed in a few weeks!\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}