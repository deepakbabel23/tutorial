{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "06_TVM_Tutorial_MicroTVM.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/uwsampl/tutorial/blob/master/notebook/06_TVM_Tutorial_MicroTVM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xFoF4FhyYcwE",
        "colab_type": "text"
      },
      "source": [
        "Please run the following block to ensure TVM is setup for *this notebook*.  Each notebook may have its own runtime."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ce5eshWvUiyN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "try:\n",
        "  import google.colab\n",
        "  IN_COLAB = True\n",
        "except:\n",
        "  IN_COLAB = False\n",
        "\n",
        "if IN_COLAB:\n",
        "    ! gsutil cp \"gs://tvm-fcrc-binariesd5fce43e-8373-11e9-bfb6-0242ac1c0002/tvm.tar.gz\" /tmp/tvm.tar.gz\n",
        "    ! mkdir -p /tvm\n",
        "    ! tar -xf /tmp/tvm.tar.gz --strip-components=4 --directory /tvm\n",
        "    ! ls -la /tvm\n",
        "    ! bash /tvm/package.sh\n",
        "    # Add TVM to the Python path.\n",
        "    import sys\n",
        "    sys.path.append('/tvm/python')\n",
        "    sys.path.append('/tvm/topi/python')\n",
        "    ! pip install mxnet\n",
        "else:\n",
        "    print('Notebook executing locally, skipping Colab setup ...')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hHFxmiOzjFF2",
        "colab_type": "text"
      },
      "source": [
        "# μTVM: Deep Learning on Bare-Metal Devices"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RHHOBXwRuJGl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import pprint\n",
        "\n",
        "import numpy as np\n",
        "import tvm\n",
        "from tvm.contrib import graph_runtime, util\n",
        "from tvm import relay\n",
        "import tvm.micro as micro"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CLTjoVHpW3ex",
        "colab_type": "text"
      },
      "source": [
        "The proliferation of low-cost, AI-powered consumer devices has lead to widespread interest in \"bare-metal\" (low-power, often without an operating system) devices among ML researchers and practitioners.\n",
        "\n",
        "μTVM brings deep learning code generated by TVM to bare-metal devices that lack mature software stacks or do not have an operating system. Here's a high-level overview of how μTVM operates:\n",
        "\n",
        "This tutorial aims to provide a broad understanding of how μTVM works, and primarily focuses on how you can use μTVM to run TVM-generated code on your own bare-metal device.\n",
        "\n",
        "In this notebook, we're going to cover:\n",
        "\n",
        "*   the C code generation backend\n",
        "*   the cross-compiler interface\n",
        "*   the low-level device interface\n",
        "*   the graph runtime\n",
        "*   a demo of ResNeT-18 using the graph runtime\n",
        "\n",
        "\n",
        "There are three requirements for using μTVM:\n",
        "- A TVM model written in Python\n",
        "- A low-level device interface to the target device\n",
        "- A cross-compiler for the target device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-jpZPaMcVX8",
        "colab_type": "text"
      },
      "source": [
        "**Note**: We have only tested μTVM using a Linux host machine.  It may or may not work on Mac.\n",
        "\n",
        "**Disclaimer**: μTVM is still experimental and its API may change in the future."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ERvCOpVjsn_U",
        "colab_type": "text"
      },
      "source": [
        "## C Codegen\n",
        "\n",
        "If we're going to work with bare-metal devices, we need some type of code generation backend that targets them.  At first glance, LLVM seems like a great option, but unfortunately, we can't use it.  Bare-metal devices often lack support for common languages and IRs.\n",
        "\n",
        "However, most bare-metal devices ***do*** support C.  We have taken advantage of this fact by developing a C code generation backend for TVM.  With this backend, any programs expressible in TVM's low-level IR can be compiled into C.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aEGvx5OVymcD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_c_module(func):\n",
        "  \"\"\"Builds a C source module by compiling `func` with Relay.\"\"\"\n",
        "  # Note: μTVM currently does not support vectorized instructions.\n",
        "  with tvm.build_config(disable_vectorize=True):\n",
        "    # We set `target` to \"c\", and the build is now routed to the C backend.\n",
        "    _, src_mod, _ = relay.build(func, target='c', params={})\n",
        "  return src_mod"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uKCijhKQFFOi",
        "colab_type": "text"
      },
      "source": [
        "Now, let's write a simple Relay program and examine the generated code.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fAozo7Slus7I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# First, we use Relay to construct our function which computes `x + y`.\n",
        "ty = relay.TensorType(shape=(1024,), dtype='float32')\n",
        "x = relay.var('x', ty)\n",
        "y = relay.var('y', ty)\n",
        "func = relay.Function([x, y], relay.add(x, y))\n",
        "print(func)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dWh8sKdU3ecf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Then build it into a C source module.\n",
        "src_mod = build_c_module(func)\n",
        "# Now, we can see the source that was generated.\n",
        "print(src_mod.get_source())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26Bm6r-FfyMQ",
        "colab_type": "text"
      },
      "source": [
        "## The Cross-Compiler Interface\n",
        "With the ability to generate C, we need access to a cross-compiler toolchain for our device, in order to create binaries for it.\n",
        "\n",
        "How you obtain the toolchain will vary, depending on the device you're targetting.  Once you have a toolchain, you need to make sure it's accessible from a terminal by adding it to your system's PATH variable.\n",
        "\n",
        "All of the binaries in the toolchain should have a common prefix.  For example, if you're targetting RISC-V, you might want \"gcc\" to be named \"riscv-gcc\", \"ld\" to be named \"riscv-ld\", etc., and in this case, the prefix is \"riscv-\".  Having a common prefix is important, because we use this prefix to tell μTVM which binaries to use for binary compilation/manipulation.\n",
        "\n",
        "In the example below, we create a session using the host emulated device type, which creates a region of memory on the host machine to emulate device memory.  Because this device type is running on the host machine, we don't need a cross-compiler, and in fact, we can use plain old \"gcc\".  In this case, there is no prefix on the toolchain, so we use the empty string."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kGtxTXRYfGlB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device_type = 'host'\n",
        "toolchain_prefix = ''\n",
        "with micro.Session(device_type, toolchain_prefix) as sess:\n",
        "  # Do μTVM things...\n",
        "  pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJJZRRnr0gGw",
        "colab_type": "text"
      },
      "source": [
        "## The Low-Level Device Interface\n",
        "Now we have the ability to generate binaries for our device, but we don't yet have a method of loading and running these binaries onto the device.  And even if we could run it, we couldn't read the outputs.  \n",
        "\n",
        "To satisfy these requirements, we require an implementation of a low-level device interface for every device in μTVM.  The low-level device interface exposes three important operations to interact with the MicroDevice: \n",
        "- Reading from device memory\n",
        "- Writing to device memory\n",
        "- Beginning function execution\n",
        "\n",
        "This interface appears in C++ as\n",
        "\n",
        "<img src=\"https://i.imgur.com/beF6Rm3.png\" alt=\"low-level device interface\" width=\"500\"/>\n",
        "\n",
        "In the above interface, `DevBaseOffset` is a wrapper type for a `uintptr_t` that represents an offset from the base address of the device.  The full definition of the inteface can be found [here](https://github.com/uwsampl/tvm/blob/tutorial/src/runtime/micro/low_level_device.h).\n",
        "\n",
        "To add support for your own MicroDevice in μTVM, you simply need to implement the low-level device interface for your MicroDevice.  Doing so will require some form of communication with the device.  For instance, communication may occur over [JTAG](https://en.wikipedia.org/wiki/JTAG) or over a network connection.\n",
        "\n",
        "### Host Low-Level Device\n",
        "To give an example of an implementation of this interface, we will walk through the already-existing `HostLowLevelDevice` (found [here](https://github.com/uwsampl/tvm/blob/tutorial/src/runtime/micro/host_low_level_device.cc)), because it's implementation is relatively simple.\n",
        "\n",
        "This device uses an allocated region of memory on the host machine to simulate a MicroDevice. This device provides μTVM users with an easy setup to experiment with, without having to communicate with and debug external MicroDevices.\n",
        "\n",
        "#### Constructor\n",
        "To create a host device, we call `mmap` to allocate a memory region that is at least as large as the requested device size, and we assign read/write/execute permissions to that region.  The base address of the device is just the start address of the region.\n",
        "\n",
        "<img src=\"https://i.imgur.com/FVNSVnV.png\" alt=\"constructor implementation\" width=\"700\"/>\n",
        "\n",
        "#### Destructor\n",
        "To destroy a host device, we simply unmap the `mmap`'d region.\n",
        "\n",
        "<img src=\"https://i.imgur.com/rthH1Cv.png\" alt=\"destructor implementation\" width=\"400\"/>\n",
        "\n",
        "#### Read\n",
        "To read from the host device, we convert the device offset into a host virtual address, then use `memcpy` to transfer from the mapped memory into the output buffer `buf`.\n",
        "\n",
        "<img src=\"https://i.imgur.com/77lRlzZ.png\" alt=\"read implementation\" width=\"500\"/>\n",
        "\n",
        "#### Write\n",
        "Writing to the host device is similar to reading, except now `buf` serves as an input buffer.\n",
        "\n",
        "<img src=\"https://i.imgur.com/SEm4UWQ.png\" alt=\"write implementation\" width=\"500\"/>\n",
        "\n",
        "#### Execute\n",
        "For function execution, we are given an offset to the target function, but also a breakpoint indicating the memory offset at which code execution should stop.\n",
        "\n",
        "We don't need to use `breakpoint` here, because with the host device, control flow never leaves the host machine, so we can simply call the function as we would any other function.\n",
        "\n",
        "For real devices, we may need to use `breakpoint` to stop execution, because otherwise, the device may continue executing indefinitely.\n",
        "\n",
        "<img src=\"https://i.imgur.com/psuYqrt.png\" alt=\"execute implementation\" width=\"500\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqGxOMFp0NsB",
        "colab_type": "text"
      },
      "source": [
        "## Graph Runtime Execution\n",
        "\n",
        "The primary execution strategy for running models with μTVM is the graph runtime.\n",
        "\n",
        "When we use the graph runtime, the host drives the control flow of the model.  For each node in the graph, the host calls out to the device to execute the corresponding operator for that node.\n",
        "\n",
        "Note that this means models do not run entirely on the device (i.e., they aren't self-hosted).  Fully self-hosted models are not currently supported, as this would require using the Relay ahead-of-time compiler, which is still experimental.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "looZwQcl2D9H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "shape = (1024,)\n",
        "dtype = 'float32'\n",
        "    \n",
        "# Construct Relay program.\n",
        "x = relay.var('x', relay.TensorType(shape=shape, dtype=dtype))\n",
        "xx = relay.multiply(x, x)\n",
        "z = relay.add(xx, relay.const(1.0))\n",
        "func = relay.Function([x], z)\n",
        "print(func)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kISYiPHYZWSh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with micro.Session('host', '') as sess:\n",
        "  mod, params = sess.build(func)\n",
        "  mod.set_input(**params)\n",
        "\n",
        "  x_in = np.random.uniform(size=shape[0]).astype(dtype)\n",
        "  print(f'input:\\t\\t\\t{x_in}')\n",
        "  # Run with `x_in` as the input. \n",
        "  mod.run(x=x_in)\n",
        "  result = mod.get_output(0).asnumpy()\n",
        "\n",
        "  expected = (x_in * x_in) + 1.0\n",
        "  print()\n",
        "  print(f'expected result:\\t{expected}')\n",
        "  print(f'μTVM result:\\t\\t{result}')\n",
        "  print()\n",
        "  if np.array_equal(expected, result):\n",
        "    print('Looks good!')\n",
        "  else:\n",
        "    print('Something\\'s wrong!')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITy9wnt1WsN_",
        "colab_type": "text"
      },
      "source": [
        "## ResNet-18 Demo\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gk61wn9jvJDJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import io\n",
        "import mxnet as mx\n",
        "from mxnet.gluon.model_zoo.vision import get_model\n",
        "from mxnet.gluon.utils import download\n",
        "from PIL import Image\n",
        "import IPython.display\n",
        "\n",
        "RESNET_INPUT_IMG_SHAPE = (1, 3, 224, 224)\n",
        "\n",
        "def get_image():\n",
        "  img_name = 'cat.png'\n",
        "  download('https://github.com/dmlc/mxnet.js/blob/master/data/cat.png?raw=true',\n",
        "           img_name)\n",
        "  image = Image.open(img_name)\n",
        "  return image\n",
        "\n",
        "\n",
        "def show_image(image):\n",
        "  img_byte_arr = io.BytesIO()\n",
        "  image.save(img_byte_arr, format='png')\n",
        "  img_byte_arr = img_byte_arr.getvalue()\n",
        "  IPython.display.display(IPython.display.Image(img_byte_arr))\n",
        "\n",
        "\n",
        "def convert_image(image):\n",
        "  dtype = 'float32'\n",
        "  image = image.resize(RESNET_INPUT_IMG_SHAPE[2:])\n",
        "  image = np.array(image) - np.array([123., 117., 104.])\n",
        "  image /= np.array([58.395, 57.12, 57.375])\n",
        "  image = image.transpose((2, 0, 1))\n",
        "  image = image[np.newaxis, :]\n",
        "  image = tvm.nd.array(image.astype(dtype))\n",
        "  return image\n",
        "\n",
        "\n",
        "def get_resnet():    \n",
        "  block = get_model('resnet18_v1', pretrained=True)\n",
        "  module, params = relay.frontend.from_mxnet(\n",
        "    block, shape={'data': RESNET_INPUT_IMG_SHAPE})\n",
        "  func = module['main']\n",
        "  return func, params\n",
        "  \n",
        "  \n",
        "# Fetch a mapping from class IDs to human-readable labels.\n",
        "synset_url = ''.join(['https://gist.githubusercontent.com/zhreshold/',\n",
        "                      '4d0b62f3d01426887599d4f7ede23ee5/raw/',\n",
        "                      '596b27d23537e5a1b5751d2b0481ef172f58b539/',\n",
        "                      'imagenet1000_clsid_to_human.txt'])\n",
        "synset_name = 'synset.txt'\n",
        "download(synset_url, synset_name)\n",
        "with open(synset_name) as f:\n",
        "  synset = eval(f.read())\n",
        "\n",
        "    \n",
        "def get_predictions(mod):\n",
        "  output = mod.get_output(0).asnumpy()[0]\n",
        "  top_k = output.argsort()[-3:][::-1]\n",
        "  predictions = list(map(lambda x: synset[x], top_k))\n",
        "  return predictions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZAxIkBheYrOp",
        "colab_type": "text"
      },
      "source": [
        "It's great that we can prototype models and immediately run them on a device, but the function we just ran was pretty simple.  Let's try a more interesting example.\n",
        "\n",
        "In this section, we will run ResNet-18 on μTVM to perform image recognition.  And our example input is..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FrTSYq4wqc_N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "image = get_image()\n",
        "show_image(image)\n",
        "# Convert image to a format the model can understand.\n",
        "image = convert_image(image)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2EEuEtrjtcRj",
        "colab_type": "text"
      },
      "source": [
        "...a cat!\n",
        "\n",
        "Luckily, MxNet is one of the many frameworks that TVM can import models from, so we can use a pretrained ResNet-18 model from the Gluon Model Zoo.\n",
        "\n",
        "Below, we grab a relay function that expresses our model, along with its set of pretrained parameters.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t52Cr6rd7n81",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "resnet, params = get_resnet()\n",
        "print(resnet)\n",
        "print()\n",
        "pprint.pprint(list(params.keys()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RIGStzjDx8bx",
        "colab_type": "text"
      },
      "source": [
        "With that, we can create a μTVM session, build a graph runtime module, then run the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lAEJ7ceR7aF1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with micro.Session('host', '') as sess:\n",
        "  print('Building...')\n",
        "  module, params = sess.build(resnet, params=params)\n",
        "  module.set_input(**params)\n",
        "  print()\n",
        "  print('Running...', end='')\n",
        "  # Execute with `image` as the input.\n",
        "  module.run(data=image)\n",
        "  print()\n",
        "  print('Predictions')\n",
        "  for i, prediction in enumerate(get_predictions(module)):\n",
        "    print(f'  {i + 1}: {prediction}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "azzGhNPY2QIJ",
        "colab_type": "text"
      },
      "source": [
        "# Additional Resources\n",
        "\n",
        "For more details on how μTVM is implemented, you can look into the following pull requests on the [TVM GitHub](https://github.com/dmlc/tvm) repository: \n",
        "\n",
        "- [C code generation backend](https://github.com/dmlc/tvm/pull/2161)\n",
        "- [μTVM RFC](https://github.com/dmlc/tvm/issues/2563) (somewhat outdated)\n",
        "- [μTVM on host-emulated device](https://github.com/dmlc/tvm/pull/3227)\n",
        "- μTVM on RISC-V: Will be upstreamed in a few weeks!\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}