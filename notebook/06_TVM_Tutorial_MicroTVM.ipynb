{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "06_TVM_Tutorial_MicroTVM.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/uwsampl/tutorial/blob/master/notebook/06_TVM_Tutorial_MicroTVM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xFoF4FhyYcwE",
        "colab_type": "text"
      },
      "source": [
        "Please run the following block to ensure TVM is setup for *this notebook*.  Each notebook may have its own runtime."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ce5eshWvUiyN",
        "colab_type": "code",
        "outputId": "59fb1286-04c4-47c0-abaa-96b5baf87627",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3438
        }
      },
      "source": [
        "try:\n",
        "  import google.colab\n",
        "  IN_COLAB = True\n",
        "except:\n",
        "  IN_COLAB = False\n",
        "\n",
        "if IN_COLAB:\n",
        "    ! gsutil cp \"gs://tvm-fcrc-binariesd5fce43e-8373-11e9-bfb6-0242ac1c0002/tvm.tar.gz\" /tmp/tvm.tar.gz\n",
        "    ! mkdir -p /tvm\n",
        "    ! tar -xf /tmp/tvm.tar.gz --strip-components=4 --directory /tvm\n",
        "    ! ls -la /tvm\n",
        "    ! bash /tvm/package.sh\n",
        "    # Add TVM to the Python path.\n",
        "    import sys\n",
        "    sys.path.append('/tvm/python')\n",
        "    sys.path.append('/tvm/topi/python')\n",
        "    ! pip install mxnet\n",
        "else:\n",
        "    print(\"Notebook executing locally, skipping Colab setup ...\")\n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Copying gs://tvm-fcrc-binariesd5fce43e-8373-11e9-bfb6-0242ac1c0002/tvm.tar.gz...\n",
            "\\\n",
            "Operation completed over 1 objects/115.9 MiB.                                    \n",
            "total 164\n",
            "drwxr-xr-x 21 root root  4096 Jun 15 00:25 .\n",
            "drwxr-xr-x  1 root root  4096 Jun 15 00:25 ..\n",
            "drwx------  8 root root  4096 May 31 08:14 3rdparty\n",
            "drwx------ 12 root root  4096 Jun 14 21:19 apps\n",
            "drwx------  3 root root  4096 Jun 15 00:20 build\n",
            "drwx------  4 root root  4096 Jun 14 21:19 cmake\n",
            "-rw-------  1 root root 10778 Jun 14 21:19 CMakeLists.txt\n",
            "drwx------  6 root root  4096 Jun 14 21:19 conda\n",
            "-rw-------  1 root root  5736 Jun 14 21:19 CONTRIBUTORS.md\n",
            "drwx------  3 root root  4096 Jun 14 21:19 docker\n",
            "drwx------ 11 root root  4096 Jun 14 21:19 docs\n",
            "drwx------  4 root root  4096 Jun 14 21:19 golang\n",
            "drwx------  3 root root  4096 May 31 08:14 include\n",
            "-rw-------  1 root root 10542 Jun 14 21:19 Jenkinsfile\n",
            "drwx------  6 root root  4096 Jun 14 21:19 jvm\n",
            "-rw-------  1 root root 11357 Jun 14 21:19 LICENSE\n",
            "-rw-------  1 root root  4267 Jun 14 21:19 Makefile\n",
            "-rw-------  1 root root 10476 Jun 14 21:19 NEWS.md\n",
            "drwx------  9 root root  4096 Jun 14 21:19 nnvm\n",
            "-rw-------  1 root root    61 Jun 14 21:19 NOTICE\n",
            "-rwx------  1 root root   374 Jun 14 21:19 package.sh\n",
            "drwx------  3 root root  4096 Jun 14 21:19 python\n",
            "-rw-------  1 root root  2705 Jun 14 21:19 README.md\n",
            "drwx------  6 root root  4096 Jun 14 21:19 rust\n",
            "drwx------ 14 root root  4096 Jun 14 21:19 src\n",
            "drwx------  9 root root  4096 May 31 08:14 tests\n",
            "drwx------  7 root root  4096 Jun 14 21:19 topi\n",
            "drwx------  8 root root  4096 Jun 14 21:19 tutorials\n",
            "-rw-------  1 root root  2902 Jun 14 21:19 version.py\n",
            "drwx------ 10 root root  4096 Jun 14 21:19 vta\n",
            "drwx------  2 root root  4096 Jun 14 21:19 web\n",
            "Installing Dependencies ...\n",
            "deb https://dl.bintray.com/sbt/debian /\n",
            "Executing: /tmp/apt-key-gpghome.W51YNxvFjS/gpg.1.sh --keyserver hkp://keyserver.ubuntu.com:80 --recv 2EE0EA64E40A89B84B2DF73499E82A75642AC823\n",
            "gpg: key 99E82A75642AC823: public key \"sbt build tool <scalasbt@gmail.com>\" imported\n",
            "gpg: Total number processed: 1\n",
            "gpg:               imported: 1\n",
            "Ign:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Get:2 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease [21.3 kB]\n",
            "Get:3 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Ign:5 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
            "Hit:7 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Get:8 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Ign:9 https://dl.bintray.com/sbt/debian  InRelease\n",
            "Get:11 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic InRelease [15.4 kB]\n",
            "Get:12 https://dl.bintray.com/sbt/debian  Release [815 B]\n",
            "Get:14 https://dl.bintray.com/sbt/debian  Release.gpg [821 B]\n",
            "Get:15 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran35/ InRelease [3,626 B]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Get:17 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic/main amd64 Packages [33.0 kB]\n",
            "Get:18 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [717 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [1,220 kB]\n",
            "Get:20 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic/main Sources [1,647 kB]\n",
            "Get:21 https://dl.bintray.com/sbt/debian  Packages [3,424 B]\n",
            "Get:22 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran35/ Packages [58.0 kB]\n",
            "Get:23 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [535 kB]\n",
            "Get:24 http://security.ubuntu.com/ubuntu bionic-security/multiverse amd64 Packages [4,169 B]\n",
            "Get:25 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [7,239 B]\n",
            "Get:26 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [837 kB]\n",
            "Get:27 http://archive.ubuntu.com/ubuntu bionic-backports/universe amd64 Packages [3,902 B]\n",
            "Get:28 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic/main amd64 Packages [791 kB]\n",
            "Fetched 6,151 kB in 3s (2,324 kB/s)\n",
            "Reading package lists... Done\n",
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "zlib1g-dev is already the newest version (1:1.2.11.dfsg-0ubuntu2).\n",
            "zlib1g-dev set to manually installed.\n",
            "clinfo is already the newest version (2.2.18.03.26-1).\n",
            "libtinfo-dev is already the newest version (6.1-1ubuntu1.18.04).\n",
            "libtinfo-dev set to manually installed.\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-410\n",
            "Use 'sudo apt autoremove' to remove it.\n",
            "Suggested packages:\n",
            "  llvm-6.0-doc\n",
            "The following NEW packages will be installed:\n",
            "  binfmt-support libffi-dev llvm-6.0 llvm-6.0-dev llvm-6.0-runtime tree\n",
            "0 upgraded, 6 newly installed, 0 to remove and 120 not upgraded.\n",
            "Need to get 28.3 MB of archives.\n",
            "After this operation, 178 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/main amd64 binfmt-support amd64 2.1.8-2 [51.6 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/main amd64 llvm-6.0-runtime amd64 1:6.0-1ubuntu2 [200 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic/main amd64 llvm-6.0 amd64 1:6.0-1ubuntu2 [4,838 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic/main amd64 libffi-dev amd64 3.2.1-8 [156 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu bionic/main amd64 llvm-6.0-dev amd64 1:6.0-1ubuntu2 [23.0 MB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu bionic/universe amd64 tree amd64 1.7.0-5 [40.7 kB]\n",
            "Fetched 28.3 MB in 2s (16.5 MB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 6.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package binfmt-support.\n",
            "(Reading database ... 130912 files and directories currently installed.)\n",
            "Preparing to unpack .../0-binfmt-support_2.1.8-2_amd64.deb ...\n",
            "Unpacking binfmt-support (2.1.8-2) ...\n",
            "Selecting previously unselected package llvm-6.0-runtime.\n",
            "Preparing to unpack .../1-llvm-6.0-runtime_1%3a6.0-1ubuntu2_amd64.deb ...\n",
            "Unpacking llvm-6.0-runtime (1:6.0-1ubuntu2) ...\n",
            "Selecting previously unselected package llvm-6.0.\n",
            "Preparing to unpack .../2-llvm-6.0_1%3a6.0-1ubuntu2_amd64.deb ...\n",
            "Unpacking llvm-6.0 (1:6.0-1ubuntu2) ...\n",
            "Selecting previously unselected package libffi-dev:amd64.\n",
            "Preparing to unpack .../3-libffi-dev_3.2.1-8_amd64.deb ...\n",
            "Unpacking libffi-dev:amd64 (3.2.1-8) ...\n",
            "Selecting previously unselected package llvm-6.0-dev.\n",
            "Preparing to unpack .../4-llvm-6.0-dev_1%3a6.0-1ubuntu2_amd64.deb ...\n",
            "Unpacking llvm-6.0-dev (1:6.0-1ubuntu2) ...\n",
            "Selecting previously unselected package tree.\n",
            "Preparing to unpack .../5-tree_1.7.0-5_amd64.deb ...\n",
            "Unpacking tree (1.7.0-5) ...\n",
            "Setting up binfmt-support (2.1.8-2) ...\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/binfmt-support.service → /lib/systemd/system/binfmt-support.service.\n",
            "invoke-rc.d: could not determine current runlevel\n",
            "invoke-rc.d: policy-rc.d denied execution of start.\n",
            "Setting up tree (1.7.0-5) ...\n",
            "Setting up libffi-dev:amd64 (3.2.1-8) ...\n",
            "Setting up llvm-6.0-runtime (1:6.0-1ubuntu2) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1) ...\n",
            "Setting up llvm-6.0 (1:6.0-1ubuntu2) ...\n",
            "Processing triggers for systemd (237-3ubuntu10.21) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Setting up llvm-6.0-dev (1:6.0-1ubuntu2) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1) ...\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-410\n",
            "Use 'sudo apt autoremove' to remove it.\n",
            "Suggested packages:\n",
            "  gtkwave systemc\n",
            "The following NEW packages will be installed:\n",
            "  sbt verilator\n",
            "0 upgraded, 2 newly installed, 0 to remove and 120 not upgraded.\n",
            "Need to get 4,005 kB of archives.\n",
            "After this operation, 14.4 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 verilator amd64 3.916-1build1 [2,878 kB]\n",
            "Get:2 https://dl.bintray.com/sbt/debian  sbt 1.2.8 [1,126 kB]\n",
            "Fetched 4,005 kB in 1s (4,652 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 2.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package sbt.\n",
            "(Reading database ... 132553 files and directories currently installed.)\n",
            "Preparing to unpack .../apt/archives/sbt_1.2.8_all.deb ...\n",
            "Unpacking sbt (1.2.8) ...\n",
            "Selecting previously unselected package verilator.\n",
            "Preparing to unpack .../verilator_3.916-1build1_amd64.deb ...\n",
            "Unpacking verilator (3.916-1build1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Setting up verilator (3.916-1build1) ...\n",
            "Setting up sbt (1.2.8) ...\n",
            "Creating system group: sbt\n",
            "Creating system user: sbt in sbt with sbt daemon-user and shell /bin/false\n",
            "Collecting mxnet\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/58/f4/bc147a1ba7175f9890523ff8f1a928a43ac8a79d5897a067158cac4d092f/mxnet-1.4.1-py2.py3-none-manylinux1_x86_64.whl (28.4MB)\n",
            "\u001b[K     |████████████████████████████████| 28.4MB 1.3MB/s \n",
            "\u001b[?25hCollecting graphviz<0.9.0,>=0.8.1 (from mxnet)\n",
            "  Downloading https://files.pythonhosted.org/packages/53/39/4ab213673844e0c004bed8a0781a0721a3f6bb23eb8854ee75c236428892/graphviz-0.8.4-py2.py3-none-any.whl\n",
            "Collecting numpy<1.15.0,>=1.8.2 (from mxnet)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/c4/395ebb218053ba44d64935b3729bc88241ec279915e72100c5979db10945/numpy-1.14.6-cp36-cp36m-manylinux1_x86_64.whl (13.8MB)\n",
            "\u001b[K     |████████████████████████████████| 13.8MB 41.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests>=2.20.0 in /usr/local/lib/python3.6/dist-packages (from mxnet) (2.21.0)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20.0->mxnet) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20.0->mxnet) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20.0->mxnet) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20.0->mxnet) (2019.3.9)\n",
            "\u001b[31mERROR: spacy 2.1.4 has requirement numpy>=1.15.0, but you'll have numpy 1.14.6 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: imgaug 0.2.9 has requirement numpy>=1.15.0, but you'll have numpy 1.14.6 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: fastai 1.0.52 has requirement numpy>=1.15, but you'll have numpy 1.14.6 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: blis 0.2.4 has requirement numpy>=1.15.0, but you'll have numpy 1.14.6 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: graphviz, numpy, mxnet\n",
            "  Found existing installation: graphviz 0.10.1\n",
            "    Uninstalling graphviz-0.10.1:\n",
            "      Successfully uninstalled graphviz-0.10.1\n",
            "  Found existing installation: numpy 1.16.4\n",
            "    Uninstalling numpy-1.16.4:\n",
            "      Successfully uninstalled numpy-1.16.4\n",
            "Successfully installed graphviz-0.8.4 mxnet-1.4.1 numpy-1.14.6\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hHFxmiOzjFF2",
        "colab_type": "text"
      },
      "source": [
        "# MicroTVM\n",
        "\n",
        "In this notebook, we're going to cover:\n",
        "\n",
        "*   the C code generation backend\n",
        "*   the graph runtime\n",
        "*   a demo of ResNeT on the graph runtime\n",
        "*   the low-level device interface\n",
        "\n",
        "TODO: Do we even show the low-level device interface?\n",
        "\n",
        "First, we run some necessary imports."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RHHOBXwRuJGl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import tvm\n",
        "from tvm.contrib import graph_runtime, util\n",
        "from tvm import relay\n",
        "import tvm.micro as micro"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ERvCOpVjsn_U",
        "colab_type": "text"
      },
      "source": [
        "## C Codegen\n",
        "\n",
        "Let's take a look at the C codegen backend for a simple function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fAozo7Slus7I",
        "colab_type": "code",
        "outputId": "13ad03df-9458-46e4-945e-ca6c80256428",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2315
        }
      },
      "source": [
        "# First, we construct the function.\n",
        "ty = relay.TensorType(shape=(1024,), dtype=\"float32\")\n",
        "x = relay.var(\"x\", ty)\n",
        "y = relay.var(\"y\", ty)\n",
        "func = relay.Function([x, y], relay.add(x, y))\n",
        "\n",
        "# Then build it with the appropriate configuration.\n",
        "with tvm.build_config(disable_vectorize=True):\n",
        "  _, src_mod, _ = relay.build(func, target=\"c\", params={})\n",
        "\n",
        "# Now, we can see the source that was generated.\n",
        "print(src_mod.get_source())"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "#include \"tvm/runtime/c_runtime_api.h\"\n",
            "#include \"tvm/runtime/c_backend_api.h\"\n",
            "#include \"tvm/runtime/micro/utvm_device_lib.h\"\n",
            "extern void* __tvm_module_ctx = NULL;\n",
            "#ifdef __cplusplus\n",
            "extern \"C\"\n",
            "#endif\n",
            "TVM_DLL int32_t fused_add( void* args,  void* arg_type_ids, int32_t num_args) {\n",
            "  if (!((num_args == 3))) {\n",
            "    TVMAPISetLastError(\"fused_add: num_args should be 3\");\n",
            "    return -1;\n",
            "  }\n",
            "  void* arg0 = (((TVMValue*)args)[0].v_handle);\n",
            "  int32_t arg0_code = (( int32_t*)arg_type_ids)[0];\n",
            "  void* arg1 = (((TVMValue*)args)[1].v_handle);\n",
            "  int32_t arg1_code = (( int32_t*)arg_type_ids)[1];\n",
            "  void* arg2 = (((TVMValue*)args)[2].v_handle);\n",
            "  int32_t arg2_code = (( int32_t*)arg_type_ids)[2];\n",
            "  float* placeholder = (float*)(((TVMArray*)arg0)[0].data);\n",
            "  int64_t* arg0_shape = (int64_t*)(((TVMArray*)arg0)[0].shape);\n",
            "  int64_t* arg0_strides = (int64_t*)(((TVMArray*)arg0)[0].strides);\n",
            "  if (!(arg0_strides == NULL)) {\n",
            "    if (!((1 == ((int32_t)arg0_strides[0])))) {\n",
            "      TVMAPISetLastError(\"arg0.strides: expected to be compact array\");\n",
            "      return -1;\n",
            "    }\n",
            "  }\n",
            "  int32_t dev_type = (((TVMArray*)arg0)[0].ctx.device_type);\n",
            "  int32_t dev_id = (((TVMArray*)arg0)[0].ctx.device_id);\n",
            "  float* placeholder1 = (float*)(((TVMArray*)arg1)[0].data);\n",
            "  int64_t* arg1_shape = (int64_t*)(((TVMArray*)arg1)[0].shape);\n",
            "  int64_t* arg1_strides = (int64_t*)(((TVMArray*)arg1)[0].strides);\n",
            "  if (!(arg1_strides == NULL)) {\n",
            "    if (!((1 == ((int32_t)arg1_strides[0])))) {\n",
            "      TVMAPISetLastError(\"arg1.strides: expected to be compact array\");\n",
            "      return -1;\n",
            "    }\n",
            "  }\n",
            "  float* T_add = (float*)(((TVMArray*)arg2)[0].data);\n",
            "  int64_t* arg2_shape = (int64_t*)(((TVMArray*)arg2)[0].shape);\n",
            "  int64_t* arg2_strides = (int64_t*)(((TVMArray*)arg2)[0].strides);\n",
            "  if (!(arg2_strides == NULL)) {\n",
            "    if (!((1 == ((int32_t)arg2_strides[0])))) {\n",
            "      TVMAPISetLastError(\"arg2.strides: expected to be compact array\");\n",
            "      return -1;\n",
            "    }\n",
            "  }\n",
            "  if (!(((((arg0_code == 3) || (arg0_code == 13)) || (arg0_code == 7)) || (arg0_code == 4)))) {\n",
            "    TVMAPISetLastError(\"fused_add: Expect arg[0] to be pointer\");\n",
            "    return -1;\n",
            "  }\n",
            "  if (!(((((arg1_code == 3) || (arg1_code == 13)) || (arg1_code == 7)) || (arg1_code == 4)))) {\n",
            "    TVMAPISetLastError(\"fused_add: Expect arg[1] to be pointer\");\n",
            "    return -1;\n",
            "  }\n",
            "  if (!(((((arg2_code == 3) || (arg2_code == 13)) || (arg2_code == 7)) || (arg2_code == 4)))) {\n",
            "    TVMAPISetLastError(\"fused_add: Expect arg[2] to be pointer\");\n",
            "    return -1;\n",
            "  }\n",
            "  if (!((dev_type == 1))) {\n",
            "    TVMAPISetLastError(\"device_type need to be 1\");\n",
            "    return -1;\n",
            "  }\n",
            "  if (!((1 == (((TVMArray*)arg0)[0].ndim)))) {\n",
            "    TVMAPISetLastError(\"arg0.ndim is expected to equal 1\");\n",
            "    return -1;\n",
            "  }\n",
            "  if (!(((((((TVMArray*)arg0)[0].dtype.code) == (uint8_t)2) && ((((TVMArray*)arg0)[0].dtype.bits) == (uint8_t)32)) && ((((TVMArray*)arg0)[0].dtype.lanes) == (uint16_t)1)))) {\n",
            "    TVMAPISetLastError(\"arg0.dtype is expected to be float32\");\n",
            "    return -1;\n",
            "  }\n",
            "  if (!((((int32_t)arg0_shape[0]) == 1024))) {\n",
            "    TVMAPISetLastError(\"Argument arg0.shape[0] has an unsatisfied constraint\");\n",
            "    return -1;\n",
            "  }\n",
            "  if (!(((((TVMArray*)arg0)[0].byte_offset) == (uint64_t)0))) {\n",
            "    TVMAPISetLastError(\"Argument arg0.byte_offset has an unsatisfied constraint\");\n",
            "    return -1;\n",
            "  }\n",
            "  if (!((1 == (((TVMArray*)arg1)[0].ndim)))) {\n",
            "    TVMAPISetLastError(\"arg1.ndim is expected to equal 1\");\n",
            "    return -1;\n",
            "  }\n",
            "  if (!(((((((TVMArray*)arg1)[0].dtype.code) == (uint8_t)2) && ((((TVMArray*)arg1)[0].dtype.bits) == (uint8_t)32)) && ((((TVMArray*)arg1)[0].dtype.lanes) == (uint16_t)1)))) {\n",
            "    TVMAPISetLastError(\"arg1.dtype is expected to be float32\");\n",
            "    return -1;\n",
            "  }\n",
            "  if (!((((int32_t)arg1_shape[0]) == 1024))) {\n",
            "    TVMAPISetLastError(\"Argument arg1.shape[0] has an unsatisfied constraint\");\n",
            "    return -1;\n",
            "  }\n",
            "  if (!(((((TVMArray*)arg1)[0].byte_offset) == (uint64_t)0))) {\n",
            "    TVMAPISetLastError(\"Argument arg1.byte_offset has an unsatisfied constraint\");\n",
            "    return -1;\n",
            "  }\n",
            "  if (!((1 == (((TVMArray*)arg1)[0].ctx.device_type)))) {\n",
            "    TVMAPISetLastError(\"Argument arg1.device_type has an unsatisfied constraint\");\n",
            "    return -1;\n",
            "  }\n",
            "  if (!((dev_id == (((TVMArray*)arg1)[0].ctx.device_id)))) {\n",
            "    TVMAPISetLastError(\"Argument arg1.device_id has an unsatisfied constraint\");\n",
            "    return -1;\n",
            "  }\n",
            "  if (!((1 == (((TVMArray*)arg2)[0].ndim)))) {\n",
            "    TVMAPISetLastError(\"arg2.ndim is expected to equal 1\");\n",
            "    return -1;\n",
            "  }\n",
            "  if (!(((((((TVMArray*)arg2)[0].dtype.code) == (uint8_t)2) && ((((TVMArray*)arg2)[0].dtype.bits) == (uint8_t)32)) && ((((TVMArray*)arg2)[0].dtype.lanes) == (uint16_t)1)))) {\n",
            "    TVMAPISetLastError(\"arg2.dtype is expected to be float32\");\n",
            "    return -1;\n",
            "  }\n",
            "  if (!((((int32_t)arg2_shape[0]) == 1024))) {\n",
            "    TVMAPISetLastError(\"Argument arg2.shape[0] has an unsatisfied constraint\");\n",
            "    return -1;\n",
            "  }\n",
            "  if (!(((((TVMArray*)arg2)[0].byte_offset) == (uint64_t)0))) {\n",
            "    TVMAPISetLastError(\"Argument arg2.byte_offset has an unsatisfied constraint\");\n",
            "    return -1;\n",
            "  }\n",
            "  if (!((1 == (((TVMArray*)arg2)[0].ctx.device_type)))) {\n",
            "    TVMAPISetLastError(\"Argument arg2.device_type has an unsatisfied constraint\");\n",
            "    return -1;\n",
            "  }\n",
            "  if (!((dev_id == (((TVMArray*)arg2)[0].ctx.device_id)))) {\n",
            "    TVMAPISetLastError(\"Argument arg2.device_id has an unsatisfied constraint\");\n",
            "    return -1;\n",
            "  }\n",
            "  for (int32_t ax0 = 0; ax0 < 1024; ++ax0) {\n",
            "    T_add[ax0] = (placeholder[ax0] + placeholder1[ax0]);\n",
            "  }\n",
            "  return 0;\n",
            "}\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aEGvx5OVymcD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_c_module(func):\n",
        "  with tvm.build_config(disable_vectorize=True):\n",
        "    _, src_mod, _ = relay.build(func, target=\"c\", params={})\n",
        "  return src_mod"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QSJcyuD-0AJa",
        "colab_type": "code",
        "outputId": "3402e226-33bb-4928-d6b5-be11a1cc38f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 531
        }
      },
      "source": [
        "func = relay.Function([], relay.const(1.0))\n",
        "print(build_c_module(func).get_source())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TVMError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTVMError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-7dca14713a62>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrelay\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrelay\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconst\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuild_c_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_source\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-5-019b33d98b56>\u001b[0m in \u001b[0;36mbuild_c_module\u001b[0;34m(func)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mbuild_c_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mtvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdisable_vectorize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrelay\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0msrc_mod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tvm/python/tvm/relay/build_module.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(func, target, target_host, params)\u001b[0m\n\u001b[1;32m    194\u001b[0m         \u001b[0mbld_mod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBuildModule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m         graph_json, mod, params = bld_mod.build(func, target, target_host,\n\u001b[0;32m--> 196\u001b[0;31m                                                 params)\n\u001b[0m\u001b[1;32m    197\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_json\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tvm/python/tvm/relay/build_module.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, func, target, target_host, params)\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0;31m# Build the function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_host\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m         \u001b[0;31m# Get artifacts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0mgraph_json\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tvm/python/tvm/_ffi/_ctypes/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    207\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtcodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m                 ctypes.byref(ret_val), ctypes.byref(ret_tcode)) != 0:\n\u001b[0;32m--> 209\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mget_last_ffi_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m         \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m         \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTVMError\u001b[0m: Traceback (most recent call last):\n  [bt] (8) /tvm/build/libtvm.so(+0x4bbad7) [0x7f8930fa1ad7]\n  [bt] (7) /tvm/build/libtvm.so(+0x4bb8ff) [0x7f8930fa18ff]\n  [bt] (6) /tvm/build/libtvm.so(+0x4baef4) [0x7f8930fa0ef4]\n  [bt] (5) /tvm/build/libtvm.so(tvm::build(tvm::Map<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::Array<tvm::LoweredFunc, void>, void, void> const&, tvm::Target const&, tvm::BuildConfig const&)+0x309) [0x7f8930d397e9]\n  [bt] (4) /tvm/build/libtvm.so(tvm::build(tvm::Map<tvm::Target, tvm::Array<tvm::LoweredFunc, void>, void, void> const&, tvm::Target const&, tvm::BuildConfig const&)+0x620) [0x7f8930d38ec0]\n  [bt] (3) /tvm/build/libtvm.so(+0x25a2b7) [0x7f8930d402b7]\n  [bt] (2) /tvm/build/libtvm.so(+0x948b73) [0x7f893142eb73]\n  [bt] (1) /tvm/build/libtvm.so(+0x94aaab) [0x7f8931430aab]\n  [bt] (0) /tvm/build/libtvm.so(+0x154973) [0x7f8930c3a973]\n  File \"/content/gdrive/My Drive/tvm/src/codegen/llvm/llvm_module.cc\", line 180\nTVMError: Check failed: funcs.size() != 0U (0 vs. 0) : "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CjayGmPKynSH",
        "colab_type": "text"
      },
      "source": [
        "Try building some other functions and see what source is generated."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HJbJDEaqtv5t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: Try another function.\n",
        "func = ...\n",
        "print(build_c_module(func).get_source())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqGxOMFp0NsB",
        "colab_type": "text"
      },
      "source": [
        "## Graph Runtime Execution\n",
        "\n",
        "The primary execution strategy for running models with MicroTVM is the graph runtime.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "looZwQcl2D9H",
        "colab_type": "code",
        "outputId": "5ce7ca9d-104b-4a3f-a7a8-2b2234d6eaf9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "DEVICE_TYPE = \"host\"\n",
        "# TODO: Explain this variable.\n",
        "BINUTIL_PREFIX = \"\"\n",
        "\n",
        "shape = (1024,)\n",
        "dtype = \"float32\"\n",
        "    \n",
        "# Construct Relay program.\n",
        "x = relay.var(\"x\", relay.TensorType(shape=shape, dtype=dtype))\n",
        "xx = relay.multiply(x, x)\n",
        "z = relay.add(xx, relay.const(1.0))\n",
        "func = relay.Function([x], z)\n",
        "\n",
        "with micro.Session(DEVICE_TYPE, BINUTIL_PREFIX) as sess:\n",
        "    mod, params = sess.micro_build(func)\n",
        "\n",
        "    mod.set_input(**params)\n",
        "    x_in = np.random.uniform(size=shape[0]).astype(dtype)\n",
        "    print(f\"input: {x_in}\")\n",
        "    mod.run(x=x_in)\n",
        "    result = mod.get_output(0).asnumpy()\n",
        "\n",
        "    print(f\"result: {result}\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input: [0.29276216 0.39650485 0.71247035 ... 0.8835988  0.80035686 0.70527506]\n",
            "result: [1.0857097 1.1572161 1.507614  ... 1.7807468 1.6405711 1.4974129]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITy9wnt1WsN_",
        "colab_type": "text"
      },
      "source": [
        "## ResNet-18 Demo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZAxIkBheYrOp",
        "colab_type": "text"
      },
      "source": [
        "Before we get into the mechanics of MicroTVM, we start with a motivating example.  In the code block below, we perform image recognition on a picture of a cat using ResNet-18.\n",
        "\n",
        "First, we import the model from MxNet's model zoo and use TVM's MxNet model importer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FrTSYq4wqc_N",
        "colab_type": "code",
        "outputId": "c9035faa-76de-46ac-9e06-f478b0bc8405",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        }
      },
      "source": [
        "import mxnet as mx\n",
        "from mxnet.gluon.model_zoo.vision import get_model\n",
        "\n",
        "# Fetch a mapping from class IDs to human-readable labels.\n",
        "synset_url = \"\".join([\"https://gist.githubusercontent.com/zhreshold/\",\n",
        "                      \"4d0b62f3d01426887599d4f7ede23ee5/raw/\",\n",
        "                      \"596b27d23537e5a1b5751d2b0481ef172f58b539/\",\n",
        "                      \"imagenet1000_clsid_to_human.txt\"])\n",
        "synset_name = \"synset.txt\"\n",
        "download(synset_url, synset_name)\n",
        "with open(synset_name) as f:\n",
        "    synset = eval(f.read())\n",
        "    \n",
        "block = get_model(\"resnet18_v1\", pretrained=True)\n",
        "func, params = relay.frontend.from_mxnet(\n",
        "    block, shape={\"data\": image.shape})"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-7eb2f755b910>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mmxnet\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmxnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgluon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_zoo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvision\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Fetch a mapping from class IDs to human-readable labels.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m synset_url = \"\".join([\"https://gist.githubusercontent.com/zhreshold/\",\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'mxnet'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1OjQ4rdBtSer",
        "colab_type": "text"
      },
      "source": [
        "Now, we download a picture of a cat and convert it to a format the model can understand."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iKH4Hmp6WwPD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from mxnet.gluon.utils import download\n",
        "from PIL import Image\n",
        "\n",
        "dtype = \"float32\"\n",
        "\n",
        "img_name = \"cat.png\"\n",
        "download(\"https://github.com/dmlc/mxnet.js/blob/master/data/cat.png?raw=true\",\n",
        "         img_name)\n",
        "image = Image.open(img_name).resize((224, 224))\n",
        "image = np.array(image) - np.array([123., 117., 104.])\n",
        "image /= np.array([58.395, 57.12, 57.375])\n",
        "image = image.transpose((2, 0, 1))\n",
        "image = image[np.newaxis, :]\n",
        "image = tvm.nd.array(image.astype(dtype))\n",
        "\n",
        "# TODO: print image"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2EEuEtrjtcRj",
        "colab_type": "text"
      },
      "source": [
        "With that, we can create a MicroTVM session, build a graph runtime module, then run the model.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CqmbMcmga4Vp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DEVICE_TYPE = \"host\"\n",
        "# TODO: Explain this variable.\n",
        "BINUTIL_PREFIX = \"\"\n",
        "\n",
        "with micro.Session(DEVICE_TYPE, BINUTIL_PREFIX) as sess:\n",
        "    mod, params = relay_micro_build(func, BINUTIL_PREFIX, params=params)\n",
        "    # Set model weights.\n",
        "    mod.set_input(**params)\n",
        "    # Execute with `image` as the input.\n",
        "    mod.run(data=image)\n",
        "    # Get outputs.\n",
        "    tvm_output = mod.get_output(0)\n",
        "\n",
        "    prediction_idx = np.argmax(tvm_output.asnumpy()[0])\n",
        "    prediction = synset[prediction_idx]\n",
        "    print(prediction)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJJZRRnr0gGw",
        "colab_type": "text"
      },
      "source": [
        "## The Low-Level Device Interface\n",
        "\n",
        "The low-level device interface is used by TVM to communicate with MicroDevices. It exposes three important functions to interact with the MicroDevice: \n",
        "- Read from device memory.\n",
        "- Write to device memory.\n",
        "- Start function execution.\n",
        "\n",
        "\n",
        "Below is an example which uses an allocated region of memory on the host machine to simulate a MicroDevice. This HostLowLevelDevice provides MicroTVM users with an easy test setup to experiment with, without having to communicate and debug external MicroDevices. However, the LowLevelDevice interface can be implemented for any MicroDevice, for example over JTAG or a network connection, which will enable you to run TVM on your own MicroDevice.\n",
        "\n",
        "TODO: Add prints that are only on the `tutorial` branch?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dm1fZjgi66te",
        "colab_type": "text"
      },
      "source": [
        "```\n",
        "#@title \n",
        "#include <sys/mman.h>\n",
        "#include <cstring>\n",
        "#include \"low_level_device.h\"\n",
        "#include \"micro_common.h\"\n",
        "\n",
        "namespace tvm {\n",
        "namespace runtime {\n",
        "/*!\n",
        " * \\brief emulated low-level device on host machine\n",
        " */\n",
        "class HostLowLevelDevice final : public LowLevelDevice {\n",
        " public:\n",
        "  /*!\n",
        "   * \\brief constructor to initialize on-host memory region to act as device\n",
        "   * \\param num_bytes size of the emulated on-device memory region\n",
        "   */\n",
        "  explicit HostLowLevelDevice(size_t num_bytes)\n",
        "    : size_(num_bytes) {\n",
        "    size_t size_in_pages = (num_bytes + kPageSize - 1) / kPageSize;\n",
        "    int mmap_prot = PROT_READ | PROT_WRITE | PROT_EXEC;\n",
        "    int mmap_flags = MAP_ANONYMOUS | MAP_PRIVATE;\n",
        "    base_addr_ = DevBaseAddr(\n",
        "      (reinterpret_cast<std::uintptr_t>(\n",
        "        mmap(nullptr, size_in_pages * kPageSize, mmap_prot, mmap_flags, -1, 0))));\n",
        "  }\n",
        "\n",
        "  /*!\n",
        "   * \\brief destructor to deallocate on-host device region\n",
        "   */\n",
        "  ~HostLowLevelDevice() {\n",
        "    munmap(base_addr_.cast_to<void*>(), size_);\n",
        "  }\n",
        "\n",
        "  void Write(DevBaseOffset offset,\n",
        "             void* buf,\n",
        "             size_t num_bytes) final {\n",
        "    void* addr = (offset + base_addr_).cast_to<void*>();\n",
        "    std::memcpy(addr, buf, num_bytes);\n",
        "  }\n",
        "\n",
        "  void Read(DevBaseOffset offset,\n",
        "            void* buf,\n",
        "            size_t num_bytes) final {\n",
        "    void* addr = (offset + base_addr_).cast_to<void*>();\n",
        "    std::memcpy(buf, addr, num_bytes);\n",
        "  }\n",
        "\n",
        "  void Execute(DevBaseOffset func_offset, DevBaseOffset breakpoint) final {\n",
        "    DevAddr func_addr = func_offset + base_addr_;\n",
        "    reinterpret_cast<void (*)(void)>(func_addr.value())();\n",
        "  }\n",
        "\n",
        "  DevBaseAddr base_addr() const final {\n",
        "    return base_addr_;\n",
        "  }\n",
        "\n",
        "  const char* device_type() const final {\n",
        "    return \"host\";\n",
        "  }\n",
        "\n",
        " private:\n",
        "  /*! \\brief base address of the micro device memory region */\n",
        "  DevBaseAddr base_addr_;\n",
        "  /*! \\brief size of memory region */\n",
        "  size_t size_;\n",
        "};\n",
        "\n",
        "const std::shared_ptr<LowLevelDevice> HostLowLevelDeviceCreate(size_t num_bytes) {\n",
        "  std::shared_ptr<LowLevelDevice> lld =\n",
        "      std::make_shared<HostLowLevelDevice>(num_bytes);\n",
        "  return lld;\n",
        "}\n",
        "}  // namespace runtime\n",
        "}  // namespace tvm\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g8dibeuWH8Nt",
        "colab_type": "text"
      },
      "source": [
        "# Homework: RISC-V\n",
        "\n",
        "To use RISC-V as a target device, there are some extra steps that can't be done within this notebook.\n",
        "\n",
        "First, you will need to download and compile [TVM](https://github.com/dmlc/tvm) on your own machine.\n",
        "\n",
        "Next, you will need [Spike](https://github.com/riscv/riscv-isa-sim) (a RISC-V ISA simulator) and [OpenOCD](https://github.com/ntfreak/openocd) (provides a high-level debugging interface to compatible devices).\n",
        "\n",
        "TODO: Flesh out."
      ]
    }
  ]
}