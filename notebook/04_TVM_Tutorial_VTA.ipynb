{
  "cells": [
    {
     "cell_type": "markdown",
     "metadata": {
      "colab_type": "text",
      "id": "SeiRi-zc0NuZ"
     },
     "source": [
      "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/uwsampl/tutorial/blob/master/notebook/04_TVM_Tutorial_VTA.ipynb)"
     ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Please run the following block to ensure TVM is setup for this notebook, each notebook may have its own runtime."
     ]
    },
    {
     "cell_type": "code",
     "execution_count": null,
     "metadata": {},
     "outputs": [],
     "source": [
      "try:\n",
      "  import google.colab\n",
      "  IN_COLAB = True\n",
      "except:\n",
      "  IN_COLAB = False\n",
      "\n",
      "from google.colab import auth\n",
      "auth.authenticate_user()\n",
      "\n",
      "if IN_COLAB:\n",
      "    ! gsutil cp \"gs://tvm-fcrc-binaries-7f775516ff9dfab922c304049f294cec/tvm.tar.gz\" /tmp/tvm.tar.gz\n",
      "    ! mkdir -p /tvm\n",
      "    ! tar -xf /tmp/tvm.tar.gz --strip-components=4 --directory /tvm\n",
      "    ! ls -la /tvm\n",
      "    ! bash /tvm/package.sh\n",
      "    # Add TVM to the Python path.\n",
      "    import sys\n",
      "    sys.path.append('/tvm/python')\n",
      "    sys.path.append('/tvm/topi/python')\n",
      "    sys.path.append('/tvm/nnvm/python')\n",
      "    sys.path.append('/tvm/vta/python')\n",
      "else:\n",
      "    print(\"Notebook executing locally, skipping Colab setup ...\")"
     ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nResNet Inference Example\n========================\n**Author**: `Thierry Moreau <https://homes.cs.washington.edu/~moreau/>`_\n\nThis tutorial provides an end-to-end demo, on how to run ResNet-18 inference\nonto the VTA accelerator design to perform ImageNet classification tasks.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Import Libraries\n----------------\nWe start by importing libraries to run this example.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from __future__ import absolute_import, print_function\n\nimport argparse, json, os, requests, time\nfrom io import BytesIO\nfrom os.path import join, isfile\nfrom PIL import Image\n\nfrom mxnet.gluon.model_zoo import vision\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\nimport tvm\nfrom tvm import rpc, autotvm, relay\nfrom tvm.contrib import graph_runtime, util, download\nfrom tvm.contrib.debugger import debug_runtime\n\nimport vta\nfrom vta.testing import simulator\nfrom vta.top import graph_pack\n\n# Make sure that TVM was compiled with RPC=1\nassert tvm.module.enabled(\"rpc\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define the platform and model targets\n----------------\nExecute on CPU vs. VTA, and define the model.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Load VTA parameters from the vta/config/vta_config.json file\nenv = vta.get_env()\n\n# Set ``device=arm_cpu`` to run inference on the CPU\n# or ``device=vta`` to run inference on the FPGA.\ndevice = \"vta\"\ntarget = env.target if device == \"vta\" else env.target_vta_cpu\n\n# Name of Gluon model to compile\nmodel = \"resnet18_v1\"\nstart_pack=\"nn.max_pool2d\"\nstop_pack=\"nn.global_avg_pool2d\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Obtain an execution remote\n---------------------------------\nWhen target is 'pynq', reconfigure FPGA and runtime.\nOtherwise, if target is 'sim', execute locally.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "if env.TARGET != \"sim\":\n\n    # Get remote from fleet node if environment variable is set\n    tracker_host = os.environ.get(\"TVM_TRACKER_HOST\", None)\n    tracker_port = int(os.environ.get(\"TVM_TRACKER_PORT\", None))\n    device_host = os.environ.get(\"VTA_PYNQ_RPC_HOST\", \"192.168.2.99\")\n    device_port = int(os.environ.get(\"VTA_PYNQ_RPC_PORT\", \"9091\"))\n    if not tracker_host or not tracker_port:\n        remote = rpc.connect(device_host, device_port)\n    else:\n        remote = autotvm.measure.request_remote(env.TARGET, tracker_host, tracker_port, timeout=10000)\n\n    # Reconfigure the JIT runtime and FPGA.\n    # You can program the FPGA with your own custom bitstream\n    # by passing the path to the bitstream file instead of None.\n    reconfig_start = time.time()\n    vta.reconfig_runtime(remote)\n    vta.program_fpga(remote, bitstream=None)\n    reconfig_time = time.time() - reconfig_start\n    print(\"Reconfigured FPGA and RPC runtime in {0:.2f}s!\".format(reconfig_time))\n\n# In simulation mode, host the RPC server locally.\nelse:\n    remote = rpc.LocalSession()\n\n# Get execution context from remote\nctx = remote.ext_dev(0) if device == \"vta\" else remote.cpu(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Build the inference runtime\n------------------------\nBuild ResNet from Gluon with Relay.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Load pre-configured AutoTVM schedules\nwith autotvm.tophub.context(target):\n\n    # Populate the shape and data type dictionary for ResNet input\n    dtype_dict = {\"data\": 'float32'}\n    shape_dict = {\"data\": (env.BATCH, 3, 224, 224)}\n\n    # Get off the shelf gluon model, and convert to relay\n    gluon_model = vision.get_model(model, pretrained=True)\n\n    # Measure build start time\n    build_start = time.time()\n\n    # Start front end compilation\n    relay_prog, params = relay.frontend.from_mxnet(gluon_model, shape_dict)\n    print(relay_prog)\n    # exit()\n\n    # Update shape and type dictionary\n    shape_dict.update({k: v.shape for k, v in params.items()})\n    dtype_dict.update({k: str(v.dtype) for k, v in params.items()})\n\n    # Perform quantization in Relay\n    with relay.quantize.qconfig(global_scale=8.0, skip_k_conv=1):\n        relay_prog = relay.quantize.quantize(relay_prog, params=params)\n\n    # Perform graph packing and constant folding for VTA target\n    if target.device_name == \"vta\":\n        assert env.BLOCK_IN == env.BLOCK_OUT\n        relay_prog = graph_pack(\n            relay_prog,\n            env.BATCH,\n            env.BLOCK_OUT,\n            env.WGT_WIDTH,\n            start_name=start_pack,\n            stop_name=stop_pack)\n        relay_prog = relay.ir_pass.fold_constant(relay_prog)\n\n    # Compile Relay program with AlterOpLayout disabled\n    with relay.build_config(opt_level=3, disabled_pass={\"AlterOpLayout\"}):\n        if target.device_name != \"vta\":\n            graph, lib, params = relay.build(\n                relay_prog, target=target,\n                params=params, target_host=env.target_host)\n        else:\n            with vta.build_config():\n                graph, lib, params = relay.build(\n                    relay_prog, target=target,\n                    params=params, target_host=env.target_host)\n\n    # Measure Relay build time\n    build_time = time.time() - build_start\n    print(model + \" inference graph built in {0:.2f}s!\".format(build_time))\n\n    # Send the inference library over to the remote RPC server\n    temp = util.tempdir()\n    lib.save(temp.relpath(\"graphlib.o\"))\n    remote.upload(temp.relpath(\"graphlib.o\"))\n    lib = remote.load_module(\"graphlib.o\")\n\n    # Graph runtime\n    m = graph_runtime.create(graph, lib, ctx)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Perform ResNet-18 inference\n------------------------\nWe run classification on an image sample from ImageNet\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Download ImageNet categories\ncateg_url = \"https://github.com/uwsaml/web-data/raw/master/vta/models/synset.txt\"\ncateg_fn = \"synset.txt\"\ndownload.download(join(categ_url, categ_fn), categ_fn)\nsynset = eval(open(categ_fn).read())\n\n# Download test image\nimage_url = 'https://homes.cs.washington.edu/~moreau/media/vta/cat.jpg'\nresponse = requests.get(image_url)\n\n# Prepare test image for inference\nimage = Image.open(BytesIO(response.content)).resize((224, 224))\nplt.imshow(image)\nplt.show()\nimage = np.array(image) - np.array([123., 117., 104.])\nimage /= np.array([58.395, 57.12, 57.375])\nimage = image.transpose((2, 0, 1))\nimage = image[np.newaxis, :]\nimage = np.repeat(image, env.BATCH, axis=0)\n\n# Set the network parameters and inputs\nm.set_input(**params)\nm.set_input('data', image)\n\n# Perform inference\ntimer = m.module.time_evaluator(\"run\", ctx, number=4, repeat=3)\ntcost = timer()\n\n# Get classification results\ntvm_output = m.get_output(0, tvm.nd.empty((env.BATCH, 1000), \"float32\", remote.cpu(0)))\ntop_categories = np.argsort(tvm_output.asnumpy()[0])\n\n# Report top-5 classification results\nstd = np.std(tcost.results) * 1000 / env.BATCH\nmean = tcost.mean * 1000 / env.BATCH\nprint(\"%s prediction\" % model)\nprint(\"                     #1:\", synset[top_categories[-1]])\nprint(\"                     #2:\", synset[top_categories[-2]])\nprint(\"                     #3:\", synset[top_categories[-3]])\nprint(\"                     #4:\", synset[top_categories[-4]])\nprint(\"                     #5:\", synset[top_categories[-5]])\nprint(\"Performed inference in %.2fms/sample (std = %.2f)\" % (mean, std))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}