{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SeiRi-zc0NuZ"
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/uwsampl/tutorial/blob/master/notebook/04_TVM_Tutorial_VTA.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please run the following block to ensure TVM is setup for this notebook, each notebook may have its own runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  import google.colab\n",
    "  IN_COLAB = True\n",
    "except:\n",
    "  IN_COLAB = False\n",
    "\n",
    "from google.colab import auth",
    "auth.authenticate_user()",
    "\n",
    "if IN_COLAB:\n",
    "    ! gsutil cp \"gs://tvm-fcrc-binaries-7f775516ff9dfab922c304049f294cec/tvm.tar.gz\" /tmp/tvm.tar.gz\n",
    "    ! mkdir -p /tvm\n",
    "    ! tar -xf /tmp/tvm.tar.gz --strip-components=4 --directory /tvm\n",
    "    ! ls -la /tvm\n",
    "    ! bash /tvm/package.sh\n",
    "    # Add TVM to the Python path.\n",
    "    import sys\n",
    "    sys.path.append('/tvm/python')\n",
    "    sys.path.append('/tvm/topi/python')\n",
    "    sys.path.append('/tvm/nnvm/python')\n",
    "    sys.path.append('/tvm/vta/python')\n",
    "else:\n",
    "    print(\"Notebook executing locally, skipping Colab setup ...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "ResNet Inference Example\n",
    "========================\n",
    "**Author**: `Thierry Moreau <https://homes.cs.washington.edu/~moreau/>`_\n",
    "\n",
    "This tutorial provides an end-to-end demo, on how to run ResNet-18 inference\n",
    "onto the VTA accelerator design to perform ImageNet classification tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Libraries\n",
    "----------------\n",
    "We start by importing the tvm, vta, nnvm libraries to run this example.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, print_function\n",
    "\n",
    "import os\n",
    "import time\n",
    "from io import BytesIO\n",
    "\n",
    "import numpy as np\n",
    "import requests\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "import tvm\n",
    "from tvm import rpc, autotvm\n",
    "from tvm.contrib import graph_runtime, util\n",
    "from tvm.contrib.download import download\n",
    "import nnvm.compiler\n",
    "import vta\n",
    "import vta.testing\n",
    "\n",
    "# Load VTA parameters from the vta/config/vta_config.json file\n",
    "env = vta.get_env()\n",
    "\n",
    "# Helper to crop an image to a square (224, 224)\n",
    "# Takes in an Image object, returns an Image object\n",
    "def thumbnailify(image, pad=15):\n",
    "    w, h = image.size\n",
    "    crop = ((w-h)//2+pad, pad, h+(w-h)//2-pad, h-pad)\n",
    "    image = image.crop(crop)\n",
    "    image = image.resize((224, 224))\n",
    "    return image\n",
    "\n",
    "# Helper function to read in image\n",
    "# Takes in Image object, returns an ND array\n",
    "def process_image(image):\n",
    "    # Convert to neural network input format\n",
    "    image = np.array(image) - np.array([123., 117., 104.])\n",
    "    image /= np.array([58.395, 57.12, 57.375])\n",
    "    image = image.transpose((2, 0, 1))\n",
    "    image = image[np.newaxis, :]\n",
    "\n",
    "    return tvm.nd.array(image.astype(\"float32\"))\n",
    "\n",
    "# Classification helper function\n",
    "# Takes in the graph runtime, and an image, and returns top result and time\n",
    "def classify(m, image):\n",
    "    m.set_input('data', image)\n",
    "    timer = m.module.time_evaluator(\"run\", ctx, number=1)\n",
    "    tcost = timer()\n",
    "    tvm_output = m.get_output(0)\n",
    "    top = np.argmax(tvm_output.asnumpy()[0])\n",
    "    tcost = \"t={0:.2f}s\".format(tcost.mean)\n",
    "    return tcost + \" {}\".format(synset[top])\n",
    "\n",
    "# Helper function to compile the NNVM graph\n",
    "# Takes in a path to a graph file, params file, and device target\n",
    "# Returns the NNVM graph object, a compiled library object, and the params dict\n",
    "def generate_graph(graph_fn, params_fn, device=\"vta\"):\n",
    "    # Measure build start time\n",
    "    build_start = time.time()\n",
    "\n",
    "    # Derive the TVM target\n",
    "    target = tvm.target.create(\"llvm -device={}\".format(device))\n",
    "\n",
    "    # Derive the LLVM compiler flags\n",
    "    # When targetting the Pynq, cross-compile to ARMv7 ISA\n",
    "    if env.TARGET == \"sim\":\n",
    "        target_host = \"llvm\"\n",
    "    elif env.TARGET == \"pynq\":\n",
    "        target_host = \"llvm -mtriple=armv7-none-linux-gnueabihf -mcpu=cortex-a9 -mattr=+neon\"\n",
    "\n",
    "    # Load the ResNet-18 graph and parameters\n",
    "    sym = nnvm.graph.load_json(open(graph_fn).read())\n",
    "    params = nnvm.compiler.load_param_dict(open(params_fn, 'rb').read())\n",
    "\n",
    "    # Populate the shape and data type dictionary\n",
    "    shape_dict = {\"data\": (1, 3, 224, 224)}\n",
    "    dtype_dict = {\"data\": 'float32'}\n",
    "    shape_dict.update({k: v.shape for k, v in params.items()})\n",
    "    dtype_dict.update({k: str(v.dtype) for k, v in params.items()})\n",
    "\n",
    "    # Apply NNVM graph optimization passes\n",
    "    sym = vta.graph.clean_cast(sym)\n",
    "    sym = vta.graph.clean_conv_fuse(sym)\n",
    "    if target.device_name == \"vta\":\n",
    "        assert env.BLOCK_IN == env.BLOCK_OUT\n",
    "        sym = vta.graph.pack(sym, shape_dict, env.BATCH, env.BLOCK_OUT)\n",
    "\n",
    "    # Compile NNVM graph\n",
    "    with nnvm.compiler.build_config(opt_level=3):\n",
    "        if target.device_name != \"vta\":\n",
    "            graph, lib, params = nnvm.compiler.build(\n",
    "                sym, target, shape_dict, dtype_dict,\n",
    "                params=params, target_host=target_host)\n",
    "        else:\n",
    "            with vta.build_config():\n",
    "                graph, lib, params = nnvm.compiler.build(\n",
    "                    sym, target, shape_dict, dtype_dict,\n",
    "                    params=params, target_host=target_host)\n",
    "\n",
    "    # Save the compiled inference graph library\n",
    "    assert tvm.module.enabled(\"rpc\")\n",
    "    temp = util.tempdir()\n",
    "    lib.save(temp.relpath(\"graphlib.o\"))\n",
    "\n",
    "    # Send the inference library over to the remote RPC server\n",
    "    remote.upload(temp.relpath(\"graphlib.o\"))\n",
    "    lib = remote.load_module(\"graphlib.o\")\n",
    "\n",
    "    # Measure build time\n",
    "    build_time = time.time() - build_start\n",
    "    print(\"ResNet-18 inference graph built in {0:.2f}s!\".format(build_time))\n",
    "\n",
    "    return graph, lib, params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download ResNet Model\n",
    "--------------------------------------------\n",
    "Download the necessary files to run ResNet-18.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain ResNet model and download them into _data dir\n",
    "url = \"https://github.com/uwsaml/web-data/raw/master/vta/models/\"\n",
    "categ_fn = 'synset.txt'\n",
    "graph_fn = 'resnet18_qt8.json'\n",
    "params_fn = 'resnet18_qt8.params'\n",
    "\n",
    "# Create data dir\n",
    "data_dir = \"_data/\"\n",
    "if not os.path.exists(data_dir):\n",
    "    os.makedirs(data_dir)\n",
    "\n",
    "# Download files\n",
    "for file in [categ_fn, graph_fn, params_fn]:\n",
    "    download(os.path.join(url, file), os.path.join(data_dir, file))\n",
    "\n",
    "# Read in ImageNet Categories\n",
    "synset = eval(open(os.path.join(data_dir, categ_fn)).read())\n",
    "\n",
    "# Download pre-tuned op parameters of conv2d for ARM CPU used in VTA\n",
    "autotvm.tophub.check_backend('vta')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup the Pynq Board's RPC Server\n",
    "---------------------------------\n",
    "Build the RPC server's VTA runtime and program the Pynq FPGA.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure build start time\n",
    "reconfig_start = time.time()\n",
    "\n",
    "# We read the Pynq RPC host IP address and port number from the OS environment\n",
    "host = os.environ.get(\"VTA_PYNQ_RPC_HOST\", \"192.168.2.99\")\n",
    "port = int(os.environ.get(\"VTA_PYNQ_RPC_PORT\", \"9091\"))\n",
    "\n",
    "# We configure both the bitstream and the runtime system on the Pynq\n",
    "# to match the VTA configuration specified by the vta_config.json file.\n",
    "if env.TARGET == \"pynq\":\n",
    "    # Make sure that TVM was compiled with RPC=1\n",
    "    assert tvm.module.enabled(\"rpc\")\n",
    "    remote = rpc.connect(host, port)\n",
    "\n",
    "    # Reconfigure the JIT runtime\n",
    "    vta.reconfig_runtime(remote)\n",
    "\n",
    "    # Program the FPGA with a pre-compiled VTA bitstream.\n",
    "    # You can program the FPGA with your own custom bitstream\n",
    "    # by passing the path to the bitstream file instead of None.\n",
    "    vta.program_fpga(remote, bitstream=None)\n",
    "\n",
    "    # Report on reconfiguration time\n",
    "    reconfig_time = time.time() - reconfig_start\n",
    "    print(\"Reconfigured FPGA and RPC runtime in {0:.2f}s!\".format(reconfig_time))\n",
    "\n",
    "# In simulation mode, host the RPC server locally.\n",
    "elif env.TARGET == \"sim\":\n",
    "    remote = rpc.LocalSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the ResNet Runtime\n",
    "------------------------\n",
    "Build the ResNet graph runtime, and configure the parameters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set ``device=vtacpu`` to run inference on the CPU\n",
    "# or ``device=vta`` to run inference on the FPGA.\n",
    "device = \"vta\"\n",
    "\n",
    "# Device context\n",
    "ctx = remote.ext_dev(0) if device == \"vta\" else remote.cpu(0)\n",
    "\n",
    "# Build the graph runtime\n",
    "graph, lib, params = generate_graph(os.path.join(data_dir, graph_fn),\n",
    "                                    os.path.join(data_dir, params_fn),\n",
    "                                    device)\n",
    "m = graph_runtime.create(graph, lib, ctx)\n",
    "\n",
    "# Set the parameters\n",
    "m.set_input(**params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run ResNet-18 inference on a sample image\n",
    "-----------------------------------------\n",
    "Perform image classification on test image.\n",
    "You can change the test image URL to any image of your choosing.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in test image\n",
    "image_url = 'https://homes.cs.washington.edu/~moreau/media/vta/cat.jpg'\n",
    "# Read in test image\n",
    "response = requests.get(image_url)\n",
    "image = Image.open(BytesIO(response.content)).resize((224, 224))\n",
    "# Show Image\n",
    "plt.imshow(image)\n",
    "plt.show()\n",
    "# Set the input\n",
    "image = process_image(image)\n",
    "m.set_input('data', image)\n",
    "\n",
    "# Perform inference\n",
    "timer = m.module.time_evaluator(\"run\", ctx, number=1)\n",
    "tcost = timer()\n",
    "\n",
    "# Get classification results\n",
    "tvm_output = m.get_output(0)\n",
    "top_categories = np.argsort(tvm_output.asnumpy()[0])\n",
    "\n",
    "# Report top-5 classification results\n",
    "print(\"ResNet-18 Prediction #1:\", synset[top_categories[-1]])\n",
    "print(\"                     #2:\", synset[top_categories[-2]])\n",
    "print(\"                     #3:\", synset[top_categories[-3]])\n",
    "print(\"                     #4:\", synset[top_categories[-4]])\n",
    "print(\"                     #5:\", synset[top_categories[-5]])\n",
    "print(\"Performed inference in {0:.2f}s\".format(tcost.mean))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run a Youtube Video Image Classifier\n",
    "------------------------------------\n",
    "Perform image classification on test stream on 1 frame every 48 frames.\n",
    "Comment the `if False:` out to run the demo\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early exit - remove for Demo\n",
    "if False:\n",
    "\n",
    "    import cv2\n",
    "    import pafy\n",
    "    from IPython.display import clear_output\n",
    "\n",
    "    # Helper to crop an image to a square (224, 224)\n",
    "    # Takes in an Image object, returns an Image object\n",
    "    def thumbnailify(image, pad=15):\n",
    "        w, h = image.size\n",
    "        crop = ((w-h)//2+pad, pad, h+(w-h)//2-pad, h-pad)\n",
    "        image = image.crop(crop)\n",
    "        image = image.resize((224, 224))\n",
    "        return image\n",
    "\n",
    "    # 16:16 inches\n",
    "    plt.rcParams['figure.figsize'] = [16, 16]\n",
    "\n",
    "    # Stream the video in\n",
    "    url = \"https://www.youtube.com/watch?v=PJlmYh27MHg&t=2s\"\n",
    "    video = pafy.new(url)\n",
    "    best = video.getbest(preftype=\"mp4\")\n",
    "    cap = cv2.VideoCapture(best.url)\n",
    "\n",
    "    # Process one frame out of every 48 for variety\n",
    "    count = 0\n",
    "    guess = \"\"\n",
    "    while(count<2400):\n",
    "\n",
    "        # Capture frame-by-frame\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        # Process one every 48 frames\n",
    "        if count % 48 == 1:\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            frame = Image.fromarray(frame)\n",
    "            # Crop and resize\n",
    "            thumb = np.array(thumbnailify(frame))\n",
    "            image = process_image(thumb)\n",
    "            guess = classify(m, image)\n",
    "\n",
    "            # Insert guess in frame\n",
    "            frame = cv2.rectangle(thumb,(0,0),(200,0),(0,0,0),50)\n",
    "            cv2.putText(frame, guess, (5,15), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (256,256,256), 1, cv2.LINE_AA)\n",
    "\n",
    "            plt.imshow(thumb)\n",
    "            plt.axis('off')\n",
    "            plt.show()\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "            clear_output(wait=True)\n",
    "\n",
    "        count += 1\n",
    "\n",
    "    # When everything done, release the capture\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
