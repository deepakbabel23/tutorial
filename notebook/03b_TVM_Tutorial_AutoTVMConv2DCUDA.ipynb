{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of 03b_TVM_Tutorial_AutoTVMConv2DCUDA.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/uwsampl/tutorial/blob/master/notebook/03b_TVM_Tutorial_AutoTVMConv2DCUDA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8T0-Z4MasbY",
        "colab_type": "text"
      },
      "source": [
        "Tuning High Performance Convolution on NVIDIA GPUs\n",
        "=========================================================================\n",
        "**Author**: `Lianmin Zheng <https://github.com/merrymercy>`_\n",
        "\n",
        "Adapted by `Eddie Yan <https://github.com/eqy>`_\n",
        "\n",
        "This is an advanced tutorial for writing high performance tunable template for\n",
        "NVIDIA GPU. By running auto-tuner on this template, we can outperform the\n",
        "vendor provided library CuDNN in many cases.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9soFF1q_cV-y",
        "colab_type": "text"
      },
      "source": [
        "Please run the following block to ensure TVM is setup for *this notebook*, each notebook may have its own runtime.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G_lzOGuFcUgG",
        "colab_type": "code",
        "outputId": "3d7b2fb2-c331-46f7-d6d8-269f49328504",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2893
        }
      },
      "source": [
        "! gsutil cp \"gs://tvm-fcrc-binariesd5fce43e-8373-11e9-bfb6-0242ac1c0002/tvm.tar.gz\" /tmp/tvm.tar.gz\n",
        "! mkdir -p /tvm\n",
        "! tar -xf /tmp/tvm.tar.gz --strip-components=4 --directory /tvm\n",
        "! ls -la /tvm\n",
        "# Move this block after we are done with pkg step\n",
        "! bash /tvm/package.sh\n",
        "import sys\n",
        "sys.path.append('/tvm/python')\n",
        "sys.path.append('/tvm/topi/python')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Copying gs://tvm-fcrc-binariesd5fce43e-8373-11e9-bfb6-0242ac1c0002/tvm.tar.gz...\n",
            "- [1 files][119.6 MiB/119.6 MiB]                                                \n",
            "Operation completed over 1 objects/119.6 MiB.                                    \n",
            "total 164\n",
            "drwxr-xr-x 21 root root  4096 Jun 17 23:58 .\n",
            "drwxr-xr-x  1 root root  4096 Jun 17 23:58 ..\n",
            "drwx------  8 root root  4096 May 31 08:14 3rdparty\n",
            "drwx------ 12 root root  4096 Jun 17 19:41 apps\n",
            "drwx------  3 root root  4096 Jun 17 20:16 build\n",
            "drwx------  4 root root  4096 Jun 17 19:41 cmake\n",
            "-rw-------  1 root root 11053 Jun 17 19:41 CMakeLists.txt\n",
            "drwx------  6 root root  4096 Jun 17 19:41 conda\n",
            "-rw-------  1 root root  5736 Jun 17 19:41 CONTRIBUTORS.md\n",
            "drwx------  3 root root  4096 Jun 17 19:41 docker\n",
            "drwx------ 11 root root  4096 Jun 17 19:41 docs\n",
            "drwx------  4 root root  4096 Jun 17 19:41 golang\n",
            "drwx------  3 root root  4096 May 31 08:14 include\n",
            "-rw-------  1 root root 10607 Jun 17 19:41 Jenkinsfile\n",
            "drwx------  6 root root  4096 Jun 17 19:41 jvm\n",
            "-rw-------  1 root root 11357 Jun 17 19:41 LICENSE\n",
            "-rw-------  1 root root  4267 Jun 17 19:41 Makefile\n",
            "-rw-------  1 root root 10476 Jun 17 19:41 NEWS.md\n",
            "drwx------  9 root root  4096 Jun 17 19:41 nnvm\n",
            "-rw-------  1 root root    61 Jun 17 19:41 NOTICE\n",
            "-rwx------  1 root root   374 Jun 17 19:41 package.sh\n",
            "drwx------  3 root root  4096 Jun 17 19:41 python\n",
            "-rw-------  1 root root  2705 Jun 17 19:41 README.md\n",
            "drwx------  6 root root  4096 Jun 17 19:42 rust\n",
            "drwx------ 14 root root  4096 Jun 17 19:42 src\n",
            "drwx------  9 root root  4096 May 31 08:14 tests\n",
            "drwx------  7 root root  4096 Jun 17 19:42 topi\n",
            "drwx------  8 root root  4096 Jun 17 19:42 tutorials\n",
            "-rw-------  1 root root  2902 Jun 17 19:42 version.py\n",
            "drwx------ 10 root root  4096 Jun 17 19:42 vta\n",
            "drwx------  2 root root  4096 Jun 17 19:42 web\n",
            "Installing Dependencies ...\n",
            "deb https://dl.bintray.com/sbt/debian /\n",
            "Executing: /tmp/apt-key-gpghome.PnMvbWH2V1/gpg.1.sh --keyserver hkp://keyserver.ubuntu.com:80 --recv 2EE0EA64E40A89B84B2DF73499E82A75642AC823\n",
            "gpg: key 99E82A75642AC823: public key \"sbt build tool <scalasbt@gmail.com>\" imported\n",
            "gpg: Total number processed: 1\n",
            "gpg:               imported: 1\n",
            "Ign:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Get:2 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease [21.3 kB]\n",
            "Get:3 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Ign:4 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Hit:7 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Ign:8 https://dl.bintray.com/sbt/debian  InRelease\n",
            "Get:10 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Get:11 https://dl.bintray.com/sbt/debian  Release [815 B]\n",
            "Get:12 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic InRelease [15.4 kB]\n",
            "Get:13 https://dl.bintray.com/sbt/debian  Release.gpg [821 B]\n",
            "Get:15 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran35/ InRelease [3,626 B]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Get:17 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [717 kB]\n",
            "Get:18 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic/main amd64 Packages [33.0 kB]\n",
            "Get:19 https://dl.bintray.com/sbt/debian  Packages [3,424 B]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [854 kB]\n",
            "Get:21 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic/main Sources [1,647 kB]\n",
            "Get:22 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran35/ Packages [58.0 kB]\n",
            "Get:23 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [535 kB]\n",
            "Get:24 http://security.ubuntu.com/ubuntu bionic-security/multiverse amd64 Packages [4,169 B]\n",
            "Get:25 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [7,239 B]\n",
            "Get:26 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [1,220 kB]\n",
            "Get:27 http://archive.ubuntu.com/ubuntu bionic-backports/universe amd64 Packages [3,918 B]\n",
            "Get:28 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic/main amd64 Packages [791 kB]\n",
            "Fetched 6,169 kB in 3s (2,270 kB/s)\n",
            "Reading package lists... Done\n",
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "zlib1g-dev is already the newest version (1:1.2.11.dfsg-0ubuntu2).\n",
            "zlib1g-dev set to manually installed.\n",
            "clinfo is already the newest version (2.2.18.03.26-1).\n",
            "libtinfo-dev is already the newest version (6.1-1ubuntu1.18.04).\n",
            "libtinfo-dev set to manually installed.\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-410\n",
            "Use 'sudo apt autoremove' to remove it.\n",
            "Suggested packages:\n",
            "  llvm-6.0-doc\n",
            "The following NEW packages will be installed:\n",
            "  binfmt-support libffi-dev llvm-6.0 llvm-6.0-dev llvm-6.0-runtime tree\n",
            "0 upgraded, 6 newly installed, 0 to remove and 121 not upgraded.\n",
            "Need to get 28.3 MB of archives.\n",
            "After this operation, 178 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/main amd64 binfmt-support amd64 2.1.8-2 [51.6 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/main amd64 llvm-6.0-runtime amd64 1:6.0-1ubuntu2 [200 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic/main amd64 llvm-6.0 amd64 1:6.0-1ubuntu2 [4,838 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic/main amd64 libffi-dev amd64 3.2.1-8 [156 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu bionic/main amd64 llvm-6.0-dev amd64 1:6.0-1ubuntu2 [23.0 MB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu bionic/universe amd64 tree amd64 1.7.0-5 [40.7 kB]\n",
            "Fetched 28.3 MB in 2s (16.5 MB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 6.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package binfmt-support.\n",
            "(Reading database ... 130912 files and directories currently installed.)\n",
            "Preparing to unpack .../0-binfmt-support_2.1.8-2_amd64.deb ...\n",
            "Unpacking binfmt-support (2.1.8-2) ...\n",
            "Selecting previously unselected package llvm-6.0-runtime.\n",
            "Preparing to unpack .../1-llvm-6.0-runtime_1%3a6.0-1ubuntu2_amd64.deb ...\n",
            "Unpacking llvm-6.0-runtime (1:6.0-1ubuntu2) ...\n",
            "Selecting previously unselected package llvm-6.0.\n",
            "Preparing to unpack .../2-llvm-6.0_1%3a6.0-1ubuntu2_amd64.deb ...\n",
            "Unpacking llvm-6.0 (1:6.0-1ubuntu2) ...\n",
            "Selecting previously unselected package libffi-dev:amd64.\n",
            "Preparing to unpack .../3-libffi-dev_3.2.1-8_amd64.deb ...\n",
            "Unpacking libffi-dev:amd64 (3.2.1-8) ...\n",
            "Selecting previously unselected package llvm-6.0-dev.\n",
            "Preparing to unpack .../4-llvm-6.0-dev_1%3a6.0-1ubuntu2_amd64.deb ...\n",
            "Unpacking llvm-6.0-dev (1:6.0-1ubuntu2) ...\n",
            "Selecting previously unselected package tree.\n",
            "Preparing to unpack .../5-tree_1.7.0-5_amd64.deb ...\n",
            "Unpacking tree (1.7.0-5) ...\n",
            "Setting up binfmt-support (2.1.8-2) ...\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/binfmt-support.service → /lib/systemd/system/binfmt-support.service.\n",
            "invoke-rc.d: could not determine current runlevel\n",
            "invoke-rc.d: policy-rc.d denied execution of start.\n",
            "Setting up tree (1.7.0-5) ...\n",
            "Setting up libffi-dev:amd64 (3.2.1-8) ...\n",
            "Setting up llvm-6.0-runtime (1:6.0-1ubuntu2) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1) ...\n",
            "Setting up llvm-6.0 (1:6.0-1ubuntu2) ...\n",
            "Processing triggers for systemd (237-3ubuntu10.21) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Setting up llvm-6.0-dev (1:6.0-1ubuntu2) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1) ...\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-410\n",
            "Use 'sudo apt autoremove' to remove it.\n",
            "Suggested packages:\n",
            "  gtkwave systemc\n",
            "The following NEW packages will be installed:\n",
            "  sbt verilator\n",
            "0 upgraded, 2 newly installed, 0 to remove and 121 not upgraded.\n",
            "Need to get 4,005 kB of archives.\n",
            "After this operation, 14.4 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 verilator amd64 3.916-1build1 [2,878 kB]\n",
            "Get:2 https://dl.bintray.com/sbt/debian  sbt 1.2.8 [1,126 kB]\n",
            "Fetched 4,005 kB in 1s (4,689 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 2.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package sbt.\n",
            "(Reading database ... 132553 files and directories currently installed.)\n",
            "Preparing to unpack .../apt/archives/sbt_1.2.8_all.deb ...\n",
            "Unpacking sbt (1.2.8) ...\n",
            "Selecting previously unselected package verilator.\n",
            "Preparing to unpack .../verilator_3.916-1build1_amd64.deb ...\n",
            "Unpacking verilator (3.916-1build1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Setting up verilator (3.916-1build1) ...\n",
            "Setting up sbt (1.2.8) ...\n",
            "Creating system group: sbt\n",
            "Creating system user: sbt in sbt with sbt daemon-user and shell /bin/false\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "agr421w8a49k",
        "colab_type": "text"
      },
      "source": [
        "Import packages:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C9JnukA4aTLJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import logging\n",
        "import sys\n",
        "import numpy as np\n",
        "\n",
        "import tvm\n",
        "import topi\n",
        "from topi.testing import conv2d_nchw_python\n",
        "\n",
        "from tvm import autotvm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u5YlgPYma_5r",
        "colab_type": "text"
      },
      "source": [
        "Step 0: Vanilla direct 2D convolution implementation without a tunable template\n",
        "---------------------------------------------------------------------------------------------\n",
        "\n",
        "We reuse the conv2d with NCHW data layout in the TVM operator inventory (TOPI).\n",
        "This definition gives us the default schedule (loop nest) seen below.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DhtW-j3bbMLI",
        "colab_type": "code",
        "outputId": "6b7307d4-fc46-49a0-9fd1-1a5890c68b6f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 550
        }
      },
      "source": [
        "# the last layer in resnet\n",
        "N, H, W, CO, CI, KH, KW, stride, padding = 1, 7, 7, 512, 512, 3, 3, (1, 1), (1, 1)\n",
        "assert N == 1, \"Only consider batch_size = 1 in this template\"\n",
        "\n",
        "data = tvm.placeholder((N, CI, H, W), name='data')\n",
        "kernel = tvm.placeholder((CO, CI, KH, KW), name='kernel')\n",
        "conv = topi.nn.conv2d_nchw(data, kernel, stride, padding, dilation=1, out_dtype='float32')\n",
        "s = tvm.create_schedule([conv.op])\n",
        "print(\"Default Schedule:\")\n",
        "print(tvm.lower(s, [data, kernel, conv], simple_mode=True))\n",
        "\n",
        "# assign axes of the default schedule to variables\n",
        "n, f, y, x = s[conv].op.axis\n",
        "rc, ry, rx = s[conv].op.reduce_axis"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Default Schedule:\n",
            "// attr [pad_temp] storage_scope = \"global\"\n",
            "allocate pad_temp[float32 * 41472]\n",
            "produce pad_temp {\n",
            "  for (i1, 0, 512) {\n",
            "    for (i2, 0, 9) {\n",
            "      for (i3, 0, 9) {\n",
            "        pad_temp[((((i1*9) + i2)*9) + i3)] = tvm_if_then_else(((((1 <= i2) && (i2 < 8)) && (1 <= i3)) && (i3 < 8)), data[(((((i1*7) + i2)*7) + i3) + -8)], 0.000000f)\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "produce compute {\n",
            "  for (ff, 0, 512) {\n",
            "    for (yy, 0, 7) {\n",
            "      for (xx, 0, 7) {\n",
            "        compute[((((ff*7) + yy)*7) + xx)] = 0.000000f\n",
            "        for (rc, 0, 512) {\n",
            "          for (ry, 0, 3) {\n",
            "            for (rx, 0, 3) {\n",
            "              compute[((((ff*7) + yy)*7) + xx)] = (compute[((((ff*7) + yy)*7) + xx)] + (pad_temp[((((((rc*9) + yy) + ry)*9) + xx) + rx)]*kernel[((((((ff*512) + rc)*3) + ry)*3) + rx)]))\n",
            "            }\n",
            "          }\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PYacZkkNd_xG",
        "colab_type": "text"
      },
      "source": [
        "Here, we inline padding into the computation (as opposed to padding in the input in a second operator) and declare cache stages. Cache stages are prepare a subset of the input (read) or output (write) for improved temporal locality with higher performance memories (e.g., registers and shared memory vs. global memory)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "afD69lgUd-QG",
        "colab_type": "code",
        "outputId": "a750b4f7-a772-4576-fe3b-c85de820ac7d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1329
        }
      },
      "source": [
        "# inline padding\n",
        "pad_data = s[conv].op.input_tensors[0]\n",
        "s[pad_data].compute_inline()\n",
        "input = data\n",
        "data, raw_data = pad_data, data\n",
        "\n",
        "output = conv\n",
        "OL = s.cache_write(conv, 'local')\n",
        "\n",
        "# create cache stage\n",
        "AA = s.cache_read(data, 'shared', [OL])\n",
        "WW = s.cache_read(kernel, 'shared', [OL])\n",
        "AL = s.cache_read(AA, 'local', [OL])\n",
        "WL = s.cache_read(WW, 'local', [OL])\n",
        "\n",
        "print(tvm.lower(s, [input, kernel, conv], simple_mode=True))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "// attr [pad_temp.shared] storage_scope = \"shared\"\n",
            "allocate pad_temp.shared[float32 * 41472]\n",
            "// attr [pad_temp.shared.local] storage_scope = \"local\"\n",
            "allocate pad_temp.shared.local[float32 * 41472]\n",
            "// attr [kernel.shared] storage_scope = \"shared\"\n",
            "allocate kernel.shared[float32 * 2359296]\n",
            "// attr [kernel.shared.local] storage_scope = \"local\"\n",
            "allocate kernel.shared.local[float32 * 2359296]\n",
            "// attr [compute.local] storage_scope = \"local\"\n",
            "allocate compute.local[float32 * 25088]\n",
            "produce pad_temp.shared {\n",
            "  for (ax1, 0, 512) {\n",
            "    for (ax2, 0, 9) {\n",
            "      for (ax3, 0, 9) {\n",
            "        pad_temp.shared[((((ax1*9) + ax2)*9) + ax3)] = tvm_if_then_else(((((1 <= ax2) && (ax2 < 8)) && (1 <= ax3)) && (ax3 < 8)), data[(((((ax1*7) + ax2)*7) + ax3) + -8)], 0.000000f)\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "produce pad_temp.shared.local {\n",
            "  for (ax1, 0, 512) {\n",
            "    for (ax2, 0, 9) {\n",
            "      for (ax3, 0, 9) {\n",
            "        pad_temp.shared.local[((((ax1*9) + ax2)*9) + ax3)] = pad_temp.shared[((((ax1*9) + ax2)*9) + ax3)]\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "produce kernel.shared {\n",
            "  for (ax0, 0, 512) {\n",
            "    for (ax1, 0, 512) {\n",
            "      for (ax2, 0, 3) {\n",
            "        for (ax3, 0, 3) {\n",
            "          kernel.shared[((((((ax0*512) + ax1)*3) + ax2)*3) + ax3)] = kernel[((((((ax0*512) + ax1)*3) + ax2)*3) + ax3)]\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "produce kernel.shared.local {\n",
            "  for (ax0, 0, 512) {\n",
            "    for (ax1, 0, 512) {\n",
            "      for (ax2, 0, 3) {\n",
            "        for (ax3, 0, 3) {\n",
            "          kernel.shared.local[((((((ax0*512) + ax1)*3) + ax2)*3) + ax3)] = kernel.shared[((((((ax0*512) + ax1)*3) + ax2)*3) + ax3)]\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "produce compute.local {\n",
            "  for (ff.c, 0, 512) {\n",
            "    for (yy.c, 0, 7) {\n",
            "      for (xx.c, 0, 7) {\n",
            "        compute.local[((((ff.c*7) + yy.c)*7) + xx.c)] = 0.000000f\n",
            "        for (rc, 0, 512) {\n",
            "          for (ry, 0, 3) {\n",
            "            for (rx, 0, 3) {\n",
            "              compute.local[((((ff.c*7) + yy.c)*7) + xx.c)] = (compute.local[((((ff.c*7) + yy.c)*7) + xx.c)] + (pad_temp.shared.local[((((((rc*9) + yy.c) + ry)*9) + xx.c) + rx)]*kernel.shared.local[((((((ff.c*512) + rc)*3) + ry)*3) + rx)]))\n",
            "            }\n",
            "          }\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "produce compute {\n",
            "  for (ff, 0, 512) {\n",
            "    for (yy, 0, 7) {\n",
            "      for (xx, 0, 7) {\n",
            "        compute[((((ff*7) + yy)*7) + xx)] = compute.local[((((ff*7) + yy)*7) + xx)]\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iDGX2K6MeWH6",
        "colab_type": "text"
      },
      "source": [
        "Here, we first grab the spatial axes from the schedule. Next, we define several magic numbers that are tiling factors that we use to split the original loop nest into one with several additional levels. We reorder the levels to redefine the computation order (and the memory access order) or the computation. As we will see in the next cell, this transformation also readies the schedule for a mapping from loop nests to GPU computation indicies (grids, blocks, threads)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yLaKLAH7eWRH",
        "colab_type": "code",
        "outputId": "5256675d-3ffd-41e6-c1e2-6ac2c52be1e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1363
        }
      },
      "source": [
        "# tile spatial axes\n",
        "n, f, y, x = s[output].op.axis\n",
        "tile_f_factors = [8, 8, 8, 1]\n",
        "tile_x_factors = [7, 7, 7, 1]\n",
        "tile_y_factors = [7, 7, 7, 1]\n",
        "\n",
        "bf, vf = s[output].split(f, factor=tile_f_factors[1])\n",
        "vf, tf = s[output].split(vf, factor=tile_f_factors[2])\n",
        "tf, fi = s[output].split(tf, factor=tile_f_factors[3])\n",
        "\n",
        "by, vy = s[output].split(y, factor=tile_y_factors[1])\n",
        "vy, ty = s[output].split(vy, factor=tile_y_factors[2])\n",
        "ty, yi = s[output].split(ty, factor=tile_y_factors[3])\n",
        "\n",
        "bx, vx = s[output].split(x, factor=tile_x_factors[1])\n",
        "vx, tx = s[output].split(vx, factor=tile_x_factors[2])\n",
        "tx, xi, = s[output].split(tx, factor=tile_x_factors[3])\n",
        "\n",
        "kernel_scope = n  # this is the scope to attach global config inside this kernel\n",
        "\n",
        "s[output].reorder(n, bf, by, bx, vf, vy, vx, tf, ty, tx, fi, yi, xi)\n",
        "print(tvm.lower(s, [input, kernel, conv], simple_mode=True))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "// attr [pad_temp.shared] storage_scope = \"shared\"\n",
            "allocate pad_temp.shared[float32 * 41472]\n",
            "// attr [pad_temp.shared.local] storage_scope = \"local\"\n",
            "allocate pad_temp.shared.local[float32 * 41472]\n",
            "// attr [kernel.shared] storage_scope = \"shared\"\n",
            "allocate kernel.shared[float32 * 2359296]\n",
            "// attr [kernel.shared.local] storage_scope = \"local\"\n",
            "allocate kernel.shared.local[float32 * 2359296]\n",
            "// attr [compute.local] storage_scope = \"local\"\n",
            "allocate compute.local[float32 * 25088]\n",
            "produce pad_temp.shared {\n",
            "  for (ax1, 0, 512) {\n",
            "    for (ax2, 0, 9) {\n",
            "      for (ax3, 0, 9) {\n",
            "        pad_temp.shared[((((ax1*9) + ax2)*9) + ax3)] = tvm_if_then_else(((((1 <= ax2) && (ax2 < 8)) && (1 <= ax3)) && (ax3 < 8)), data[(((((ax1*7) + ax2)*7) + ax3) + -8)], 0.000000f)\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "produce pad_temp.shared.local {\n",
            "  for (ax1, 0, 512) {\n",
            "    for (ax2, 0, 9) {\n",
            "      for (ax3, 0, 9) {\n",
            "        pad_temp.shared.local[((((ax1*9) + ax2)*9) + ax3)] = pad_temp.shared[((((ax1*9) + ax2)*9) + ax3)]\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "produce kernel.shared {\n",
            "  for (ax0, 0, 512) {\n",
            "    for (ax1, 0, 512) {\n",
            "      for (ax2, 0, 3) {\n",
            "        for (ax3, 0, 3) {\n",
            "          kernel.shared[((((((ax0*512) + ax1)*3) + ax2)*3) + ax3)] = kernel[((((((ax0*512) + ax1)*3) + ax2)*3) + ax3)]\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "produce kernel.shared.local {\n",
            "  for (ax0, 0, 512) {\n",
            "    for (ax1, 0, 512) {\n",
            "      for (ax2, 0, 3) {\n",
            "        for (ax3, 0, 3) {\n",
            "          kernel.shared.local[((((((ax0*512) + ax1)*3) + ax2)*3) + ax3)] = kernel.shared[((((((ax0*512) + ax1)*3) + ax2)*3) + ax3)]\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "produce compute.local {\n",
            "  for (ff.c, 0, 512) {\n",
            "    for (yy.c, 0, 7) {\n",
            "      for (xx.c, 0, 7) {\n",
            "        compute.local[((((ff.c*7) + yy.c)*7) + xx.c)] = 0.000000f\n",
            "        for (rc, 0, 512) {\n",
            "          for (ry, 0, 3) {\n",
            "            for (rx, 0, 3) {\n",
            "              compute.local[((((ff.c*7) + yy.c)*7) + xx.c)] = (compute.local[((((ff.c*7) + yy.c)*7) + xx.c)] + (pad_temp.shared.local[((((((rc*9) + yy.c) + ry)*9) + xx.c) + rx)]*kernel.shared.local[((((((ff.c*512) + rc)*3) + ry)*3) + rx)]))\n",
            "            }\n",
            "          }\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "produce compute {\n",
            "  for (ff.outer, 0, 64) {\n",
            "    for (ff.inner.inner.outer, 0, 8) {\n",
            "      for (yy.inner.inner.outer, 0, 7) {\n",
            "        for (xx.inner.inner.outer, 0, 7) {\n",
            "          compute[((((((ff.outer*8) + ff.inner.inner.outer)*7) + yy.inner.inner.outer)*7) + xx.inner.inner.outer)] = compute.local[((((((ff.outer*8) + ff.inner.inner.outer)*7) + yy.inner.inner.outer)*7) + xx.inner.inner.outer)]\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RH6gor8XjvXM",
        "colab_type": "text"
      },
      "source": [
        "After reshaping the loop nest, we can bind portions of the computations to GPU blocks and threads. Additionally, we bind some loops to \"virtual\" threads which are effectively threads emulated in software. Virtual threads enable the expression of more sophisticated computation and memory access patterns vs. blocks and threads alone. Note binding effictively removes the associated loop axes from the schedule, as they are now parallelized based on their index instead."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_yIhMYiQjvhQ",
        "colab_type": "code",
        "outputId": "49afb33b-da32-45c2-f428-cfa10c41a571",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1125
        }
      },
      "source": [
        "s[output].bind(bf, tvm.thread_axis(\"blockIdx.z\"))\n",
        "s[output].bind(by, tvm.thread_axis(\"blockIdx.y\"))\n",
        "s[output].bind(bx, tvm.thread_axis(\"blockIdx.x\"))\n",
        "s[output].bind(vf, tvm.thread_axis(\"vthread\"))\n",
        "s[output].bind(vy, tvm.thread_axis(\"vthread\"))\n",
        "s[output].bind(vx, tvm.thread_axis(\"vthread\"))\n",
        "s[output].bind(tf, tvm.thread_axis(\"threadIdx.z\"))\n",
        "s[output].bind(ty, tvm.thread_axis(\"threadIdx.y\"))\n",
        "s[output].bind(tx, tvm.thread_axis(\"threadIdx.x\"))\n",
        "s[OL].compute_at(s[output], tx)\n",
        "print(tvm.lower(s, [input, kernel, output], simple_mode=True))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "// attr [pad_temp.shared] storage_scope = \"shared\"\n",
            "allocate pad_temp.shared[float32 * 4608]\n",
            "// attr [pad_temp.shared.local] storage_scope = \"local\"\n",
            "allocate pad_temp.shared.local[float32 * 4608]\n",
            "// attr [kernel.shared.local] storage_scope = \"local\"\n",
            "allocate kernel.shared.local[float32 * 4608]\n",
            "produce pad_temp.shared {\n",
            "  for (ax1, 0, 512) {\n",
            "    for (ax2, 0, 3) {\n",
            "      for (ax3, 0, 3) {\n",
            "        pad_temp.shared[((((ax1*3) + ax2)*3) + ax3)] = tvm_if_then_else((((((1 - threadIdx.y) <= ax2) && (ax2 < (8 - threadIdx.y))) && ((1 - threadIdx.x) <= ax3)) && (ax3 < (8 - threadIdx.x))), data[(((((((ax1*7) + ax2) + threadIdx.y)*7) + ax3) + threadIdx.x) + -8)], 0.000000f)\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "produce pad_temp.shared.local {\n",
            "  for (ax1, 0, 512) {\n",
            "    for (ax2, 0, 3) {\n",
            "      for (ax3, 0, 3) {\n",
            "        pad_temp.shared.local[((((ax1*3) + ax2)*3) + ax3)] = pad_temp.shared[((((ax1*3) + ax2)*3) + ax3)]\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "produce kernel.shared {\n",
            "  for (ax1, 0, 512) {\n",
            "    for (ax2, 0, 3) {\n",
            "      for (ax3, 0, 3) {\n",
            "        pad_temp.shared[((((ax1*3) + ax2)*3) + ax3)] = kernel[((((((((blockIdx.z*8) + threadIdx.z)*512) + ax1)*3) + ax2)*3) + ax3)]\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "produce kernel.shared.local {\n",
            "  for (ax1, 0, 512) {\n",
            "    for (ax2, 0, 3) {\n",
            "      for (ax3, 0, 3) {\n",
            "        kernel.shared.local[((((ax1*3) + ax2)*3) + ax3)] = pad_temp.shared[((((ax1*3) + ax2)*3) + ax3)]\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "produce compute {\n",
            "  // attr [iter_var(blockIdx.z, , blockIdx.z)] thread_extent = 64\n",
            "  // attr [compute.local] storage_scope = \"local\"\n",
            "  allocate compute.local[float32 * 1]\n",
            "  // attr [iter_var(blockIdx.y, , blockIdx.y)] thread_extent = 1\n",
            "  // attr [iter_var(blockIdx.x, , blockIdx.x)] thread_extent = 1\n",
            "  // attr [iter_var(threadIdx.z, , threadIdx.z)] thread_extent = 8\n",
            "  // attr [iter_var(threadIdx.y, , threadIdx.y)] thread_extent = 7\n",
            "  // attr [iter_var(threadIdx.x, , threadIdx.x)] thread_extent = 7\n",
            "  produce compute.local {\n",
            "    compute.local[0] = 0.000000f\n",
            "    for (rc, 0, 512) {\n",
            "      for (ry, 0, 3) {\n",
            "        for (rx, 0, 3) {\n",
            "          compute.local[0] = (compute.local[0] + (pad_temp.shared.local[((((rc*3) + ry)*3) + rx)]*kernel.shared.local[((((rc*3) + ry)*3) + rx)]))\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "  compute[((((((blockIdx.z*8) + threadIdx.z)*7) + threadIdx.y)*7) + threadIdx.x)] = compute.local[0]\n",
            "}\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QSicMUUukiu2",
        "colab_type": "text"
      },
      "source": [
        "Next, we apply a tiling transformation over the reduction axes, using a series of loop axes splits followed by a reorder as in the previous case. With this arrangement of loop axes, we also define the points at which each cached tensor is prepared to be read or written with `compute_at`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VT992Qu6ki6n",
        "colab_type": "code",
        "outputId": "1c93c3cc-4d5a-4096-9e9a-5e6fc564ef50",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1023
        }
      },
      "source": [
        "# tile reduction axes\n",
        "n, f, y, x = s[OL].op.axis\n",
        "rc, ry, rx = s[OL].op.reduce_axis\n",
        "rc_factors = [512, 32, 1]\n",
        "rx_factors = [3, 3, 1]\n",
        "ry_factors = [3, 3, 1]\n",
        "rco, rcm = s[OL].split(rc, factor=rc_factors[1])\n",
        "rcm, rci = s[OL].split(rcm, factor=rc_factors[2])\n",
        "ryo, rym = s[OL].split(ry, factor=ry_factors[1])\n",
        "rym, ryi = s[OL].split(rym, factor=ry_factors[2])\n",
        "rxo, rxm = s[OL].split(rx, factor=rx_factors[1])\n",
        "rxm, rxi = s[OL].split(rxm, factor=rx_factors[2])\n",
        "s[OL].reorder(rco, ryo, rxo, rcm, rym, rxm, rci, ryi, rxi, n, f, y, x)\n",
        "\n",
        "s[AA].compute_at(s[OL], rxo)\n",
        "s[WW].compute_at(s[OL], rxo)\n",
        "s[AL].compute_at(s[OL], rxm)\n",
        "s[WL].compute_at(s[OL], rxm)\n",
        "\n",
        "print(tvm.lower(s, [input, kernel, output], simple_mode=True))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "produce compute {\n",
            "  // attr [iter_var(blockIdx.z, , blockIdx.z)] thread_extent = 64\n",
            "  // attr [compute.local] storage_scope = \"local\"\n",
            "  allocate compute.local[float32 * 1]\n",
            "  // attr [pad_temp.shared] storage_scope = \"shared\"\n",
            "  allocate pad_temp.shared[float32 * 2592]\n",
            "  // attr [kernel.shared] storage_scope = \"shared\"\n",
            "  allocate kernel.shared[float32 * 2304]\n",
            "  // attr [pad_temp.shared.local] storage_scope = \"local\"\n",
            "  allocate pad_temp.shared.local[float32 * 1]\n",
            "  // attr [kernel.shared.local] storage_scope = \"local\"\n",
            "  allocate kernel.shared.local[float32 * 1]\n",
            "  // attr [iter_var(blockIdx.y, , blockIdx.y)] thread_extent = 1\n",
            "  // attr [iter_var(blockIdx.x, , blockIdx.x)] thread_extent = 1\n",
            "  // attr [iter_var(threadIdx.z, , threadIdx.z)] thread_extent = 8\n",
            "  // attr [iter_var(threadIdx.y, , threadIdx.y)] thread_extent = 7\n",
            "  // attr [iter_var(threadIdx.x, , threadIdx.x)] thread_extent = 7\n",
            "  produce compute.local {\n",
            "    compute.local[0] = 0.000000f\n",
            "    for (rc.outer, 0, 16) {\n",
            "      produce pad_temp.shared {\n",
            "        for (ax1, 0, 32) {\n",
            "          for (ax2, 0, 9) {\n",
            "            for (ax3, 0, 9) {\n",
            "              pad_temp.shared[((((ax1*9) + ax2)*9) + ax3)] = tvm_if_then_else(((((1 <= ax2) && (ax2 < 8)) && (1 <= ax3)) && (ax3 < 8)), data[(((((((rc.outer*32) + ax1)*7) + ax2)*7) + ax3) + -8)], 0.000000f)\n",
            "            }\n",
            "          }\n",
            "        }\n",
            "      }\n",
            "      produce kernel.shared {\n",
            "        for (ax0, 0, 8) {\n",
            "          for (ax1, 0, 32) {\n",
            "            for (ax2, 0, 3) {\n",
            "              for (ax3, 0, 3) {\n",
            "                kernel.shared[((((((ax0*32) + ax1)*3) + ax2)*3) + ax3)] = kernel[((((((((((blockIdx.z*8) + ax0)*16) + rc.outer)*32) + ax1)*3) + ax2)*3) + ax3)]\n",
            "              }\n",
            "            }\n",
            "          }\n",
            "        }\n",
            "      }\n",
            "      for (rc.inner.outer, 0, 32) {\n",
            "        for (ry.inner.outer, 0, 3) {\n",
            "          for (rx.inner.outer, 0, 3) {\n",
            "            produce pad_temp.shared.local {\n",
            "              pad_temp.shared.local[0] = pad_temp.shared[((((((rc.inner.outer*9) + threadIdx.y) + ry.inner.outer)*9) + threadIdx.x) + rx.inner.outer)]\n",
            "            }\n",
            "            produce kernel.shared.local {\n",
            "              kernel.shared.local[0] = kernel.shared[((((((threadIdx.z*32) + rc.inner.outer)*3) + ry.inner.outer)*3) + rx.inner.outer)]\n",
            "            }\n",
            "            compute.local[0] = (compute.local[0] + (pad_temp.shared.local[0]*kernel.shared.local[0]))\n",
            "          }\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "  compute[((((((blockIdx.z*8) + threadIdx.z)*7) + threadIdx.y)*7) + threadIdx.x)] = compute.local[0]\n",
            "}\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NfBPrhkRmMKC",
        "colab_type": "text"
      },
      "source": [
        "Next, we schedule the cooperative fetching of data and weighs for the threads in each thread block. We use the `bind` schedule primitive as before, with the main difference being that we must split the thread axes to match the number of threads that have already been declared in the prevous spatial tiling step.\n",
        "For simplicity, we omit the specification of compiler directives (e.g., loop unrolling) in this part of the tutorial---we include them when showing the full AutoTVM tuining example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XdW3ZMB9mMVA",
        "colab_type": "code",
        "outputId": "777c268e-6345-459f-b555-bd66d1632b32",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1329
        }
      },
      "source": [
        "# cooperative fetching\n",
        "for load in [AA, WW]:\n",
        "    n, f, y, x = s[load].op.axis \n",
        "    fused = s[load].fuse(n, f, y, x)\n",
        "    tz, fused = s[load].split(fused, nparts=tile_f_factors[2])\n",
        "    ty, fused = s[load].split(fused, nparts=tile_y_factors[2])\n",
        "    tx, fused = s[load].split(fused, nparts=tile_x_factors[2])\n",
        "    s[load].bind(tz, tvm.thread_axis(\"threadIdx.z\"))\n",
        "    s[load].bind(ty, tvm.thread_axis(\"threadIdx.y\"))\n",
        "    s[load].bind(tx, tvm.thread_axis(\"threadIdx.x\"))\n",
        "print(tvm.lower(s, [input, kernel, output], simple_mode=True))\n",
        "# tune unroll\n",
        "#s[output].pragma(kernel_scope, 'auto_unroll_max_step', cfg['auto_unroll_max_step'].val)\n",
        "#s[output].pragma(kernel_scope, 'unroll_explicit', cfg['unroll_explicit'].val)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "produce compute {\n",
            "  // attr [iter_var(blockIdx.z, , blockIdx.z)] thread_extent = 64\n",
            "  // attr [compute.local] storage_scope = \"local\"\n",
            "  allocate compute.local[float32 * 1]\n",
            "  // attr [pad_temp.shared] storage_scope = \"shared\"\n",
            "  allocate pad_temp.shared[float32 * 2592]\n",
            "  // attr [kernel.shared] storage_scope = \"shared\"\n",
            "  allocate kernel.shared[float32 * 2304]\n",
            "  // attr [pad_temp.shared.local] storage_scope = \"local\"\n",
            "  allocate pad_temp.shared.local[float32 * 1]\n",
            "  // attr [kernel.shared.local] storage_scope = \"local\"\n",
            "  allocate kernel.shared.local[float32 * 1]\n",
            "  // attr [iter_var(blockIdx.y, , blockIdx.y)] thread_extent = 1\n",
            "  // attr [iter_var(blockIdx.x, , blockIdx.x)] thread_extent = 1\n",
            "  // attr [iter_var(threadIdx.z, , threadIdx.z)] thread_extent = 8\n",
            "  // attr [iter_var(threadIdx.y, , threadIdx.y)] thread_extent = 7\n",
            "  // attr [iter_var(threadIdx.x, , threadIdx.x)] thread_extent = 7\n",
            "  produce compute.local {\n",
            "    compute.local[0] = 0.000000f\n",
            "    for (rc.outer, 0, 16) {\n",
            "      produce pad_temp.shared {\n",
            "        // attr [iter_var(threadIdx.z, , threadIdx.z)] thread_extent = 8\n",
            "        // attr [iter_var(threadIdx.y, , threadIdx.y)] thread_extent = 7\n",
            "        // attr [iter_var(threadIdx.x, , threadIdx.x)] thread_extent = 7\n",
            "        for (ax0.ax1.fused.ax2.fused.ax3.fused.inner.inner.inner, 0, 7) {\n",
            "          if (likely(((threadIdx.z*4) < (32 - ((((threadIdx.y*47) + (threadIdx.x*7)) + ax0.ax1.fused.ax2.fused.ax3.fused.inner.inner.inner)/81))))) {\n",
            "            if (likely(((threadIdx.z*36) < (288 - ((((threadIdx.y*47) + (threadIdx.x*7)) + ax0.ax1.fused.ax2.fused.ax3.fused.inner.inner.inner)/9))))) {\n",
            "              if (likely(((threadIdx.z*324) < (((2592 - ax0.ax1.fused.ax2.fused.ax3.fused.inner.inner.inner) - (threadIdx.x*7)) - (threadIdx.y*47))))) {\n",
            "                if (likely(((threadIdx.y*47) < ((324 - ax0.ax1.fused.ax2.fused.ax3.fused.inner.inner.inner) - (threadIdx.x*7))))) {\n",
            "                  if (likely(((threadIdx.x*7) < (47 - ax0.ax1.fused.ax2.fused.ax3.fused.inner.inner.inner)))) {\n",
            "                    pad_temp.shared[((threadIdx.z*324) + (((threadIdx.y*47) + (threadIdx.x*7)) + ax0.ax1.fused.ax2.fused.ax3.fused.inner.inner.inner))] = tvm_if_then_else(((((9 <= ((((threadIdx.y*47) + (threadIdx.x*7)) + ax0.ax1.fused.ax2.fused.ax3.fused.inner.inner.inner) % 81)) && (((((threadIdx.y*47) + (threadIdx.x*7)) + ax0.ax1.fused.ax2.fused.ax3.fused.inner.inner.inner) % 81) < 72)) && (1 <= ((((threadIdx.y*47) + (threadIdx.x*7)) + ax0.ax1.fused.ax2.fused.ax3.fused.inner.inner.inner) % 9))) && (((((threadIdx.y*47) + (threadIdx.x*7)) + ax0.ax1.fused.ax2.fused.ax3.fused.inner.inner.inner) % 9) < 8)), data[((((((((((((threadIdx.z*4) + ((((threadIdx.y*47) + (threadIdx.x*7)) + ax0.ax1.fused.ax2.fused.ax3.fused.inner.inner.inner)/81))/32)*16) + rc.outer)*32) + (((threadIdx.z*4) + ((((threadIdx.y*47) + (threadIdx.x*7)) + ax0.ax1.fused.ax2.fused.ax3.fused.inner.inner.inner)/81)) % 32))*7) + (((((threadIdx.y*47) + (threadIdx.x*7)) + ax0.ax1.fused.ax2.fused.ax3.fused.inner.inner.inner) % 81)/9))*7) + ((((threadIdx.y*47) + (threadIdx.x*7)) + ax0.ax1.fused.ax2.fused.ax3.fused.inner.inner.inner) % 9)) + -8)], 0.000000f)\n",
            "                  }\n",
            "                }\n",
            "              }\n",
            "            }\n",
            "          }\n",
            "        }\n",
            "      }\n",
            "      produce kernel.shared {\n",
            "        // attr [iter_var(threadIdx.z, , threadIdx.z)] thread_extent = 8\n",
            "        // attr [iter_var(threadIdx.y, , threadIdx.y)] thread_extent = 7\n",
            "        // attr [iter_var(threadIdx.x, , threadIdx.x)] thread_extent = 7\n",
            "        for (ax0.ax1.fused.ax2.fused.ax3.fused.inner.inner.inner, 0, 6) {\n",
            "          if (likely(((((((threadIdx.y*7) + threadIdx.x)*2) + (ax0.ax1.fused.ax2.fused.ax3.fused.inner.inner.inner/3))/96) < (8 - threadIdx.z)))) {\n",
            "            if (likely(((threadIdx.z*32) < (256 - (((((threadIdx.y*7) + threadIdx.x)*2) + (ax0.ax1.fused.ax2.fused.ax3.fused.inner.inner.inner/3))/3))))) {\n",
            "              if (likely(((threadIdx.z*96) < (((768 - (ax0.ax1.fused.ax2.fused.ax3.fused.inner.inner.inner/3)) - (threadIdx.x*2)) - (threadIdx.y*14))))) {\n",
            "                if (likely(((threadIdx.z*288) < (((2304 - ax0.ax1.fused.ax2.fused.ax3.fused.inner.inner.inner) - (threadIdx.x*6)) - (threadIdx.y*42))))) {\n",
            "                  if (likely(((((threadIdx.y*7) + threadIdx.x)*6) < (288 - ax0.ax1.fused.ax2.fused.ax3.fused.inner.inner.inner)))) {\n",
            "                    if (likely(((blockIdx.z*8) < ((512 - threadIdx.z) - (((((threadIdx.y*7) + threadIdx.x)*2) + (ax0.ax1.fused.ax2.fused.ax3.fused.inner.inner.inner/3))/96))))) {\n",
            "                      kernel.shared[((((threadIdx.z*96) + ((((threadIdx.y*7) + threadIdx.x)*2) + (ax0.ax1.fused.ax2.fused.ax3.fused.inner.inner.inner/3)))*3) + (ax0.ax1.fused.ax2.fused.ax3.fused.inner.inner.inner % 3))] = kernel[(((((((((blockIdx.z*8) + (((((threadIdx.y*7) + threadIdx.x)*2) + (ax0.ax1.fused.ax2.fused.ax3.fused.inner.inner.inner/3))/96)) + threadIdx.z)*16) + rc.outer)*96) + (((((threadIdx.y*7) + threadIdx.x)*2) + (ax0.ax1.fused.ax2.fused.ax3.fused.inner.inner.inner/3)) % 96))*3) + (ax0.ax1.fused.ax2.fused.ax3.fused.inner.inner.inner % 3))]\n",
            "                    }\n",
            "                  }\n",
            "                }\n",
            "              }\n",
            "            }\n",
            "          }\n",
            "        }\n",
            "      }\n",
            "      for (rc.inner.outer, 0, 32) {\n",
            "        for (ry.inner.outer, 0, 3) {\n",
            "          for (rx.inner.outer, 0, 3) {\n",
            "            produce pad_temp.shared.local {\n",
            "              pad_temp.shared.local[0] = pad_temp.shared[((((((rc.inner.outer*9) + threadIdx.y) + ry.inner.outer)*9) + threadIdx.x) + rx.inner.outer)]\n",
            "            }\n",
            "            produce kernel.shared.local {\n",
            "              kernel.shared.local[0] = kernel.shared[((((((threadIdx.z*32) + rc.inner.outer)*3) + ry.inner.outer)*3) + rx.inner.outer)]\n",
            "            }\n",
            "            compute.local[0] = (compute.local[0] + (pad_temp.shared.local[0]*kernel.shared.local[0]))\n",
            "          }\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "  compute[((((((blockIdx.z*8) + threadIdx.z)*7) + threadIdx.y)*7) + threadIdx.x)] = compute.local[0]\n",
            "}\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRZZDR5wrRA_",
        "colab_type": "text"
      },
      "source": [
        "Finally, we run a reference implementation to generate reference results to check the correctness of the scheduled code and measure the performance of the scheduled code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jmYLCxlbrRKW",
        "colab_type": "code",
        "outputId": "6fe2986b-48fd-474d-d030-b2f55c6681e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# check correctness\n",
        "a_np = np.random.uniform(size=(N, CI, H, W)).astype(np.float32)\n",
        "w_np = np.random.uniform(size=(CO, CI, KH, KW)).astype(np.float32)\n",
        "c_np = conv2d_nchw_python(a_np, w_np, stride, padding)\n",
        "\n",
        "# compile the manually schedule convolution\n",
        "with tvm.target.create('cuda'):\n",
        "    manual_conv2d = tvm.build(s, [input, kernel, output])\n",
        "    \n",
        "ctx = tvm.gpu()\n",
        "a_tvm = tvm.nd.array(a_np, ctx=ctx)\n",
        "w_tvm = tvm.nd.array(w_np, ctx=ctx)\n",
        "c_tvm = tvm.nd.empty(c_np.shape, ctx=ctx)\n",
        "manual_conv2d(a_tvm, w_tvm, c_tvm)\n",
        "\n",
        "tvm.testing.assert_allclose(c_np, c_tvm.asnumpy(), rtol=1e-2)\n",
        "\n",
        "evaluator = manual_conv2d.time_evaluator(manual_conv2d.entry_name, ctx, number=400)\n",
        "mean = evaluator(a_tvm, w_tvm, c_tvm).mean\n",
        "print(\"complexity: \", autotvm.task.task.compute_flop(s))\n",
        "print(\"Time cost of this operator: %f\" % mean)\n",
        "manual_flops = autotvm.task.task.compute_flop(s)/mean\n",
        "print(\"GFLOPS:\", manual_flops/1e9)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "complexity:  231501312\n",
            "Time cost of this operator: 0.000992\n",
            "GFLOPS: 233.4336871503306\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8R0J9j5m_CBY",
        "colab_type": "text"
      },
      "source": [
        "Using a schedule template instead of a manually defined schedule\n",
        "====================================================\n",
        "\n",
        "Next, we show that we can avoid the magic numbers used previously and instead leave them as free variables to be decided by a tuner. This change relieves the burden of tuning on the schedule writer and also potentially opens up a much large space for optimization.\n",
        "\n",
        "From a high level schedule template is identical to the manual schedule defined previously, with hardcoded values replaced with configuration option declarations. Note that AutoTVM also provides some syntactic sugar for splitting (using `define_split`) a single axis into multiple axes at once with the `num_outputs` parameter, instead of only splitting each axis into two each time with `split`. Also note that each configuration option is now applied with calls to `cfg[...].apply(...)`.\n",
        "\n",
        "Note that we wrap the schedule in a function to leverage the `@autotvm.template` decorator for automated tuning.\n",
        "Tunable parameters that were previously manually specified are now passed using the `cfg` variable in the schedule."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_sgFW_oi_OHr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@autotvm.template\n",
        "def conv2d_no_batching(N, H, W, CO, CI, KH, KW, stride, padding):\n",
        "    assert N == 1, \"Only consider batch_size = 1 in this template\"\n",
        "\n",
        "    data = tvm.placeholder((N, CI, H, W), name='data')\n",
        "    kernel = tvm.placeholder((CO, CI, KH, KW), name='kernel')\n",
        "    conv = topi.nn.conv2d_nchw(data, kernel, stride, padding, dilation=1, out_dtype='float32')\n",
        "    s = tvm.create_schedule([conv.op])\n",
        "\n",
        "    ##### space definition begin #####\n",
        "    n, f, y, x = s[conv].op.axis\n",
        "    rc, ry, rx = s[conv].op.reduce_axis\n",
        "\n",
        "    cfg = autotvm.get_config()\n",
        "    cfg.define_split(\"tile_f\", f, num_outputs=4)\n",
        "    cfg.define_split(\"tile_y\", y, num_outputs=4)\n",
        "    cfg.define_split(\"tile_x\", x, num_outputs=4)\n",
        "    cfg.define_split(\"tile_rc\", rc, num_outputs=3)\n",
        "    cfg.define_split(\"tile_ry\", ry, num_outputs=3)\n",
        "    cfg.define_split(\"tile_rx\", rx, num_outputs=3)\n",
        "    cfg.define_knob(\"auto_unroll_max_step\", [0, 512, 1500])\n",
        "    cfg.define_knob(\"unroll_explicit\", [0, 1])\n",
        "    ##### space definition end #####\n",
        "\n",
        "    # inline padding\n",
        "    pad_data = s[conv].op.input_tensors[0]\n",
        "    s[pad_data].compute_inline()\n",
        "    data, raw_data = pad_data, data\n",
        "\n",
        "    output = conv\n",
        "    OL = s.cache_write(conv, 'local')\n",
        "\n",
        "    # create cache stage\n",
        "    AA = s.cache_read(data, 'shared', [OL])\n",
        "    WW = s.cache_read(kernel, 'shared', [OL])\n",
        "    AL = s.cache_read(AA, 'local', [OL])\n",
        "    WL = s.cache_read(WW, 'local', [OL])\n",
        "\n",
        "    # tile and bind spatial axes\n",
        "    n, f, y, x = s[output].op.axis\n",
        "    bf, vf, tf, fi = cfg[\"tile_f\"].apply(s, output, f)\n",
        "    by, vy, ty, yi = cfg[\"tile_y\"].apply(s, output, y)\n",
        "    bx, vx, tx, xi = cfg[\"tile_x\"].apply(s, output, x)\n",
        "    kernel_scope = n  # this is the scope to attach global config inside this kernel\n",
        "\n",
        "    s[output].bind(bf, tvm.thread_axis(\"blockIdx.z\"))\n",
        "    s[output].bind(by, tvm.thread_axis(\"blockIdx.y\"))\n",
        "    s[output].bind(bx, tvm.thread_axis(\"blockIdx.x\"))\n",
        "    s[output].bind(vf, tvm.thread_axis(\"vthread\"))\n",
        "    s[output].bind(vy, tvm.thread_axis(\"vthread\"))\n",
        "    s[output].bind(vx, tvm.thread_axis(\"vthread\"))\n",
        "    s[output].bind(tf, tvm.thread_axis(\"threadIdx.z\"))\n",
        "    s[output].bind(ty, tvm.thread_axis(\"threadIdx.y\"))\n",
        "    s[output].bind(tx, tvm.thread_axis(\"threadIdx.x\"))\n",
        "    s[output].reorder(n, bf, by, bx, vf, vy, vx, tf, ty, tx, fi, yi, xi)\n",
        "    s[OL].compute_at(s[output], tx)\n",
        "\n",
        "    # tile reduction axes\n",
        "    n, f, y, x = s[OL].op.axis\n",
        "    rc, ry, rx = s[OL].op.reduce_axis\n",
        "    rco, rcm, rci = cfg['tile_rc'].apply(s, OL, rc)\n",
        "    ryo, rym, ryi = cfg['tile_rx'].apply(s, OL, ry)\n",
        "    rxo, rxm, rxi = cfg['tile_ry'].apply(s, OL, rx)\n",
        "    s[OL].reorder(rco, ryo, rxo, rcm, rym, rxm, rci, ryi, rxi, n, f, y, x)\n",
        "\n",
        "    s[AA].compute_at(s[OL], rxo)\n",
        "    s[WW].compute_at(s[OL], rxo)\n",
        "    s[AL].compute_at(s[OL], rxm)\n",
        "    s[WL].compute_at(s[OL], rxm)\n",
        "\n",
        "    # cooperative fetching\n",
        "    for load in [AA, WW]:\n",
        "        n, f, y, x = s[load].op.axis\n",
        "        fused = s[load].fuse(n, f, y, x)\n",
        "        tz, fused = s[load].split(fused, nparts=cfg[\"tile_f\"].size[2])\n",
        "        ty, fused = s[load].split(fused, nparts=cfg[\"tile_y\"].size[2])\n",
        "        tx, fused = s[load].split(fused, nparts=cfg[\"tile_x\"].size[2])\n",
        "        s[load].bind(tz, tvm.thread_axis(\"threadIdx.z\"))\n",
        "        s[load].bind(ty, tvm.thread_axis(\"threadIdx.y\"))\n",
        "        s[load].bind(tx, tvm.thread_axis(\"threadIdx.x\"))\n",
        "\n",
        "    # tune unroll\n",
        "    s[output].pragma(kernel_scope, 'auto_unroll_max_step', cfg['auto_unroll_max_step'].val)\n",
        "    s[output].pragma(kernel_scope, 'unroll_explicit', cfg['unroll_explicit'].val)\n",
        "\n",
        "    return s, [raw_data, kernel, conv]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-CFfMD6Ie1O",
        "colab_type": "text"
      },
      "source": [
        "Start Infrastructure for Tuning (tracker)\n",
        "===============================\n",
        "AutoTVM leverages the TVM RPC system to abstract and multiplex the actual hardware tuning targets. TVM RPC provides a tracker that distributes hardware resources so that multiple tuning jobs can share a pool of hardware devices. In this case we start a tracker instance on the machine running the laboratory notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Ch6kPVaIfDo",
        "colab_type": "code",
        "outputId": "0abad3ba-bffa-4140-c184-79d887b416df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%%script bash --bg --out output --err error\n",
        "PYTHONPATH=/tvm/python:$PYTHONPATH && python3 -m tvm.exec.rpc_tracker --host 0.0.0.0 --port 9190 &\n",
        "while true; do\n",
        "  res=$(PYTHONPATH=/tvm/python:$PYTHONPATH && python3 -m tvm.exec.query_rpc_tracker --host 0.0.0.0 --port 9190 2>&1 | grep 'Cannot connect to tracker')\n",
        "  if [ \"$res\" == \"\" ]; then\n",
        "    echo \"OK @ \" $(date) \"...\" >> status.log\n",
        "  else\n",
        "    echo \"RESTARTING @ \" $(date) \"...\" >> status.log\n",
        "    PYTHONPATH=/tvm/python:$PYTHONPATH && python3 -m tvm.exec.rpc_tracker --host 0.0.0.0 --port 9190 &\n",
        "  fi\n",
        "  sleep 5\n",
        "done"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting job # 0 in a separate thread.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8v4NcPtIm8R",
        "colab_type": "text"
      },
      "source": [
        "Start Infrastructure for Tuning (server)\n",
        "===============================\n",
        "We then start an RPC server instance that manages this notebook's GPU, and configure it to report to the tracker."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "liZMOINaIm_v",
        "colab_type": "code",
        "outputId": "8db6644e-f905-4832-9c04-4793e60ee707",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%%script bash --bg --out output2 --err error2\n",
        "while true; do\n",
        "echo \"started server at \" $(date) >> status.log\n",
        "PYTHONPATH=/tvm/python:/tvm/topi/python:$PYTHONPATH && python3 -m tvm.exec.rpc_server --key 1080ti --tracker 0.0.0.0:9190\n",
        "sleep 30\n",
        "done"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting job # 2 in a separate thread.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TRzivXLpKtCk",
        "colab_type": "text"
      },
      "source": [
        "Check the status of the tracker\n",
        "========================="
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v6R8SoScKtJ6",
        "colab_type": "code",
        "outputId": "9c33c0dd-3cf8-4487-e7f6-bb213c8fda27",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 335
        }
      },
      "source": [
        "! cat status.log | tail\n",
        "! PYTHONPATH=/tvm/python:$PYTHONPATH && python3 -m tvm.exec.query_rpc_tracker --host 0.0.0.0 --port 9190 "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "started server at  Mon Jun 17 23:59:13 UTC 2019\n",
            "Tracker address 0.0.0.0:9190\n",
            "\n",
            "Server List\n",
            "----------------------------\n",
            "server-address\tkey\n",
            "----------------------------\n",
            "127.0.0.1:43944\tserver:1080ti\n",
            "----------------------------\n",
            "\n",
            "Queue Status\n",
            "------------------------------\n",
            "key      total  free  pending\n",
            "------------------------------\n",
            "1080ti   1      1     0      \n",
            "------------------------------\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fC6QtSgnw29Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "peak = 0.0\n",
        "count = 0\n",
        "from matplotlib import pyplot as plt\n",
        "from IPython.display import clear_output\n",
        "\n",
        "def plot_callback():\n",
        "  y = list()\n",
        "  x = list()\n",
        "  fig,ax = plt.subplots(1,1)\n",
        "  def _callback(_, inputs, results):\n",
        "    global peak\n",
        "    global count\n",
        "    for inp, res in zip(inputs, results):\n",
        "      count += 1\n",
        "      if res.error_no == 0:\n",
        "        cost = np.mean(res.costs)\n",
        "        perf = inp.task.flop/cost\n",
        "        if perf > peak:\n",
        "          print(\"reached new peak: {:.2f} GFLOPS\".format(perf/1e9))\n",
        "          peak = perf\n",
        "      x.append(count)\n",
        "      y.append(peak)\n",
        "      plt.plot(x, y)\n",
        "      plt.axhline(y=manual_flops, color='r', linestyle=':')\n",
        "      plt.ylabel('performance (FLOP/S)')\n",
        "      plt.xlabel('trials run')\n",
        "      clear_output()\n",
        "      plt.show()\n",
        "  return _callback"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZfznWuPiIVQw",
        "colab_type": "text"
      },
      "source": [
        "Launch the Tuning Process\n",
        "========================\n",
        "We first acquire a logger to show the results of tuning and define the tuning task to be the last convolutional layer of resnet-18.\n",
        "The remaining code is boilerplate for specifying tuning options, specifying that we are running tuning over RPC and the target RPC device type and timeouts for building and running schedule configurations. Finally, we launch the tuning job with `tuner.tune()`, passing the number of trials (number of configurations to profile) that we want to allocate for tuning.\n",
        "\n",
        "\n",
        "Note that due to the balance of CPU and GPU resources on colab notebook runtimes, we use XGB `knob` features, which are much less CPU intensive to compute than `itervar` features. However, `itervar` features remain useful when we want to leverage transfer learning for efficient tuning across multiple tuning jobs.\n",
        "\n",
        "Finally, we recommend using dedicated hardware resources for full-scale tuning experiments, as the stability of colab notebook runtimes and tuning is uncertain over long time periods in our experience."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cPgLaW8eIYdR",
        "colab_type": "code",
        "outputId": "083ebf2b-b62f-4547-dbb9-6dc7ac694348",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 290
        }
      },
      "source": [
        "# logging config (for printing tuning log to screen)\n",
        "#logging.getLogger('autotvm').setLevel(logging.DEBUG)\n",
        "#logging.getLogger('autotvm').addHandler(logging.StreamHandler(sys.stdout))\n",
        "\n",
        "# the last layer in resnet\n",
        "N, H, W, CO, CI, KH, KW, strides, padding = 1, 7, 7, 512, 512, 3, 3, (1, 1), (1, 1)\n",
        "task = autotvm.task.create(conv2d_no_batching,\n",
        "                           args=(N, H, W, CO, CI, KH, KW, strides, padding),\n",
        "                           target='cuda')\n",
        "\n",
        "# Use local gpu, measure 10 times for every config to reduce variance\n",
        "# The timeout of compiling a program is 10 seconds, the timeout for running is 4 seconds\n",
        "measure_option = autotvm.measure_option(\n",
        "    builder=autotvm.LocalBuilder(),\n",
        "            runner=autotvm.RPCRunner(\n",
        "            '1080ti',  # change the device key to your key\n",
        "            '0.0.0.0', 9190,\n",
        "            number=256, repeat=3, timeout=1, min_repeat_ms=50)\n",
        ")\n",
        "\n",
        "# Begin tuning, log records to file `conv2d.log`\n",
        "# During tuning we will also try many invalid configs, so you are expected to\n",
        "# see many error reports. As long as you can see non-zero GFLOPS, it is okay.\n",
        "tuner = autotvm.tuner.XGBTuner(task, feature_type='knob')\n",
        "tuner.tune(n_trial=512,\n",
        "           measure_option=measure_option,\n",
        "           callbacks=[plot_callback()])\n",
        "           #callbacks=[autotvm.callback.log_to_file('conv2d.log'), plot_callback()])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAERCAYAAACU1LsdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAHzJJREFUeJzt3XmYXXWd5/H3J1VZgAQCVLGYhbAE\nNCAilICCEBTGQDswPmILagP90Ea7mxnbrYVmBhV7erRdsFVcojCoY4tAu0RAUVkaHpAl7CQQCBBI\n2JJAEkKWqrr3fuePc6q4qVTdOlWpc9fP63nqqXvO+d1zvyc5Vd/6Lef3U0RgZmYGMK7WAZiZWf1w\nUjAzs35OCmZm1s9JwczM+jkpmJlZPycFMzPr15BJQdLlklZJeiRD2eMk3SepIOn0sv2HSfqzpMWS\nHpL0wXyjNjOrfw2ZFIArgHkZyz4LnAP8+4D9m4CzIuLg9FzflDR1rAI0M2tE7bUOYDQi4lZJs8r3\nSdofuBToJPmF/9GIeCwilqfHSwPO8XjZ6+clrUrfuy7X4M3M6lhDJoUhLAA+HhFPSDoK+C7wrixv\nlHQkMAF4Msf4zMzqXlMkBUmTgXcAV0vq2z0x43v3Bn4KnB0RpeHKm5k1s6ZICiR9I+si4rCRvEnS\nzsB1wIURcWcukZmZNZBG7WjeSkS8Cjwt6QMASryl0nskTQB+BfwkIq6pQphmZnVPjThLqqSfA3OB\nDuAl4PPATcD3gL2B8cCVEXGxpLeR/PLfFdgCvBgRB0v6CPB/gcVlpz4nIh6o2oWYmdWZhkwKZmaW\nj6ZoPjIzs7HRcB3NHR0dMWvWrFqHYWbWUO699941EdE5XLmGSwqzZs1i0aJFtQ7DzKyhSHomSzk3\nH5mZWT8nBTMz65dbUhhuJlNJH05nJ31Y0h3DPVdgZmb5y7OmcAWVZzJ9Gjg+It4MfIlk7iIzM6uh\n3DqaB5vJdMDxO8o27wSm5xWLmZllUy99CucCvxvqoKT5khZJWrR69eoqhmVm1lpqnhQknUCSFD43\nVJmIWBARXRHR1dk57DBbMzMbpZo+pyDpUOBHwMkR8XItYzEzy9NLr27hyrtXUCyNfob+rlm7cdyB\n+f5hXLOkIGkm8Evgr8pXQTMza0ZX3r2CS/70OK8v+TJyHz9+/8ZNCuUzmUpaSTKT6XiAiPg+cBGw\nO/DddGGcQkR05RWPmVktPfPKRvbaeRJ3/tO7ax1KRXmOPjpzmON/A/xNXp9vZlZPVr6ymRm77VDr\nMIZV845mM7NWsGLtJmbsumOtwxhWw02IZ2ZWr358x3IW3PrUoMdeWL+F6bs5KZhZjb26pZcTv/6f\nvLKxp9ahNL1CKXjLjKnM3mPyNsfax4n3vXVaDaIaGScFsya3ekM3qzZ0c9KcPTlwz21/WdnY2Wli\nO3919D5MmTS+1qGMmpOCWZMrFJMld//bYdP4i0P3rnE0Vu/c0WzW5HqLycNSbeO2Y4C8tQwnBbMm\nVywlNYXxbU4KNjwnBbMmV0inVWhv84+7Dc93iVmT6037FNrdfGQZOCmYNbm+5iMnBcvCScGsyfV1\nNLv5yLLwXWLW5FxTsJFwUjBrcv19Ch59ZBk4KZg1ub7RR+PdfGQZ+C4xa3J9zUd+eM2ycFIwa3J9\nzUfjx/nH3Ybnu8SsyRX6Rx+5pmDDc1Iwa3IFjz6yEXBSMGtyBT+nYCPgu8SsyRXc0Wwj4KRg1uQK\nniXVRsBJwazJ9TcfefSRZeC7xKzJeZZUGwknBbMmVywF4wTjnBQsg9ySgqTLJa2S9MgQxyXpW5KW\nSXpI0uF5xWLWynpLJY88sszyvFOuAOZVOH4yMDv9mg98L8dYzFpWoRhuOrLMcksKEXEr8EqFIqcB\nP4nEncBUSXvnFY9ZqyqWnBQsu1rWKacBK8q2V6b7zGwM9RZLniHVMmuIO0XSfEmLJC1avXp1rcMx\nayiFYvjBNcuslknhOWBG2fb0dN82ImJBRHRFRFdnZ2dVgjNrFoVSuKZgmdXyTlkInJWOQjoaWB8R\nL9QwHrOmVCiVPEOqZdae14kl/RyYC3RIWgl8HhgPEBHfB64HTgGWAZuAv84rFrNWVii5+ciyyy0p\nRMSZwxwP4O/z+nwzSxSKJS+wY5n5TjFrcoViuPnIMsutpmBm22f5mo1s2FLY7vOs3dTj5xQsMycF\nszq0fM1G5n7tljE737EHdIzZuay5OSmY1aGXN3YD8MkTD+TgN+y83ec7eNr2n8Nag5OCWR3q7k3W\nQDh6v904ar/daxyNtRJ3NJvVoe5CkhQmjm+rcSTWapwUzOpQd6EIwMR2/4hadfmOM6tD/TUFJwWr\nMt9xZnWor0/BzUdWbU4KZnXIzUdWK77jzOqQm4+sVioOSZU0CXgv8E7gDcBm4BHguohYnH94Zq3p\n9aTg5iOrriGTgqQvkiSEW4C7gFXAJOBA4Mtpwvh0RDxUhTjNWkp3bxEJxnvOIquySjWFuyPi80Mc\n+4akPYCZOcRk1vK6CyUmto9DclKw6hoyKUTEdQP3SdoVWBeJVSS1BzMbY0lScNORVd+QvViSLpL0\nxvT1REk3A08CL0k6sVoBmrWi7kLRncxWE5Xuug8CS9PXZ6ffO4HjgX/JMyizVtfdW2LieCcFq75K\nd11PujoawHuAKyOiGBGP4on0zHLl5iOrlUpJoVvSIZI6gROAP5Qd2zHfsMxam5uPrFYq/cX/CeAa\nkiajSyLiaQBJpwD3VyE2s5bVN/rIrNoqJYVxwJvKmpAAiIjrgetzjcqswRVLwaae0S+luamn6OYj\nq4lKSeEs4FJJjwO/B34fES9WJyyzxvbBH/yZRc+s3a5zvPuNe4xRNGbZVXpO4W8B0mGpJwNXSNoF\nuJkkSdweEcWqRGnWQIql4MGV63jn7A6OP7Bz1Od55+zRv9dstIYdRRQRjwGPAZdI2oGk0/kDwDeA\nrnzDM2s8L726hd5iMO+QvfjwUfvUOhyzEak099Ek4OPAAcDDwGURsZmkP8F9CmZDWLl2MwAzdvUg\nPWs8lYY3/JikJvAwSfPR10d6cknzJC2VtEzS+YMcnynpZkn3S3ooHdlk1tBWvLIJgOm77lDjSMxG\nrlLz0ZyIeDOApMuAu0dyYkltwKXAScBK4B5JCyNiSVmx/wlcFRHfkzSHpAYyaySfY1ZvXtqwBYC9\ndplU40jMRq5STaG370VEjGZs3ZHAsoh4KiJ6gCuB0waUCWDn9PUuwPOj+ByzulIoJqO4PaTUGlGl\nmsJbJL0K9M3du0PZdkTEzkO/FYBpwIqy7ZXAUQPKfAH4g6T/DuwEDDrRnqT5wHyAmTM9W7fVt0Ip\nSQrjPOu1NaAhawoR0RYRO0fElPSrvWx7uISQ1ZnAFRExHTgF+KmkbWKKiAUR0RURXZ2dHqZn9a1Y\nKtE+Tl4LwRrSsENSJZ0AHJxuPhIRt2Q893PAjLLt6em+cucC8wAi4s/piKcOvE6DNbBCKWhzNcEa\nVKX1FKZJuoukiWe/9OuLku6WNC3Due8BZkvaV9IE4Axg4YAyzwLvTj/vTSTLfa4e8VWY1ZFiMWh3\nUrAGVamm8B3gexFxRflOSWcB32XbTuOtRERB0nnADUAbcHlELJZ0MbAoIhYCnwZ+KOmTJJ3O5wyc\na8ms0bimYI1suCGp7xu4MyJ+IunCLCcfbPK8iLio7PUS4JiMsZo1hGIpaG/zDKfWmCrduYMeSzuC\nPdbObAiuKVgjq5QUrpX0Q0k79e1IX38fT3NhNqS+0UdmjahSUvhHYD3wjKR7Jd0LLAdeJekLMLNB\nuKZgjazS1Nm9wGck/S+SSfEAnoyITZKOAu6qRoBmjaZY8ugja1xZps7eTDIpXrmrAT9abDaIQtE1\nBWtcox0i4TvebAiFUonxHn1kDWq0d66fJTAbQtF9CtbAKi2y81sG/+UvYPfcIjJrcAX3KVgDq9Sn\n8LVRHjNraa4pWCOrlBSejohnqxaJWZMoFIP2ce5TsMZU6c79dd8LSf9RhVjMmoJrCtbIKiWF8rt6\nv7wDMWsWhVKJ9jYnBWtMlZJCDPHazCpwTcEaWdblOPuW4oTsy3GatSSPPrJGVmmaC8+EajYKrilY\nI6u08trk4d6cpYxZq0lqCh59ZI2p0p37G0lfl3TcgOmz95N0rqQbSNdXNrPXuaZgjaxS89G7JZ0C\nfAw4RtKuQAFYClwHnB0RL1YnTLPGUfB6CtbAKs6SOthymmZWWdGzpFoDc8On2RgreI1ma2C+c83G\nmBfZsUbmpGA2xrwcpzWyTElB0rGS/jp93Slp33zDMmtcrilYIxs2KUj6PPA54IJ013jg/+UZlFkj\nK5RKtHnuI2tQWWoK7wNOBTYCRMTzwJQsJ5c0T9JSScsknT9Emb+UtETSYkn/njVws3rlmoI1sopD\nUlM9ERGSAqD8QbZKJLUBlwInASuBeyQtjIglZWVmk9RAjomItZL2GPEVmNWZpE/B3XXWmLLcuVdJ\n+gEwVdJHgT8BP8zwviOBZRHxVET0AFcCpw0o81Hg0ohYCxARq7KHblZ/SqUgAtcUrGENW1OIiK9J\nOgl4FTgIuCgi/pjh3NOAFWXbK4GjBpQ5EEDS7UAb8IWI+H2WwM3qUW+pBODRR9awhk0K6Uij2/oS\ngaQdJM2KiOVj9PmzgbnAdOBWSW+OiHUDYpgPzAeYOXPmGHysWT6KpWTpEdcUrFFlaT66GiiVbRfT\nfcN5DphRtj093VduJbAwInoj4mngcZIksZWIWBARXRHR1dnZmeGjzWqjkCYF1xSsUWVJCu1pnwAA\n6esJGd53DzBb0r6SJgBnAAsHlPk1SS0BSR0kzUlPZTi3WV16Yd0WADomT6xxJGajkyUprJZ0at+G\npNOANcO9KSIKwHnADcCjwFURsVjSxWXnuwF4WdIS4GbgsxHx8kgvwqxePLQyafl88/RdahyJ2ego\novLyy5L2B34GvIFkKc4VwFkRsSz/8LbV1dUVixYtqsVHW5N5rbvA7cvWMNzPQFYbthT47DUPsdOE\nNh7+wnsY5yYkqyOS7o2IruHKZRl99CRwdN8qaxHx2hjEZ1Zzl932NJf86fExPWfH5Amc845ZTgjW\nsLKMPpoIvB+YBbRLyc0eERfnGplZzp5ft5mOyRP46bkDR0qP3r4dOzFpvJc3t8aV5Ynm3wDrgXuB\n7nzDMaue1a91s+fOk3jT3jvXOhSzupElKUyPCK/FbE1nzWvdHiVkNkCW0Ud3SHpz7pGYVdmaDU4K\nZgNlqSkcC5wj6WmS5iMBERGH5hqZWY4igjWv9dAxJcsjN2atI0tSODn3KMxysHpDN5+66gE29xS3\nOVaKoKdYotM1BbOtDNt8FBHPRMQzwGYgyr7M6tptT6zmtifWIMHE8eO2+tphQhtzD+pk7kGeNsWs\nXJYhqacCXyd5eG0VsA/JE8oH5xua2fZ59IVXmdg+jp9/9Gja27y+gVkWWZqPvgQcDfwpIt4q6QTg\nI/mGZZZdT6HEt296gg1bClvtv+mxVRy01xQnBLMRyJIUeiPiZUnjJI2LiJslfTP3yMwyenDlOr59\n0zJ2nNC2zZTVZxw5Y4h3mdlgsiSFdekUF7cCP5O0inS9ZrN6sHZjMonvL+a/3RPRmW2nLPXq00g6\nmT8J/B54EviveQZlNhLrNvUCMHXH8TWOxKzxZZkQbyOApJ2B3+YekdkIrduc1BScFMy2X5bRRx8D\nvghsIVmBTSRDUvfLNzSzbNZt6qV9nJg8MUtrqJlVkuWn6DPAIREx7MI6ZrWwdlMvU3ccT98MvmY2\neln6FJ4ENuUdiNlord/cwy47uOnIbCxkqSlcQDIp3l2UTZ0dEf8jt6jMRmDtxl523dFzGJmNhSxJ\n4QfATcDDJH0KZnVl3eZepk2dVOswzJpClqQwPiI+lXskZqO0blMPc7xQjtmYyNKn8DtJ8yXtLWm3\nvq/cIzPLaN2mXnb1cFSzMZGlpnBm+v2Csn0ekmp1YUtvkc29RT+jYDZGKiYFSeOAj0TE7VWKx2xE\n1m/ue5rZHc1mY6Fi81FElIDvVCkWsxFbu8lPM5uNpSx9CjdKer/8ZJDVob55jzwk1WxsZOlT+Bjw\nKaAoaTOvr9E87HAPSfOAfwPagB9FxJeHKPd+4BrgbRGxKGvwNnKlUvBaT2H4gg3ixfVbAPzwmtkY\nyTIh3pTRnFhSG3ApcBKwErhH0sKIWDKg3BTgE8Bdo/mcVrRhSy+Pv/TaKN4Z/PN1j3L/s+vGPKZa\n6/Bay2ZjItMMYumSnMelm7dExLUZ3nYksCwinkrPcSXJNNxLBpT7EvAV4LOZIjb+6VeP8NsHnx/1\n+8874YCmaoPvnDKRvXbxw2tmYyHLLKlfBt4G/Czd9QlJx0TEBRXeBjANWFG2vRI4asC5DwdmRMR1\nkoZMCpLmA/MBZs6cOVzITe+Vjd0cuOdkLvyLOSN+7147T+KgvUZV+TOzFpClpnAKcFg6EglJPwbu\nZ+vnFkYsHe76DeCc4cpGxAJgAUBXV1dsz+c2g+7eEh2TJ3L8gZ21DsXMmkzWFc2nlr3Out7hc0D5\nArnT0319pgCHALdIWg4cDSyU1JXx/C2rp1hiYrsXozezsZelpvB/gPsl3Uwy8ug44PwM77sHmC1p\nX5JkcAbwob6DEbEe6OjblnQL8BmPPhpeT6HEBCcFM8vBkEkh7Te4HfglcAtJvwLA5yLixeFOHBEF\nSecBN5AMSb08IhZLuhhYFBELtzv6FtVdKDGxva3WYZhZE6pUU/gWcATw54g4HBjxL/GIuB64fsC+\ni4YoO3ek529VrimYWV4qJYVeSQuA6ZK+NfCgF9mpne5C0UnBzHJRKSm8FzgReA9wb3XCsSyS5iMn\nBTMbe0MmhYhYI+lq4A0R8eMqxmTD6HbzkZnlZLhZUosko4asTkQEPe5oNrOcZBmSeruk7wC/ADb2\n7YyI+3KLyobUW0ye3XPzkZnlIUtSOCz9fnHZvgDeNfbh2HC6C0XAScHM8pFlltQTqhGIZdNTKAG4\nT8HMcjHsbxZJe0q6TNLv0u05ks7NPzQbTHdfUmhzUjCzsZflN8sVJE8lvyHdfhz4h7wCssr6agoT\nxzspmNnYy/KbpSMirgJKkExfARRzjcqG9HpNwaOPzGzsZUkKGyXtTtK5jKSjgfW5RmVD6q8puE/B\nzHKQZfTRp0jmPdpf0u1AJ3B6rlHZkHqKSSXNHc1mlocso4/uk3Q8cBDJ1NlLI6I398hsUN29rimY\nWX6yLMc5Cfg74FiSJqTbJH0/IrbkHVwrW7ephwt//Qibugtb7X9lYw/gmoKZ5SNL89FPgA3At9Pt\nDwE/BT6QV1AGi59/leseeoH9Ondi8sSt/5uOOWB39t9jco0iM7NmliUpHBIR5SvE3yxpSV4BWaJQ\nSqaz+Orph3LEPrvVOBozaxVZ2iDuS0ccASDpKMBLZuaslCaFtnFuJjKz6slSUzgCuEPSs+n2TGCp\npIeBiIhDc4uuhRX7koJU40jMrJVkSQrzco/CtlGMJCm4omBm1ZRlSOoz1QjEttZfUxjnmoKZVY//\nDq1TfUmh3UnBzKrISaFOlfqaj9ynYGZV5KRQpwpFNx+ZWfU5KdSpomsKZlYDuSYFSfMkLZW0TNL5\ngxz/lKQlkh6SdKOkffKMp5H0PafQ3uakYGbVk1tSkNQGXAqcDMwBzpQ0Z0Cx+4Gu9FmHa4B/zSue\nRtNXU/BzCmZWTXnWFI4ElkXEUxHRA1wJnFZeICJujohN6eadwPQc42kofaOPxrlPwcyqKM+kMA1Y\nUba9Mt03lHOB3w12QNJ8SYskLVq9evUYhli//ESzmdVCXXQ0S/oI0AV8dbDjEbEgIroioquzs7O6\nwdVIf1Jwn4KZVVGWaS5G6zlgRtn29HTfViSdCFwIHB8R3TnG01BK7lMwsxrIs6ZwDzBb0r6SJgBn\nkCzr2U/SW4EfAKdGxKocY2k4BU9zYWY1kFtSiIgCcB5wA/AocFVELJZ0saRT02JfBSYDV0t6QNLC\nIU7XcvqGpPo5BTOrpjybj4iI64HrB+y7qOz1iXl+fiMrJksxe+4jM6uquuhotm0VS0lW8JBUM6sm\nJ4U6VYxwf4KZVZ2TQp0qljzyyMyqz0mhTpVcUzCzGnBSqFOFopOCmVWfk0KdKkXgnGBm1eakUKeK\nJdcUzKz6nBTqVDL6yP89ZlZd/q1Tp4rFoM3/O2ZWZf61U6eKER6SamZV56RQp0ql8NPMZlZ1Tgp1\nqlAKz3tkZlXnpFCniuGagplVn5NCnSqV3KdgZtXnpFCn/JyCmdWCk0KdclIws1pwUqhTnjrbzGrB\nSaFOFUvhpTjNrOqcFOqUp842s1pwUqhTnjrbzGrBSaFOlTzNhZnVgJNCnfLoIzOrBSeFOlX03Edm\nVgNOCnWqGJ77yMyqz0mhThVLeEiqmVVdrklB0jxJSyUtk3T+IMcnSvpFevwuSbPyjKeRlEpeZMfM\nqi+3XzuS2oBLgZOBOcCZkuYMKHYusDYiDgAuAb6SVzyNplAquaPZzKquPcdzHwksi4inACRdCZwG\nLCkrcxrwhfT1NcB3JCkiYqyD+c/HV/PP1y4ZvmCdePaVTbxp751rHYaZtZg8k8I0YEXZ9krgqKHK\nRERB0npgd2BNeSFJ84H5ADNnzhxVMJMntjN7z8mjem8tzN5zMn/ZNaPWYZhZi8kzKYyZiFgALADo\n6uoaVS3iiH125Yh9jhjTuMzMmk2eXZnPAeV/6k5P9w1aRlI7sAvwco4xmZlZBXkmhXuA2ZL2lTQB\nOANYOKDMQuDs9PXpwE159CeYmVk2uTUfpX0E5wE3AG3A5RGxWNLFwKKIWAhcBvxU0jLgFZLEYWZm\nNZJrn0JEXA9cP2DfRWWvtwAfyDMGMzPLzo9HmZlZPycFMzPr56RgZmb9nBTMzKyfGm0EqKTVwDOj\nfHsHA56WbmKtdK3g621mrXStkN/17hMRncMVariksD0kLYqIrlrHUQ2tdK3g621mrXStUPvrdfOR\nmZn1c1IwM7N+rZYUFtQ6gCpqpWsFX28za6VrhRpfb0v1KZiZWWWtVlMwM7MKnBTMzKxfSyQFSfMk\nLZW0TNL5tY5nLEi6XNIqSY+U7dtN0h8lPZF+3zXdL0nfSq//IUmH1y7ykZM0Q9LNkpZIWizpE+n+\nZr3eSZLulvRger1fTPfvK+mu9Lp+kU5Jj6SJ6fay9PisWsY/GpLaJN0v6dp0u5mvdbmkhyU9IGlR\nuq9u7uWmTwqS2oBLgZOBOcCZkubUNqoxcQUwb8C+84EbI2I2cGO6Dcm1z06/5gPfq1KMY6UAfDoi\n5gBHA3+f/h826/V2A++KiLcAhwHzJB0NfAW4JCIOANYC56blzwXWpvsvScs1mk8Aj5ZtN/O1ApwQ\nEYeVPY9QP/dyRDT1F/B24Iay7QuAC2od1xhd2yzgkbLtpcDe6eu9gaXp6x8AZw5WrhG/gN8AJ7XC\n9QI7AveRrG++BmhP9/ff1yRrlrw9fd2ellOtYx/BNU4n+UX4LuBaQM16rWncy4GOAfvq5l5u+poC\nMA1YUba9Mt3XjPaMiBfS1y8Ce6avm+bfIG0ueCtwF018vWlzygPAKuCPwJPAuogopEXKr6n/etPj\n64Hdqxvxdvkm8I9AKd3enea9VoAA/iDpXknz0311cy/nusiO1U5EhKSmGm8saTLwH8A/RMSrkvqP\nNdv1RkQROEzSVOBXwBtrHFIuJL0XWBUR90qaW+t4quTYiHhO0h7AHyU9Vn6w1vdyK9QUngNmlG1P\nT/c1o5ck7Q2Qfl+V7m/4fwNJ40kSws8i4pfp7qa93j4RsQ64maQJZaqkvj/kyq+p/3rT47sAL1c5\n1NE6BjhV0nLgSpImpH+jOa8VgIh4Lv2+iiThH0kd3cutkBTuAWanoxkmkKwDvbDGMeVlIXB2+vps\nkrb3vv1npSMZjgbWl1VV656SKsFlwKMR8Y2yQ816vZ1pDQFJO5D0nzxKkhxOT4sNvN6+f4fTgZsi\nbYCudxFxQURMj4hZJD+bN0XEh2nCawWQtJOkKX2vgf8CPEI93cu17nSpUsfOKcDjJO2yF9Y6njG6\npp8DLwC9JO2M55K0rd4IPAH8CdgtLSuSEVhPAg8DXbWOf4TXeixJO+xDwAPp1ylNfL2HAven1/sI\ncFG6fz/gbmAZcDUwMd0/Kd1elh7fr9bXMMrrngtc28zXml7Xg+nX4r7fR/V0L3uaCzMz69cKzUdm\nZpaRk4KZmfVzUjAzs35OCmZm1s9JwczM+jkpWMuRNFXS3w1T5o4M53lt7KIyqw9OCtaKpgKDJoW+\np2gj4h15fXg6c69ZXXJSsFb0ZWD/dD77r0qaK+k2SQuBJfB6LUDSZEk3SrovnQP/tIEnk7S3pFvT\n8z0i6Z2DlFku6SuS7gM+IOkWSV3psY50mgcknSPpl5J+n86t/6/5/TOYbcsT4lkrOh84JCIOA0gn\nYjs83ff0gLJbgPdFMgFfB3CnpIWx9VOfHyKZ2vl/p7WAHYf43Jcj4vD0Mz9eIb7DSGaC7QaWSvp2\nRKyoUN5szDgpmCXuHiQhQDLNwL9IOo5kaudpJNMav1hW5h7g8nTSvl9HxANDfMYvMsZyY0SsB5C0\nBNiHradPNsuNm4/MEhuH2P9hoBM4Iq1ZvEQy/06/iLgVOI5k9sorJJ2V4TMKvP7zN2lAue6y10X8\nx5tVkZOCtaINwJSMZXchme+/V9IJJH+1b0XSPsBLEfFD4EckTVHDWQ4ckb4+vUI5s6pyUrCWExEv\nA7enncJfHab4z4AuSQ8DZwGPDVJmLvCgpPuBD5KsBzCcrwF/m76nI3PwZjnzLKlmZtbPNQUzM+vn\npGBmZv2cFMzMrJ+TgpmZ9XNSMDOzfk4KZmbWz0nBzMz6/X+DdFfWw/SAjQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}