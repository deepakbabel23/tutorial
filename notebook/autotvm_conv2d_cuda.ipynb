{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "autotvm_conv2d_cuda.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/uwsampl/tutorial/blob/master/notebook/autotvm_conv2d_cuda.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8T0-Z4MasbY",
        "colab_type": "text"
      },
      "source": [
        "Tuning High Performance Convolution on NVIDIA GPUs\n",
        "=========================================================================\n",
        "**Author**: `Lianmin Zheng <https://github.com/merrymercy>`_\n",
        "\n",
        "Adapted by `Eddie Yan <https://github.com/eqy>`_\n",
        "\n",
        "This is an advanced tutorial for writing high performance tunable template for\n",
        "NVIDIA GPU. By running auto-tuner on this template, we can outperform the\n",
        "vendor provided library CuDNN in many cases.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9soFF1q_cV-y",
        "colab_type": "text"
      },
      "source": [
        "Please run the following block to ensure TVM is setup for *this notebook*, each notebook may have its own runtime.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G_lzOGuFcUgG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1482
        },
        "outputId": "92f3a9b3-929b-4767-a9ea-01f2d7941b12"
      },
      "source": [
        "! gsutil cp \"gs://tvm-fcrc-binariesd5fce43e-8373-11e9-bfb6-0242ac1c0002/tvm.tar.gz\" /tmp/tvm.tar.gz\n",
        "! mkdir -p /tvm\n",
        "! tar -xf /tmp/tvm.tar.gz --strip-components=4 --directory /tvm\n",
        "! ls -la /tvm\n",
        "# Move this block after we are done with pkg step\n",
        "! bash /tvm/package.sh\n",
        "import sys\n",
        "sys.path.append('/tvm/python')\n",
        "sys.path.append('/tvm/topi/python')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Copying gs://tvm-fcrc-binariesd5fce43e-8373-11e9-bfb6-0242ac1c0002/tvm.tar.gz...\n",
            "- [1 files][112.9 MiB/112.9 MiB]                                                \n",
            "Operation completed over 1 objects/112.9 MiB.                                    \n",
            "total 164\n",
            "drwxr-xr-x 21 root root  4096 Jun  4 21:43 .\n",
            "drwxr-xr-x  1 root root  4096 Jun  4 21:05 ..\n",
            "drwx------  8 root root  4096 May 31 08:14 3rdparty\n",
            "drwx------ 12 root root  4096 May 31 08:14 apps\n",
            "drwx------  3 root root  4096 Jun  4 09:46 build\n",
            "drwx------  4 root root  4096 May 31 08:14 cmake\n",
            "-rw-------  1 root root 10406 May 31 08:14 CMakeLists.txt\n",
            "drwx------  6 root root  4096 May 31 08:14 conda\n",
            "-rw-------  1 root root  5673 May 31 08:14 CONTRIBUTORS.md\n",
            "drwx------  3 root root  4096 May 31 08:14 docker\n",
            "drwx------ 11 root root  4096 May 31 08:14 docs\n",
            "drwx------  4 root root  4096 May 31 08:14 golang\n",
            "drwx------  3 root root  4096 May 31 08:14 include\n",
            "-rw-------  1 root root 10027 May 31 08:14 Jenkinsfile\n",
            "drwx------  6 root root  4096 May 31 08:14 jvm\n",
            "-rw-------  1 root root 11357 May 31 08:14 LICENSE\n",
            "-rw-------  1 root root  4267 May 31 08:14 Makefile\n",
            "-rw-------  1 root root 10476 May 31 08:14 NEWS.md\n",
            "drwx------  9 root root  4096 May 31 08:14 nnvm\n",
            "-rw-------  1 root root    61 May 31 08:14 NOTICE\n",
            "-rw-------  1 root root   369 Jun  3 22:49 package.sh\n",
            "drwx------  3 root root  4096 May 31 08:14 python\n",
            "-rw-------  1 root root  2705 May 31 08:14 README.md\n",
            "drwx------  5 root root  4096 May 31 08:14 rust\n",
            "drwx------ 14 root root  4096 May 31 08:14 src\n",
            "drwx------  9 root root  4096 May 31 08:14 tests\n",
            "drwx------  7 root root  4096 May 31 08:14 topi\n",
            "drwx------  8 root root  4096 May 31 08:14 tutorials\n",
            "-rw-------  1 root root  2902 May 31 08:14 version.py\n",
            "drwx------ 10 root root  4096 May 31 08:14 vta\n",
            "drwx------  2 root root  4096 May 31 08:14 web\n",
            "Installing Dependencies ...\n",
            "deb https://dl.bintray.com/sbt/debian /\n",
            "Executing: /tmp/apt-key-gpghome.hof0b6QiHw/gpg.1.sh --keyserver hkp://keyserver.ubuntu.com:80 --recv 2EE0EA64E40A89B84B2DF73499E82A75642AC823\n",
            "gpg: key 99E82A75642AC823: \"sbt build tool <scalasbt@gmail.com>\" not changed\n",
            "gpg: Total number processed: 1\n",
            "gpg:              unchanged: 1\n",
            "Ign:1 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran35/ InRelease\n",
            "Get:2 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran35/ Release [3,089 B]\n",
            "Ign:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Ign:4 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran35/ Release.gpg\n",
            "Get:5 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Hit:6 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Ign:7 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Ign:8 https://dl.bintray.com/sbt/debian  InRelease\n",
            "Hit:9 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
            "Hit:10 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Get:11 https://dl.bintray.com/sbt/debian  Release [815 B]\n",
            "Hit:12 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Get:14 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Hit:15 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic InRelease\n",
            "Get:18 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Reading package lists... Done\n",
            "E: The repository 'https://cloud.r-project.org/bin/linux/ubuntu bionic-cran35/ Release' is no longer signed.\n",
            "N: Updating from such a repository can't be done securely, and is therefore disabled by default.\n",
            "N: See apt-secure(8) manpage for repository creation and user configuration details.\n",
            "W: Target Packages (Packages) is configured multiple times in /etc/apt/sources.list.d/sbt.list:1 and /etc/apt/sources.list.d/sbt.list:2\n",
            "W: Target Packages (Packages) is configured multiple times in /etc/apt/sources.list.d/sbt.list:1 and /etc/apt/sources.list.d/sbt.list:2\n",
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "libffi-dev is already the newest version (3.2.1-8).\n",
            "llvm-6.0 is already the newest version (1:6.0-1ubuntu2).\n",
            "zlib1g-dev is already the newest version (1:1.2.11.dfsg-0ubuntu2).\n",
            "clinfo is already the newest version (2.2.18.03.26-1).\n",
            "libtinfo-dev is already the newest version (6.1-1ubuntu1.18.04).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-410\n",
            "Use 'sudo apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 30 not upgraded.\n",
            "W: Target Packages (Packages) is configured multiple times in /etc/apt/sources.list.d/sbt.list:1 and /etc/apt/sources.list.d/sbt.list:2\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "verilator is already the newest version (3.916-1build1).\n",
            "sbt is already the newest version (1.2.8).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-410\n",
            "Use 'sudo apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 30 not upgraded.\n",
            "W: Target Packages (Packages) is configured multiple times in /etc/apt/sources.list.d/sbt.list:1 and /etc/apt/sources.list.d/sbt.list:2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "agr421w8a49k",
        "colab_type": "text"
      },
      "source": [
        "Import packages:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C9JnukA4aTLJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import logging\n",
        "import sys\n",
        "import numpy as np\n",
        "\n",
        "import tvm\n",
        "import topi\n",
        "from topi.testing import conv2d_nchw_python\n",
        "\n",
        "from tvm import autotvm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u5YlgPYma_5r",
        "colab_type": "text"
      },
      "source": [
        "Step 0: Vanilla direct 2D convolution implementation without a tunable template\n",
        "---------------------------------------------------------------------------------------------\n",
        "\n",
        "Default Schedule:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DhtW-j3bbMLI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 550
        },
        "outputId": "9c613617-2e37-44cd-f0da-f26250a6df51"
      },
      "source": [
        "#def manual_conv2d_no_batching(N, H, W, CO, CI, KH, KW, stride, padding):\n",
        "\n",
        "# the last layer in resnet\n",
        "N, H, W, CO, CI, KH, KW, stride, padding = 1, 7, 7, 512, 512, 3, 3, (1, 1), (1, 1)\n",
        "assert N == 1, \"Only consider batch_size = 1 in this template\"\n",
        "\n",
        "data = tvm.placeholder((N, CI, H, W), name='data')\n",
        "kernel = tvm.placeholder((CO, CI, KH, KW), name='kernel')\n",
        "conv = topi.nn.conv2d_nchw(data, kernel, stride, padding, dilation=1, out_dtype='float32')\n",
        "s = tvm.create_schedule([conv.op])\n",
        "print(\"Default Schedule:\")\n",
        "print(tvm.lower(s, [data, kernel, conv], simple_mode=True))\n",
        "\n",
        "##### space definition begin #####\n",
        "n, f, y, x = s[conv].op.axis\n",
        "rc, ry, rx = s[conv].op.reduce_axis\n",
        "\n",
        "    #cfg = autotvm.get_config()\n",
        "    #cfg.define_split(\"tile_f\", f, num_outputs=4)\n",
        "    #cfg.define_split(\"tile_y\", y, num_outputs=4)\n",
        "    #cfg.define_split(\"tile_x\", x, num_outputs=4)\n",
        "    #cfg.define_split(\"tile_rc\", rc, num_outputs=3)\n",
        "    #cfg.define_split(\"tile_ry\", ry, num_outputs=3)\n",
        "    #cfg.define_split(\"tile_rx\", rx, num_outputs=3)\n",
        "    #cfg.define_knob(\"auto_unroll_max_step\", [0, 512, 1500])\n",
        "    #cfg.define_knob(\"unroll_explicit\", [0, 1])\n",
        "    ##### space definition end #####\n",
        "\n"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Default Schedule:\n",
            "// attr [pad_temp] storage_scope = \"global\"\n",
            "allocate pad_temp[float32 * 41472]\n",
            "produce pad_temp {\n",
            "  for (i1, 0, 512) {\n",
            "    for (i2, 0, 9) {\n",
            "      for (i3, 0, 9) {\n",
            "        pad_temp[((((i1*9) + i2)*9) + i3)] = tvm_if_then_else(((((1 <= i2) && (i2 < 8)) && (1 <= i3)) && (i3 < 8)), data[(((((i1*7) + i2)*7) + i3) + -8)], 0.000000f)\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "produce compute {\n",
            "  for (ff, 0, 512) {\n",
            "    for (yy, 0, 7) {\n",
            "      for (xx, 0, 7) {\n",
            "        compute[((((ff*7) + yy)*7) + xx)] = 0.000000f\n",
            "        for (rc, 0, 512) {\n",
            "          for (ry, 0, 3) {\n",
            "            for (rx, 0, 3) {\n",
            "              compute[((((ff*7) + yy)*7) + xx)] = (compute[((((ff*7) + yy)*7) + xx)] + (pad_temp[((((((rc*9) + yy) + ry)*9) + xx) + rx)]*kernel[((((((ff*512) + rc)*3) + ry)*3) + rx)]))\n",
            "            }\n",
            "          }\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PYacZkkNd_xG",
        "colab_type": "text"
      },
      "source": [
        "Inline padding and create cache stages:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "afD69lgUd-QG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1329
        },
        "outputId": "d9efa083-4e16-49fd-f7c7-af31694bde91"
      },
      "source": [
        "# inline padding\n",
        "pad_data = s[conv].op.input_tensors[0]\n",
        "s[pad_data].compute_inline()\n",
        "input = data\n",
        "data, raw_data = pad_data, data\n",
        "\n",
        "output = conv\n",
        "OL = s.cache_write(conv, 'local')\n",
        "\n",
        "# create cache stage\n",
        "AA = s.cache_read(data, 'shared', [OL])\n",
        "WW = s.cache_read(kernel, 'shared', [OL])\n",
        "AL = s.cache_read(AA, 'local', [OL])\n",
        "WL = s.cache_read(WW, 'local', [OL])\n",
        "\n",
        "print(tvm.lower(s, [input, kernel, conv], simple_mode=True))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "// attr [pad_temp.shared] storage_scope = \"shared\"\n",
            "allocate pad_temp.shared[float32 * 41472]\n",
            "// attr [pad_temp.shared.local] storage_scope = \"local\"\n",
            "allocate pad_temp.shared.local[float32 * 41472]\n",
            "// attr [kernel.shared] storage_scope = \"shared\"\n",
            "allocate kernel.shared[float32 * 2359296]\n",
            "// attr [kernel.shared.local] storage_scope = \"local\"\n",
            "allocate kernel.shared.local[float32 * 2359296]\n",
            "// attr [compute.local] storage_scope = \"local\"\n",
            "allocate compute.local[float32 * 25088]\n",
            "produce pad_temp.shared {\n",
            "  for (ax1, 0, 512) {\n",
            "    for (ax2, 0, 9) {\n",
            "      for (ax3, 0, 9) {\n",
            "        pad_temp.shared[((((ax1*9) + ax2)*9) + ax3)] = tvm_if_then_else(((((1 <= ax2) && (ax2 < 8)) && (1 <= ax3)) && (ax3 < 8)), data[(((((ax1*7) + ax2)*7) + ax3) + -8)], 0.000000f)\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "produce pad_temp.shared.local {\n",
            "  for (ax1, 0, 512) {\n",
            "    for (ax2, 0, 9) {\n",
            "      for (ax3, 0, 9) {\n",
            "        pad_temp.shared.local[((((ax1*9) + ax2)*9) + ax3)] = pad_temp.shared[((((ax1*9) + ax2)*9) + ax3)]\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "produce kernel.shared {\n",
            "  for (ax0, 0, 512) {\n",
            "    for (ax1, 0, 512) {\n",
            "      for (ax2, 0, 3) {\n",
            "        for (ax3, 0, 3) {\n",
            "          kernel.shared[((((((ax0*512) + ax1)*3) + ax2)*3) + ax3)] = kernel[((((((ax0*512) + ax1)*3) + ax2)*3) + ax3)]\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "produce kernel.shared.local {\n",
            "  for (ax0, 0, 512) {\n",
            "    for (ax1, 0, 512) {\n",
            "      for (ax2, 0, 3) {\n",
            "        for (ax3, 0, 3) {\n",
            "          kernel.shared.local[((((((ax0*512) + ax1)*3) + ax2)*3) + ax3)] = kernel.shared[((((((ax0*512) + ax1)*3) + ax2)*3) + ax3)]\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "produce compute.local {\n",
            "  for (ff.c, 0, 512) {\n",
            "    for (yy.c, 0, 7) {\n",
            "      for (xx.c, 0, 7) {\n",
            "        compute.local[((((ff.c*7) + yy.c)*7) + xx.c)] = 0.000000f\n",
            "        for (rc, 0, 512) {\n",
            "          for (ry, 0, 3) {\n",
            "            for (rx, 0, 3) {\n",
            "              compute.local[((((ff.c*7) + yy.c)*7) + xx.c)] = (compute.local[((((ff.c*7) + yy.c)*7) + xx.c)] + (pad_temp.shared.local[((((((rc*9) + yy.c) + ry)*9) + xx.c) + rx)]*kernel.shared.local[((((((ff.c*512) + rc)*3) + ry)*3) + rx)]))\n",
            "            }\n",
            "          }\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "produce compute {\n",
            "  for (ff, 0, 512) {\n",
            "    for (yy, 0, 7) {\n",
            "      for (xx, 0, 7) {\n",
            "        compute[((((ff*7) + yy)*7) + xx)] = compute.local[((((ff*7) + yy)*7) + xx)]\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iDGX2K6MeWH6",
        "colab_type": "text"
      },
      "source": [
        "Define and split output spatial axes:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yLaKLAH7eWRH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1363
        },
        "outputId": "2f82cecf-b17d-4abe-f327-ccce87abde88"
      },
      "source": [
        "# tile spatial axes\n",
        "n, f, y, x = s[output].op.axis\n",
        "tile_f_factors = [512, 256, 256, 1]\n",
        "tile_x_factors = [7, 7, 7, 1]\n",
        "tile_y_factors = [7, 7, 7, 1]\n",
        "\n",
        "bf, vf = s[output].split(f, factor=tile_f_factors[1])\n",
        "vf, tf = s[output].split(vf, factor=tile_f_factors[2])\n",
        "tf, fi = s[output].split(tf, factor=tile_f_factors[3])\n",
        "\n",
        "by, vy = s[output].split(y, factor=tile_y_factors[1])\n",
        "vy, ty = s[output].split(vy, factor=tile_y_factors[2])\n",
        "ty, yi = s[output].split(ty, factor=tile_y_factors[3])\n",
        "\n",
        "bx, vx = s[output].split(x, factor=tile_x_factors[1])\n",
        "vx, tx = s[output].split(vx, factor=tile_x_factors[2])\n",
        "tx, xi, = s[output].split(tx, factor=tile_x_factors[3])\n",
        "\n",
        "#bf, vf, tf, fi = cfg[\"tile_f\"].apply(s, output, f)\n",
        "#by, vy, ty, yi = cfg[\"tile_y\"].apply(s, output, y)\n",
        "#bx, vx, tx, xi = cfg[\"tile_x\"].apply(s, output, x)\n",
        "kernel_scope = n  # this is the scope to attach global config inside this kernel\n",
        "\n",
        "s[output].reorder(n, bf, by, bx, vf, vy, vx, tf, ty, tx, fi, yi, xi)\n",
        "print(tvm.lower(s, [input, kernel, conv], simple_mode=True))\n",
        "\n",
        "\n"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "// attr [pad_temp.shared] storage_scope = \"shared\"\n",
            "allocate pad_temp.shared[float32 * 41472]\n",
            "// attr [pad_temp.shared.local] storage_scope = \"local\"\n",
            "allocate pad_temp.shared.local[float32 * 41472]\n",
            "// attr [kernel.shared] storage_scope = \"shared\"\n",
            "allocate kernel.shared[float32 * 2359296]\n",
            "// attr [kernel.shared.local] storage_scope = \"local\"\n",
            "allocate kernel.shared.local[float32 * 2359296]\n",
            "// attr [compute.local] storage_scope = \"local\"\n",
            "allocate compute.local[float32 * 25088]\n",
            "produce pad_temp.shared {\n",
            "  for (ax1, 0, 512) {\n",
            "    for (ax2, 0, 9) {\n",
            "      for (ax3, 0, 9) {\n",
            "        pad_temp.shared[((((ax1*9) + ax2)*9) + ax3)] = tvm_if_then_else(((((1 <= ax2) && (ax2 < 8)) && (1 <= ax3)) && (ax3 < 8)), data[(((((ax1*7) + ax2)*7) + ax3) + -8)], 0.000000f)\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "produce pad_temp.shared.local {\n",
            "  for (ax1, 0, 512) {\n",
            "    for (ax2, 0, 9) {\n",
            "      for (ax3, 0, 9) {\n",
            "        pad_temp.shared.local[((((ax1*9) + ax2)*9) + ax3)] = pad_temp.shared[((((ax1*9) + ax2)*9) + ax3)]\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "produce kernel.shared {\n",
            "  for (ax0, 0, 512) {\n",
            "    for (ax1, 0, 512) {\n",
            "      for (ax2, 0, 3) {\n",
            "        for (ax3, 0, 3) {\n",
            "          kernel.shared[((((((ax0*512) + ax1)*3) + ax2)*3) + ax3)] = kernel[((((((ax0*512) + ax1)*3) + ax2)*3) + ax3)]\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "produce kernel.shared.local {\n",
            "  for (ax0, 0, 512) {\n",
            "    for (ax1, 0, 512) {\n",
            "      for (ax2, 0, 3) {\n",
            "        for (ax3, 0, 3) {\n",
            "          kernel.shared.local[((((((ax0*512) + ax1)*3) + ax2)*3) + ax3)] = kernel.shared[((((((ax0*512) + ax1)*3) + ax2)*3) + ax3)]\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "produce compute.local {\n",
            "  for (ff.c, 0, 512) {\n",
            "    for (yy.c, 0, 7) {\n",
            "      for (xx.c, 0, 7) {\n",
            "        compute.local[((((ff.c*7) + yy.c)*7) + xx.c)] = 0.000000f\n",
            "        for (rc, 0, 512) {\n",
            "          for (ry, 0, 3) {\n",
            "            for (rx, 0, 3) {\n",
            "              compute.local[((((ff.c*7) + yy.c)*7) + xx.c)] = (compute.local[((((ff.c*7) + yy.c)*7) + xx.c)] + (pad_temp.shared.local[((((((rc*9) + yy.c) + ry)*9) + xx.c) + rx)]*kernel.shared.local[((((((ff.c*512) + rc)*3) + ry)*3) + rx)]))\n",
            "            }\n",
            "          }\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "produce compute {\n",
            "  for (ff.outer, 0, 2) {\n",
            "    for (ff.inner.inner.outer, 0, 256) {\n",
            "      for (yy.inner.inner.outer, 0, 7) {\n",
            "        for (xx.inner.inner.outer, 0, 7) {\n",
            "          compute[((((((ff.outer*256) + ff.inner.inner.outer)*7) + yy.inner.inner.outer)*7) + xx.inner.inner.outer)] = compute.local[((((((ff.outer*256) + ff.inner.inner.outer)*7) + yy.inner.inner.outer)*7) + xx.inner.inner.outer)]\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RH6gor8XjvXM",
        "colab_type": "text"
      },
      "source": [
        "Bind Axes:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_yIhMYiQjvhQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1125
        },
        "outputId": "9b3d9448-95a2-4f6b-ae59-bb3eaf36f7e9"
      },
      "source": [
        "s[output].bind(bf, tvm.thread_axis(\"blockIdx.z\"))\n",
        "s[output].bind(by, tvm.thread_axis(\"blockIdx.y\"))\n",
        "s[output].bind(bx, tvm.thread_axis(\"blockIdx.x\"))\n",
        "s[output].bind(vf, tvm.thread_axis(\"vthread\"))\n",
        "s[output].bind(vy, tvm.thread_axis(\"vthread\"))\n",
        "s[output].bind(vx, tvm.thread_axis(\"vthread\"))\n",
        "s[output].bind(tf, tvm.thread_axis(\"threadIdx.z\"))\n",
        "s[output].bind(ty, tvm.thread_axis(\"threadIdx.y\"))\n",
        "s[output].bind(tx, tvm.thread_axis(\"threadIdx.x\"))\n",
        "s[OL].compute_at(s[output], tx)\n",
        "print(tvm.lower(s, [input, kernel, output], simple_mode=True))"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "// attr [pad_temp.shared] storage_scope = \"shared\"\n",
            "allocate pad_temp.shared[float32 * 4608]\n",
            "// attr [pad_temp.shared.local] storage_scope = \"local\"\n",
            "allocate pad_temp.shared.local[float32 * 4608]\n",
            "// attr [kernel.shared.local] storage_scope = \"local\"\n",
            "allocate kernel.shared.local[float32 * 4608]\n",
            "produce pad_temp.shared {\n",
            "  for (ax1, 0, 512) {\n",
            "    for (ax2, 0, 3) {\n",
            "      for (ax3, 0, 3) {\n",
            "        pad_temp.shared[((((ax1*3) + ax2)*3) + ax3)] = tvm_if_then_else((((((1 - threadIdx.y) <= ax2) && (ax2 < (8 - threadIdx.y))) && ((1 - threadIdx.x) <= ax3)) && (ax3 < (8 - threadIdx.x))), data[(((((((ax1*7) + ax2) + threadIdx.y)*7) + ax3) + threadIdx.x) + -8)], 0.000000f)\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "produce pad_temp.shared.local {\n",
            "  for (ax1, 0, 512) {\n",
            "    for (ax2, 0, 3) {\n",
            "      for (ax3, 0, 3) {\n",
            "        pad_temp.shared.local[((((ax1*3) + ax2)*3) + ax3)] = pad_temp.shared[((((ax1*3) + ax2)*3) + ax3)]\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "produce kernel.shared {\n",
            "  for (ax1, 0, 512) {\n",
            "    for (ax2, 0, 3) {\n",
            "      for (ax3, 0, 3) {\n",
            "        pad_temp.shared[((((ax1*3) + ax2)*3) + ax3)] = kernel[((((((((blockIdx.z*256) + threadIdx.z)*512) + ax1)*3) + ax2)*3) + ax3)]\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "produce kernel.shared.local {\n",
            "  for (ax1, 0, 512) {\n",
            "    for (ax2, 0, 3) {\n",
            "      for (ax3, 0, 3) {\n",
            "        kernel.shared.local[((((ax1*3) + ax2)*3) + ax3)] = pad_temp.shared[((((ax1*3) + ax2)*3) + ax3)]\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "produce compute {\n",
            "  // attr [iter_var(blockIdx.z, , blockIdx.z)] thread_extent = 2\n",
            "  // attr [compute.local] storage_scope = \"local\"\n",
            "  allocate compute.local[float32 * 1]\n",
            "  // attr [iter_var(blockIdx.y, , blockIdx.y)] thread_extent = 1\n",
            "  // attr [iter_var(blockIdx.x, , blockIdx.x)] thread_extent = 1\n",
            "  // attr [iter_var(threadIdx.z, , threadIdx.z)] thread_extent = 256\n",
            "  // attr [iter_var(threadIdx.y, , threadIdx.y)] thread_extent = 7\n",
            "  // attr [iter_var(threadIdx.x, , threadIdx.x)] thread_extent = 7\n",
            "  produce compute.local {\n",
            "    compute.local[0] = 0.000000f\n",
            "    for (rc, 0, 512) {\n",
            "      for (ry, 0, 3) {\n",
            "        for (rx, 0, 3) {\n",
            "          compute.local[0] = (compute.local[0] + (pad_temp.shared.local[((((rc*3) + ry)*3) + rx)]*kernel.shared.local[((((rc*3) + ry)*3) + rx)]))\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "  compute[((((((blockIdx.z*256) + threadIdx.z)*7) + threadIdx.y)*7) + threadIdx.x)] = compute.local[0]\n",
            "}\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QSicMUUukiu2",
        "colab_type": "text"
      },
      "source": [
        "Tile reduction and define shared memory load location:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VT992Qu6ki6n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1476
        },
        "outputId": "f38bfe30-9992-4a14-bce5-de82a616ae12"
      },
      "source": [
        "# tile reduction axes\n",
        "n, f, y, x = s[OL].op.axis\n",
        "rc, ry, rx = s[OL].op.reduce_axis\n",
        "rc_factors = [512, 32, 16]\n",
        "rx_factors = [3, 3, 3]\n",
        "ry_factors = [3, 3, 3]\n",
        "rco, rcm = s[OL].split(rc, factor=rc_factors[1])\n",
        "rcm, rci = s[OL].split(rcm, factor=rc_factors[2])\n",
        "ryo, rym = s[OL].split(ry, factor=ry_factors[1])\n",
        "rym, ryi = s[OL].split(rym, factor=ry_factors[2])\n",
        "rxo, rxm = s[OL].split(rx, factor=rx_factors[1])\n",
        "rxm, rxi = s[OL].split(rxm, factor=rx_factors[2])\n",
        "#rco, rcm, rci = cfg['tile_rc'].apply(s, OL, rc)\n",
        "#ryo, rym, ryi = cfg['tile_rx'].apply(s, OL, ry)\n",
        "#rxo, rxm, rxi = cfg['tile_ry'].apply(s, OL, rx)\n",
        "s[OL].reorder(rco, ryo, rxo, rcm, rym, rxm, rci, ryi, rxi, n, f, y, x)\n",
        "\n",
        "s[AA].compute_at(s[OL], rxo)\n",
        "s[WW].compute_at(s[OL], rxo)\n",
        "s[AL].compute_at(s[OL], rxm)\n",
        "s[WL].compute_at(s[OL], rxm)\n",
        "\n",
        "print(tvm.lower(s, [input, kernel, output], simple_mode=True))\n"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "produce compute {\n",
            "  // attr [iter_var(blockIdx.z, , blockIdx.z)] thread_extent = 2\n",
            "  // attr [compute.local] storage_scope = \"local\"\n",
            "  allocate compute.local[float32 * 1]\n",
            "  // attr [pad_temp.shared] storage_scope = \"shared\"\n",
            "  allocate pad_temp.shared[float32 * 2592]\n",
            "  // attr [kernel.shared] storage_scope = \"shared\"\n",
            "  allocate kernel.shared[float32 * 73728]\n",
            "  // attr [pad_temp.shared.local] storage_scope = \"local\"\n",
            "  allocate pad_temp.shared.local[float32 * 144]\n",
            "  // attr [kernel.shared.local] storage_scope = \"local\"\n",
            "  allocate kernel.shared.local[float32 * 144]\n",
            "  // attr [iter_var(blockIdx.y, , blockIdx.y)] thread_extent = 1\n",
            "  // attr [iter_var(blockIdx.x, , blockIdx.x)] thread_extent = 1\n",
            "  // attr [iter_var(threadIdx.z, , threadIdx.z)] thread_extent = 256\n",
            "  // attr [iter_var(threadIdx.y, , threadIdx.y)] thread_extent = 7\n",
            "  // attr [iter_var(threadIdx.x, , threadIdx.x)] thread_extent = 7\n",
            "  produce compute.local {\n",
            "    compute.local[0] = 0.000000f\n",
            "    for (rc.outer, 0, 16) {\n",
            "      produce pad_temp.shared {\n",
            "        for (ax1, 0, 32) {\n",
            "          for (ax2, 0, 9) {\n",
            "            for (ax3, 0, 9) {\n",
            "              pad_temp.shared[((((ax1*9) + ax2)*9) + ax3)] = tvm_if_then_else(((((1 <= ax2) && (ax2 < 8)) && (1 <= ax3)) && (ax3 < 8)), data[(((((((rc.outer*32) + ax1)*7) + ax2)*7) + ax3) + -8)], 0.000000f)\n",
            "            }\n",
            "          }\n",
            "        }\n",
            "      }\n",
            "      produce kernel.shared {\n",
            "        for (ax0, 0, 256) {\n",
            "          for (ax1, 0, 32) {\n",
            "            for (ax2, 0, 3) {\n",
            "              for (ax3, 0, 3) {\n",
            "                kernel.shared[((((((ax0*32) + ax1)*3) + ax2)*3) + ax3)] = kernel[((((((((((blockIdx.z*256) + ax0)*16) + rc.outer)*32) + ax1)*3) + ax2)*3) + ax3)]\n",
            "              }\n",
            "            }\n",
            "          }\n",
            "        }\n",
            "      }\n",
            "      for (rc.inner.outer, 0, 2) {\n",
            "        produce pad_temp.shared.local {\n",
            "          for (ax1, 0, 16) {\n",
            "            for (ax2, 0, 3) {\n",
            "              for (ax3, 0, 3) {\n",
            "                pad_temp.shared.local[((((ax1*3) + ax2)*3) + ax3)] = pad_temp.shared[((((((((rc.inner.outer*16) + ax1)*9) + ax2) + threadIdx.y)*9) + ax3) + threadIdx.x)]\n",
            "              }\n",
            "            }\n",
            "          }\n",
            "        }\n",
            "        produce kernel.shared.local {\n",
            "          for (ax1, 0, 16) {\n",
            "            for (ax2, 0, 3) {\n",
            "              for (ax3, 0, 3) {\n",
            "                kernel.shared.local[((((ax1*3) + ax2)*3) + ax3)] = kernel.shared[((((((((threadIdx.z*2) + rc.inner.outer)*16) + ax1)*3) + ax2)*3) + ax3)]\n",
            "              }\n",
            "            }\n",
            "          }\n",
            "        }\n",
            "        for (rc.inner.inner, 0, 16) {\n",
            "          for (ry.inner.inner, 0, 3) {\n",
            "            for (rx.inner.inner, 0, 3) {\n",
            "              compute.local[0] = (compute.local[0] + (pad_temp.shared.local[((((rc.inner.inner*3) + ry.inner.inner)*3) + rx.inner.inner)]*kernel.shared.local[((((rc.inner.inner*3) + ry.inner.inner)*3) + rx.inner.inner)]))\n",
            "            }\n",
            "          }\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "  compute[((((((blockIdx.z*256) + threadIdx.z)*7) + threadIdx.y)*7) + threadIdx.x)] = compute.local[0]\n",
            "}\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-8b5e8604e5a7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mfused\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfuse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mtz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfused\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfused\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnparts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"tile_f\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0mty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfused\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfused\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnparts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"tile_y\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfused\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfused\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnparts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"tile_x\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'cfg' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NfBPrhkRmMKC",
        "colab_type": "text"
      },
      "source": [
        "Cooperative fetching:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XdW3ZMB9mMVA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# cooperative fetching\n",
        "for load in [AA, WW]:\n",
        "    n, f, y, x = s[load].op.axis \n",
        "    fused = s[load].fuse(n, f, y, x)\n",
        "    tz, fused = s[load].split(fused, nparts=cfg[\"tile_f\"].size[2])\n",
        "    ty, fused = s[load].split(fused, nparts=cfg[\"tile_y\"].size[2])\n",
        "    tx, fused = s[load].split(fused, nparts=cfg[\"tile_x\"].size[2])\n",
        "    s[load].bind(tz, tvm.thread_axis(\"threadIdx.z\"))\n",
        "    s[load].bind(ty, tvm.thread_axis(\"threadIdx.y\"))\n",
        "    s[load].bind(tx, tvm.thread_axis(\"threadIdx.x\"))\n",
        "\n",
        "# tune unroll\n",
        "#s[output].pragma(kernel_scope, 'auto_unroll_max_step', cfg['auto_unroll_max_step'].val)\n",
        "#s[output].pragma(kernel_scope, 'unroll_explicit', cfg['unroll_explicit'].val)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}