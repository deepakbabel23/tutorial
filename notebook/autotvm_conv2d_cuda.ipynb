{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "autotvm_conv2d_cuda.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/uwsampl/tutorial/blob/master/notebook/autotvm_conv2d_cuda.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8T0-Z4MasbY",
        "colab_type": "text"
      },
      "source": [
        "Tuning High Performance Convolution on NVIDIA GPUs\n",
        "=========================================================================\n",
        "**Author**: `Lianmin Zheng <https://github.com/merrymercy>`_\n",
        "\n",
        "Adapted by `Eddie Yan <https://github.com/eqy>`_\n",
        "\n",
        "This is an advanced tutorial for writing high performance tunable template for\n",
        "NVIDIA GPU. By running auto-tuner on this template, we can outperform the\n",
        "vendor provided library CuDNN in many cases.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9soFF1q_cV-y",
        "colab_type": "text"
      },
      "source": [
        "Please run the following block to ensure TVM is setup for *this notebook*, each notebook may have its own runtime.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G_lzOGuFcUgG",
        "colab_type": "code",
        "outputId": "6667754a-1fbc-464d-a337-3d30b58be5bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4443
        }
      },
      "source": [
        "! gsutil cp \"gs://tvm-fcrc-binariesd5fce43e-8373-11e9-bfb6-0242ac1c0002/tvm.tar.gz\" /tmp/tvm.tar.gz\n",
        "! mkdir -p /tvm\n",
        "! tar -xf /tmp/tvm.tar.gz --strip-components=4 --directory /tvm\n",
        "! ls -la /tvm\n",
        "# Move this block after we are done with pkg step\n",
        "! bash /tvm/package.sh\n",
        "import sys\n",
        "sys.path.append('/tvm/python')\n",
        "sys.path.append('/tvm/topi/python')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Copying gs://tvm-fcrc-binariesd5fce43e-8373-11e9-bfb6-0242ac1c0002/tvm.tar.gz...\n",
            "- [1 files][112.9 MiB/112.9 MiB]                                                \n",
            "Operation completed over 1 objects/112.9 MiB.                                    \n",
            "total 164\n",
            "drwxr-xr-x 21 root root  4096 Jun  5 00:07 .\n",
            "drwxr-xr-x  1 root root  4096 Jun  5 00:07 ..\n",
            "drwx------  8 root root  4096 May 31 08:14 3rdparty\n",
            "drwx------ 12 root root  4096 May 31 08:14 apps\n",
            "drwx------  3 root root  4096 Jun  4 09:46 build\n",
            "drwx------  4 root root  4096 May 31 08:14 cmake\n",
            "-rw-------  1 root root 10406 May 31 08:14 CMakeLists.txt\n",
            "drwx------  6 root root  4096 May 31 08:14 conda\n",
            "-rw-------  1 root root  5673 May 31 08:14 CONTRIBUTORS.md\n",
            "drwx------  3 root root  4096 May 31 08:14 docker\n",
            "drwx------ 11 root root  4096 May 31 08:14 docs\n",
            "drwx------  4 root root  4096 May 31 08:14 golang\n",
            "drwx------  3 root root  4096 May 31 08:14 include\n",
            "-rw-------  1 root root 10027 May 31 08:14 Jenkinsfile\n",
            "drwx------  6 root root  4096 May 31 08:14 jvm\n",
            "-rw-------  1 root root 11357 May 31 08:14 LICENSE\n",
            "-rw-------  1 root root  4267 May 31 08:14 Makefile\n",
            "-rw-------  1 root root 10476 May 31 08:14 NEWS.md\n",
            "drwx------  9 root root  4096 May 31 08:14 nnvm\n",
            "-rw-------  1 root root    61 May 31 08:14 NOTICE\n",
            "-rw-------  1 root root   369 Jun  3 22:49 package.sh\n",
            "drwx------  3 root root  4096 May 31 08:14 python\n",
            "-rw-------  1 root root  2705 May 31 08:14 README.md\n",
            "drwx------  5 root root  4096 May 31 08:14 rust\n",
            "drwx------ 14 root root  4096 May 31 08:14 src\n",
            "drwx------  9 root root  4096 May 31 08:14 tests\n",
            "drwx------  7 root root  4096 May 31 08:14 topi\n",
            "drwx------  8 root root  4096 May 31 08:14 tutorials\n",
            "-rw-------  1 root root  2902 May 31 08:14 version.py\n",
            "drwx------ 10 root root  4096 May 31 08:14 vta\n",
            "drwx------  2 root root  4096 May 31 08:14 web\n",
            "Installing Dependencies ...\n",
            "deb https://dl.bintray.com/sbt/debian /\n",
            "Executing: /tmp/apt-key-gpghome.UgqYcR7LVW/gpg.1.sh --keyserver hkp://keyserver.ubuntu.com:80 --recv 2EE0EA64E40A89B84B2DF73499E82A75642AC823\n",
            "gpg: key 99E82A75642AC823: public key \"sbt build tool <scalasbt@gmail.com>\" imported\n",
            "gpg: Total number processed: 1\n",
            "gpg:               imported: 1\n",
            "Get:1 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran35/ InRelease [3,626 B]\n",
            "Ign:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Ign:3 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:4 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Hit:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
            "Hit:6 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Get:7 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran35/ Packages [56.4 kB]\n",
            "Get:8 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Hit:9 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Ign:10 https://dl.bintray.com/sbt/debian  InRelease\n",
            "Get:11 https://dl.bintray.com/sbt/debian  Release [815 B]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Get:14 https://dl.bintray.com/sbt/debian  Release.gpg [821 B]\n",
            "Get:15 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic InRelease [15.4 kB]\n",
            "Get:17 https://dl.bintray.com/sbt/debian  Packages [3,424 B]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Get:19 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [322 kB]\n",
            "Get:20 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic/main Sources [1,646 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [1,211 kB]\n",
            "Get:22 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [470 kB]\n",
            "Get:23 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [825 kB]\n",
            "Get:24 http://archive.ubuntu.com/ubuntu bionic-backports/universe amd64 Packages [3,902 B]\n",
            "Get:25 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic/main amd64 Packages [790 kB]\n",
            "Fetched 5,601 kB in 3s (2,095 kB/s)\n",
            "Reading package lists... Done\n",
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "zlib1g-dev is already the newest version (1:1.2.11.dfsg-0ubuntu2).\n",
            "zlib1g-dev set to manually installed.\n",
            "clinfo is already the newest version (2.2.18.03.26-1).\n",
            "libtinfo-dev is already the newest version (6.1-1ubuntu1.18.04).\n",
            "libtinfo-dev set to manually installed.\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-410\n",
            "Use 'sudo apt autoremove' to remove it.\n",
            "Suggested packages:\n",
            "  llvm-6.0-doc\n",
            "The following NEW packages will be installed:\n",
            "  binfmt-support libffi-dev llvm-6.0 llvm-6.0-dev llvm-6.0-runtime\n",
            "0 upgraded, 5 newly installed, 0 to remove and 30 not upgraded.\n",
            "Need to get 28.2 MB of archives.\n",
            "After this operation, 178 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/main amd64 binfmt-support amd64 2.1.8-2 [51.6 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/main amd64 llvm-6.0-runtime amd64 1:6.0-1ubuntu2 [200 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic/main amd64 llvm-6.0 amd64 1:6.0-1ubuntu2 [4,838 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic/main amd64 libffi-dev amd64 3.2.1-8 [156 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu bionic/main amd64 llvm-6.0-dev amd64 1:6.0-1ubuntu2 [23.0 MB]\n",
            "Fetched 28.2 MB in 2s (14.7 MB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 5.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package binfmt-support.\n",
            "(Reading database ... 130912 files and directories currently installed.)\n",
            "Preparing to unpack .../binfmt-support_2.1.8-2_amd64.deb ...\n",
            "Unpacking binfmt-support (2.1.8-2) ...\n",
            "Selecting previously unselected package llvm-6.0-runtime.\n",
            "Preparing to unpack .../llvm-6.0-runtime_1%3a6.0-1ubuntu2_amd64.deb ...\n",
            "Unpacking llvm-6.0-runtime (1:6.0-1ubuntu2) ...\n",
            "Selecting previously unselected package llvm-6.0.\n",
            "Preparing to unpack .../llvm-6.0_1%3a6.0-1ubuntu2_amd64.deb ...\n",
            "Unpacking llvm-6.0 (1:6.0-1ubuntu2) ...\n",
            "Selecting previously unselected package libffi-dev:amd64.\n",
            "Preparing to unpack .../libffi-dev_3.2.1-8_amd64.deb ...\n",
            "Unpacking libffi-dev:amd64 (3.2.1-8) ...\n",
            "Selecting previously unselected package llvm-6.0-dev.\n",
            "Preparing to unpack .../llvm-6.0-dev_1%3a6.0-1ubuntu2_amd64.deb ...\n",
            "Unpacking llvm-6.0-dev (1:6.0-1ubuntu2) ...\n",
            "Setting up binfmt-support (2.1.8-2) ...\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/binfmt-support.service → /lib/systemd/system/binfmt-support.service.\n",
            "invoke-rc.d: could not determine current runlevel\n",
            "invoke-rc.d: policy-rc.d denied execution of start.\n",
            "Setting up libffi-dev:amd64 (3.2.1-8) ...\n",
            "Setting up llvm-6.0-runtime (1:6.0-1ubuntu2) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1) ...\n",
            "Setting up llvm-6.0 (1:6.0-1ubuntu2) ...\n",
            "Processing triggers for systemd (237-3ubuntu10.21) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Setting up llvm-6.0-dev (1:6.0-1ubuntu2) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1) ...\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-410\n",
            "Use 'sudo apt autoremove' to remove it.\n",
            "Suggested packages:\n",
            "  gtkwave systemc\n",
            "The following NEW packages will be installed:\n",
            "  sbt verilator\n",
            "0 upgraded, 2 newly installed, 0 to remove and 30 not upgraded.\n",
            "Need to get 4,005 kB of archives.\n",
            "After this operation, 14.4 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 verilator amd64 3.916-1build1 [2,878 kB]\n",
            "Get:2 https://dl.bintray.com/sbt/debian  sbt 1.2.8 [1,126 kB]\n",
            "Fetched 4,005 kB in 1s (4,202 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 2.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package sbt.\n",
            "(Reading database ... 132546 files and directories currently installed.)\n",
            "Preparing to unpack .../apt/archives/sbt_1.2.8_all.deb ...\n",
            "Unpacking sbt (1.2.8) ...\n",
            "Selecting previously unselected package verilator.\n",
            "Preparing to unpack .../verilator_3.916-1build1_amd64.deb ...\n",
            "Unpacking verilator (3.916-1build1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Setting up verilator (3.916-1build1) ...\n",
            "Setting up sbt (1.2.8) ...\n",
            "Creating system group: sbt\n",
            "Creating system user: sbt in sbt with sbt daemon-user and shell /bin/false\n",
            "Copying gs://tvm-fcrc-binariesd5fce43e-8373-11e9-bfb6-0242ac1c0002/tvm.tar.gz...\n",
            "- [1 files][112.9 MiB/112.9 MiB]                                                \n",
            "Operation completed over 1 objects/112.9 MiB.                                    \n",
            "total 164\n",
            "drwxr-xr-x 21 root root  4096 Jun  5 00:07 .\n",
            "drwxr-xr-x  1 root root  4096 Jun  5 00:07 ..\n",
            "drwx------  8 root root  4096 May 31 08:14 3rdparty\n",
            "drwx------ 12 root root  4096 May 31 08:14 apps\n",
            "drwx------  3 root root  4096 Jun  4 09:46 build\n",
            "drwx------  4 root root  4096 May 31 08:14 cmake\n",
            "-rw-------  1 root root 10406 May 31 08:14 CMakeLists.txt\n",
            "drwx------  6 root root  4096 May 31 08:14 conda\n",
            "-rw-------  1 root root  5673 May 31 08:14 CONTRIBUTORS.md\n",
            "drwx------  3 root root  4096 May 31 08:14 docker\n",
            "drwx------ 11 root root  4096 May 31 08:14 docs\n",
            "drwx------  4 root root  4096 May 31 08:14 golang\n",
            "drwx------  3 root root  4096 May 31 08:14 include\n",
            "-rw-------  1 root root 10027 May 31 08:14 Jenkinsfile\n",
            "drwx------  6 root root  4096 May 31 08:14 jvm\n",
            "-rw-------  1 root root 11357 May 31 08:14 LICENSE\n",
            "-rw-------  1 root root  4267 May 31 08:14 Makefile\n",
            "-rw-------  1 root root 10476 May 31 08:14 NEWS.md\n",
            "drwx------  9 root root  4096 May 31 08:14 nnvm\n",
            "-rw-------  1 root root    61 May 31 08:14 NOTICE\n",
            "-rw-------  1 root root   369 Jun  3 22:49 package.sh\n",
            "drwx------  3 root root  4096 May 31 08:14 python\n",
            "-rw-------  1 root root  2705 May 31 08:14 README.md\n",
            "drwx------  5 root root  4096 May 31 08:14 rust\n",
            "drwx------ 14 root root  4096 May 31 08:14 src\n",
            "drwx------  9 root root  4096 May 31 08:14 tests\n",
            "drwx------  7 root root  4096 May 31 08:14 topi\n",
            "drwx------  8 root root  4096 May 31 08:14 tutorials\n",
            "-rw-------  1 root root  2902 May 31 08:14 version.py\n",
            "drwx------ 10 root root  4096 May 31 08:14 vta\n",
            "drwx------  2 root root  4096 May 31 08:14 web\n",
            "Installing Dependencies ...\n",
            "deb https://dl.bintray.com/sbt/debian /\n",
            "Executing: /tmp/apt-key-gpghome.073xCalthj/gpg.1.sh --keyserver hkp://keyserver.ubuntu.com:80 --recv 2EE0EA64E40A89B84B2DF73499E82A75642AC823\n",
            "gpg: key 99E82A75642AC823: \"sbt build tool <scalasbt@gmail.com>\" not changed\n",
            "gpg: Total number processed: 1\n",
            "gpg:              unchanged: 1\n",
            "Hit:1 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran35/ InRelease\n",
            "Ign:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Ign:3 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:4 http://security.ubuntu.com/ubuntu bionic-security InRelease\n",
            "Hit:5 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Hit:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
            "Hit:8 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Ign:9 https://dl.bintray.com/sbt/debian  InRelease\n",
            "Hit:10 http://archive.ubuntu.com/ubuntu bionic-updates InRelease\n",
            "Get:11 https://dl.bintray.com/sbt/debian  Release [815 B]\n",
            "Hit:12 http://archive.ubuntu.com/ubuntu bionic-backports InRelease\n",
            "Hit:16 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic InRelease\n",
            "Fetched 815 B in 1s (544 B/s)\n",
            "Reading package lists... Done\n",
            "W: Target Packages (Packages) is configured multiple times in /etc/apt/sources.list.d/sbt.list:1 and /etc/apt/sources.list.d/sbt.list:2\n",
            "W: Target Packages (Packages) is configured multiple times in /etc/apt/sources.list.d/sbt.list:1 and /etc/apt/sources.list.d/sbt.list:2\n",
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "libffi-dev is already the newest version (3.2.1-8).\n",
            "llvm-6.0 is already the newest version (1:6.0-1ubuntu2).\n",
            "zlib1g-dev is already the newest version (1:1.2.11.dfsg-0ubuntu2).\n",
            "clinfo is already the newest version (2.2.18.03.26-1).\n",
            "libtinfo-dev is already the newest version (6.1-1ubuntu1.18.04).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-410\n",
            "Use 'sudo apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 30 not upgraded.\n",
            "W: Target Packages (Packages) is configured multiple times in /etc/apt/sources.list.d/sbt.list:1 and /etc/apt/sources.list.d/sbt.list:2\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "verilator is already the newest version (3.916-1build1).\n",
            "sbt is already the newest version (1.2.8).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-410\n",
            "Use 'sudo apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 30 not upgraded.\n",
            "W: Target Packages (Packages) is configured multiple times in /etc/apt/sources.list.d/sbt.list:1 and /etc/apt/sources.list.d/sbt.list:2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "agr421w8a49k",
        "colab_type": "text"
      },
      "source": [
        "Import packages:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C9JnukA4aTLJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import logging\n",
        "import sys\n",
        "import numpy as np\n",
        "\n",
        "import tvm\n",
        "import topi\n",
        "from topi.testing import conv2d_nchw_python\n",
        "\n",
        "from tvm import autotvm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u5YlgPYma_5r",
        "colab_type": "text"
      },
      "source": [
        "Step 0: Vanilla direct 2D convolution implementation without a tunable template\n",
        "---------------------------------------------------------------------------------------------\n",
        "\n",
        "Default Schedule:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DhtW-j3bbMLI",
        "colab_type": "code",
        "outputId": "8c55c541-b183-4717-f8b5-da8b1cb89000",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 550
        }
      },
      "source": [
        "# the last layer in resnet\n",
        "N, H, W, CO, CI, KH, KW, stride, padding = 1, 7, 7, 512, 512, 3, 3, (1, 1), (1, 1)\n",
        "assert N == 1, \"Only consider batch_size = 1 in this template\"\n",
        "\n",
        "data = tvm.placeholder((N, CI, H, W), name='data')\n",
        "kernel = tvm.placeholder((CO, CI, KH, KW), name='kernel')\n",
        "conv = topi.nn.conv2d_nchw(data, kernel, stride, padding, dilation=1, out_dtype='float32')\n",
        "s = tvm.create_schedule([conv.op])\n",
        "print(\"Default Schedule:\")\n",
        "print(tvm.lower(s, [data, kernel, conv], simple_mode=True))\n",
        "\n",
        "n, f, y, x = s[conv].op.axis\n",
        "rc, ry, rx = s[conv].op.reduce_axis"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Default Schedule:\n",
            "// attr [pad_temp] storage_scope = \"global\"\n",
            "allocate pad_temp[float32 * 41472]\n",
            "produce pad_temp {\n",
            "  for (i1, 0, 512) {\n",
            "    for (i2, 0, 9) {\n",
            "      for (i3, 0, 9) {\n",
            "        pad_temp[((((i1*9) + i2)*9) + i3)] = tvm_if_then_else(((((1 <= i2) && (i2 < 8)) && (1 <= i3)) && (i3 < 8)), data[(((((i1*7) + i2)*7) + i3) + -8)], 0.000000f)\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "produce compute {\n",
            "  for (ff, 0, 512) {\n",
            "    for (yy, 0, 7) {\n",
            "      for (xx, 0, 7) {\n",
            "        compute[((((ff*7) + yy)*7) + xx)] = 0.000000f\n",
            "        for (rc, 0, 512) {\n",
            "          for (ry, 0, 3) {\n",
            "            for (rx, 0, 3) {\n",
            "              compute[((((ff*7) + yy)*7) + xx)] = (compute[((((ff*7) + yy)*7) + xx)] + (pad_temp[((((((rc*9) + yy) + ry)*9) + xx) + rx)]*kernel[((((((ff*512) + rc)*3) + ry)*3) + rx)]))\n",
            "            }\n",
            "          }\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PYacZkkNd_xG",
        "colab_type": "text"
      },
      "source": [
        "Inline padding and create cache stages:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "afD69lgUd-QG",
        "colab_type": "code",
        "outputId": "919dab32-a184-41df-dc44-106a3ea4370a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1433
        }
      },
      "source": [
        "# inline padding\n",
        "pad_data = s[conv].op.input_tensors[0]\n",
        "s[pad_data].compute_inline()\n",
        "input = data\n",
        "data, raw_data = pad_data, data\n",
        "\n",
        "output = conv\n",
        "OL = s.cache_write(conv, 'local')\n",
        "\n",
        "# create cache stage\n",
        "AA = s.cache_read(data, 'shared', [OL])\n",
        "WW = s.cache_read(kernel, 'shared', [OL])\n",
        "AL = s.cache_read(AA, 'local', [OL])\n",
        "WL = s.cache_read(WW, 'local', [OL])\n",
        "\n",
        "print(tvm.lower(s, [input, kernel, conv], simple_mode=True))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "// attr [pad_temp.shared] storage_scope = \"shared\"\n",
            "allocate pad_temp.shared[float32 * 41472]\n",
            "// attr [pad_temp.shared.local] storage_scope = \"local\"\n",
            "allocate pad_temp.shared.local[float32 * 41472]\n",
            "// attr [kernel.shared] storage_scope = \"shared\"\n",
            "allocate kernel.shared[float32 * 2359296]\n",
            "// attr [kernel.shared.local] storage_scope = \"local\"\n",
            "allocate kernel.shared.local[float32 * 2359296]\n",
            "// attr [compute.local] storage_scope = \"local\"\n",
            "allocate compute.local[float32 * 25088]\n",
            "produce pad_temp.shared {\n",
            "  for (ax1, 0, 512) {\n",
            "    for (ax2, 0, 9) {\n",
            "      for (ax3, 0, 9) {\n",
            "        pad_temp.shared[((((ax1*9) + ax2)*9) + ax3)] = tvm_if_then_else(((((1 <= ax2) && (ax2 < 8)) && (1 <= ax3)) && (ax3 < 8)), data[(((((ax1*7) + ax2)*7) + ax3) + -8)], 0.000000f)\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "produce pad_temp.shared.local {\n",
            "  for (ax1, 0, 512) {\n",
            "    for (ax2, 0, 9) {\n",
            "      for (ax3, 0, 9) {\n",
            "        pad_temp.shared.local[((((ax1*9) + ax2)*9) + ax3)] = pad_temp.shared[((((ax1*9) + ax2)*9) + ax3)]\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "produce kernel.shared {\n",
            "  for (ax0, 0, 512) {\n",
            "    for (ax1, 0, 512) {\n",
            "      for (ax2, 0, 3) {\n",
            "        for (ax3, 0, 3) {\n",
            "          kernel.shared[((((((ax0*512) + ax1)*3) + ax2)*3) + ax3)] = kernel[((((((ax0*512) + ax1)*3) + ax2)*3) + ax3)]\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "produce kernel.shared.local {\n",
            "  for (ax0, 0, 512) {\n",
            "    for (ax1, 0, 512) {\n",
            "      for (ax2, 0, 3) {\n",
            "        for (ax3, 0, 3) {\n",
            "          kernel.shared.local[((((((ax0*512) + ax1)*3) + ax2)*3) + ax3)] = kernel.shared[((((((ax0*512) + ax1)*3) + ax2)*3) + ax3)]\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "produce compute.local {\n",
            "  for (ff.c, 0, 512) {\n",
            "    for (yy.c, 0, 7) {\n",
            "      for (xx.c, 0, 7) {\n",
            "        compute.local[((((ff.c*7) + yy.c)*7) + xx.c)] = 0.000000f\n",
            "        for (rc, 0, 512) {\n",
            "          for (ry, 0, 3) {\n",
            "            for (rx, 0, 3) {\n",
            "              compute.local[((((ff.c*7) + yy.c)*7) + xx.c)] = (compute.local[((((ff.c*7) + yy.c)*7) + xx.c)] + (pad_temp.shared.local[((((((rc*9) + yy.c) + ry)*9) + xx.c) + rx)]*kernel.shared.local[((((((ff.c*512) + rc)*3) + ry)*3) + rx)]))\n",
            "            }\n",
            "          }\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "produce compute {\n",
            "  for (ff, 0, 512) {\n",
            "    for (yy, 0, 7) {\n",
            "      for (xx, 0, 7) {\n",
            "        compute[((((ff*7) + yy)*7) + xx)] = compute.local[((((ff*7) + yy)*7) + xx)]\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iDGX2K6MeWH6",
        "colab_type": "text"
      },
      "source": [
        "Define and split output spatial axes:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yLaKLAH7eWRH",
        "colab_type": "code",
        "outputId": "1e5b2083-c54c-435b-d042-5269e5187a3d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1470
        }
      },
      "source": [
        "# tile spatial axes\n",
        "n, f, y, x = s[output].op.axis\n",
        "tile_f_factors = [32, 32, 32, 1]\n",
        "tile_x_factors = [1, 1, 1, 1]\n",
        "tile_y_factors = [7, 7, 7, 1]\n",
        "\n",
        "bf, vf = s[output].split(f, factor=tile_f_factors[1])\n",
        "vf, tf = s[output].split(vf, factor=tile_f_factors[2])\n",
        "tf, fi = s[output].split(tf, factor=tile_f_factors[3])\n",
        "\n",
        "by, vy = s[output].split(y, factor=tile_y_factors[1])\n",
        "vy, ty = s[output].split(vy, factor=tile_y_factors[2])\n",
        "ty, yi = s[output].split(ty, factor=tile_y_factors[3])\n",
        "\n",
        "bx, vx = s[output].split(x, factor=tile_x_factors[1])\n",
        "vx, tx = s[output].split(vx, factor=tile_x_factors[2])\n",
        "tx, xi, = s[output].split(tx, factor=tile_x_factors[3])\n",
        "\n",
        "kernel_scope = n  # this is the scope to attach global config inside this kernel\n",
        "\n",
        "s[output].reorder(n, bf, by, bx, vf, vy, vx, tf, ty, tx, fi, yi, xi)\n",
        "print(tvm.lower(s, [input, kernel, conv], simple_mode=True))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "// attr [pad_temp.shared] storage_scope = \"shared\"\n",
            "allocate pad_temp.shared[float32 * 41472]\n",
            "// attr [pad_temp.shared.local] storage_scope = \"local\"\n",
            "allocate pad_temp.shared.local[float32 * 41472]\n",
            "// attr [kernel.shared] storage_scope = \"shared\"\n",
            "allocate kernel.shared[float32 * 2359296]\n",
            "// attr [kernel.shared.local] storage_scope = \"local\"\n",
            "allocate kernel.shared.local[float32 * 2359296]\n",
            "// attr [compute.local] storage_scope = \"local\"\n",
            "allocate compute.local[float32 * 25088]\n",
            "produce pad_temp.shared {\n",
            "  for (ax1, 0, 512) {\n",
            "    for (ax2, 0, 9) {\n",
            "      for (ax3, 0, 9) {\n",
            "        pad_temp.shared[((((ax1*9) + ax2)*9) + ax3)] = tvm_if_then_else(((((1 <= ax2) && (ax2 < 8)) && (1 <= ax3)) && (ax3 < 8)), data[(((((ax1*7) + ax2)*7) + ax3) + -8)], 0.000000f)\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "produce pad_temp.shared.local {\n",
            "  for (ax1, 0, 512) {\n",
            "    for (ax2, 0, 9) {\n",
            "      for (ax3, 0, 9) {\n",
            "        pad_temp.shared.local[((((ax1*9) + ax2)*9) + ax3)] = pad_temp.shared[((((ax1*9) + ax2)*9) + ax3)]\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "produce kernel.shared {\n",
            "  for (ax0, 0, 512) {\n",
            "    for (ax1, 0, 512) {\n",
            "      for (ax2, 0, 3) {\n",
            "        for (ax3, 0, 3) {\n",
            "          kernel.shared[((((((ax0*512) + ax1)*3) + ax2)*3) + ax3)] = kernel[((((((ax0*512) + ax1)*3) + ax2)*3) + ax3)]\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "produce kernel.shared.local {\n",
            "  for (ax0, 0, 512) {\n",
            "    for (ax1, 0, 512) {\n",
            "      for (ax2, 0, 3) {\n",
            "        for (ax3, 0, 3) {\n",
            "          kernel.shared.local[((((((ax0*512) + ax1)*3) + ax2)*3) + ax3)] = kernel.shared[((((((ax0*512) + ax1)*3) + ax2)*3) + ax3)]\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "produce compute.local {\n",
            "  for (ff.c, 0, 512) {\n",
            "    for (yy.c, 0, 7) {\n",
            "      for (xx.c, 0, 7) {\n",
            "        compute.local[((((ff.c*7) + yy.c)*7) + xx.c)] = 0.000000f\n",
            "        for (rc, 0, 512) {\n",
            "          for (ry, 0, 3) {\n",
            "            for (rx, 0, 3) {\n",
            "              compute.local[((((ff.c*7) + yy.c)*7) + xx.c)] = (compute.local[((((ff.c*7) + yy.c)*7) + xx.c)] + (pad_temp.shared.local[((((((rc*9) + yy.c) + ry)*9) + xx.c) + rx)]*kernel.shared.local[((((((ff.c*512) + rc)*3) + ry)*3) + rx)]))\n",
            "            }\n",
            "          }\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "produce compute {\n",
            "  for (ff.outer, 0, 16) {\n",
            "    for (xx.outer, 0, 7) {\n",
            "      for (ff.inner.inner.outer, 0, 32) {\n",
            "        for (yy.inner.inner.outer, 0, 7) {\n",
            "          compute[((((((ff.outer*32) + ff.inner.inner.outer)*7) + yy.inner.inner.outer)*7) + xx.outer)] = compute.local[((((((ff.outer*32) + ff.inner.inner.outer)*7) + yy.inner.inner.outer)*7) + xx.outer)]\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RH6gor8XjvXM",
        "colab_type": "text"
      },
      "source": [
        "Bind Axes:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_yIhMYiQjvhQ",
        "colab_type": "code",
        "outputId": "301543bc-4689-4c6a-d996-b5b09e023c1e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1212
        }
      },
      "source": [
        "s[output].bind(bf, tvm.thread_axis(\"blockIdx.z\"))\n",
        "s[output].bind(by, tvm.thread_axis(\"blockIdx.y\"))\n",
        "s[output].bind(bx, tvm.thread_axis(\"blockIdx.x\"))\n",
        "s[output].bind(vf, tvm.thread_axis(\"vthread\"))\n",
        "s[output].bind(vy, tvm.thread_axis(\"vthread\"))\n",
        "s[output].bind(vx, tvm.thread_axis(\"vthread\"))\n",
        "s[output].bind(tf, tvm.thread_axis(\"threadIdx.z\"))\n",
        "s[output].bind(ty, tvm.thread_axis(\"threadIdx.y\"))\n",
        "s[output].bind(tx, tvm.thread_axis(\"threadIdx.x\"))\n",
        "s[OL].compute_at(s[output], tx)\n",
        "print(tvm.lower(s, [input, kernel, output], simple_mode=True))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "// attr [pad_temp.shared] storage_scope = \"shared\"\n",
            "allocate pad_temp.shared[float32 * 4608]\n",
            "// attr [pad_temp.shared.local] storage_scope = \"local\"\n",
            "allocate pad_temp.shared.local[float32 * 4608]\n",
            "// attr [kernel.shared.local] storage_scope = \"local\"\n",
            "allocate kernel.shared.local[float32 * 4608]\n",
            "produce pad_temp.shared {\n",
            "  for (ax1, 0, 512) {\n",
            "    for (ax2, 0, 3) {\n",
            "      for (ax3, 0, 3) {\n",
            "        pad_temp.shared[((((ax1*3) + ax2)*3) + ax3)] = tvm_if_then_else((((((1 - threadIdx.y) <= ax2) && (ax2 < (8 - threadIdx.y))) && ((1 - blockIdx.x) <= ax3)) && (ax3 < (8 - blockIdx.x))), data[(((((((ax1*7) + ax2) + threadIdx.y)*7) + ax3) + blockIdx.x) + -8)], 0.000000f)\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "produce pad_temp.shared.local {\n",
            "  for (ax1, 0, 512) {\n",
            "    for (ax2, 0, 3) {\n",
            "      for (ax3, 0, 3) {\n",
            "        pad_temp.shared.local[((((ax1*3) + ax2)*3) + ax3)] = pad_temp.shared[((((ax1*3) + ax2)*3) + ax3)]\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "produce kernel.shared {\n",
            "  for (ax1, 0, 512) {\n",
            "    for (ax2, 0, 3) {\n",
            "      for (ax3, 0, 3) {\n",
            "        pad_temp.shared[((((ax1*3) + ax2)*3) + ax3)] = kernel[((((((((blockIdx.z*32) + threadIdx.z)*512) + ax1)*3) + ax2)*3) + ax3)]\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "produce kernel.shared.local {\n",
            "  for (ax1, 0, 512) {\n",
            "    for (ax2, 0, 3) {\n",
            "      for (ax3, 0, 3) {\n",
            "        kernel.shared.local[((((ax1*3) + ax2)*3) + ax3)] = pad_temp.shared[((((ax1*3) + ax2)*3) + ax3)]\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "produce compute {\n",
            "  // attr [iter_var(blockIdx.z, , blockIdx.z)] thread_extent = 16\n",
            "  // attr [compute.local] storage_scope = \"local\"\n",
            "  allocate compute.local[float32 * 1]\n",
            "  // attr [iter_var(blockIdx.y, , blockIdx.y)] thread_extent = 1\n",
            "  // attr [iter_var(blockIdx.x, , blockIdx.x)] thread_extent = 7\n",
            "  // attr [iter_var(threadIdx.z, , threadIdx.z)] thread_extent = 32\n",
            "  // attr [iter_var(threadIdx.y, , threadIdx.y)] thread_extent = 7\n",
            "  // attr [iter_var(threadIdx.x, , threadIdx.x)] thread_extent = 1\n",
            "  produce compute.local {\n",
            "    compute.local[0] = 0.000000f\n",
            "    for (rc, 0, 512) {\n",
            "      for (ry, 0, 3) {\n",
            "        for (rx, 0, 3) {\n",
            "          compute.local[0] = (compute.local[0] + (pad_temp.shared.local[((((rc*3) + ry)*3) + rx)]*kernel.shared.local[((((rc*3) + ry)*3) + rx)]))\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "  compute[((((((blockIdx.z*32) + threadIdx.z)*7) + threadIdx.y)*7) + blockIdx.x)] = compute.local[0]\n",
            "}\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QSicMUUukiu2",
        "colab_type": "text"
      },
      "source": [
        "Tile reduction and define shared memory load location:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VT992Qu6ki6n",
        "colab_type": "code",
        "outputId": "a091025f-8ebd-4f95-bc7c-f784db4bc6fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 992
        }
      },
      "source": [
        "# tile reduction axes\n",
        "n, f, y, x = s[OL].op.axis\n",
        "rc, ry, rx = s[OL].op.reduce_axis\n",
        "rc_factors = [512, 32, 1]\n",
        "rx_factors = [3, 1, 1]\n",
        "ry_factors = [3, 1, 1]\n",
        "rco, rcm = s[OL].split(rc, factor=rc_factors[1])\n",
        "rcm, rci = s[OL].split(rcm, factor=rc_factors[2])\n",
        "ryo, rym = s[OL].split(ry, factor=ry_factors[1])\n",
        "rym, ryi = s[OL].split(rym, factor=ry_factors[2])\n",
        "rxo, rxm = s[OL].split(rx, factor=rx_factors[1])\n",
        "rxm, rxi = s[OL].split(rxm, factor=rx_factors[2])\n",
        "#rco, rcm, rci = cfg['tile_rc'].apply(s, OL, rc)\n",
        "#ryo, rym, ryi = cfg['tile_rx'].apply(s, OL, ry)\n",
        "#rxo, rxm, rxi = cfg['tile_ry'].apply(s, OL, rx)\n",
        "s[OL].reorder(rco, ryo, rxo, rcm, rym, rxm, rci, ryi, rxi, n, f, y, x)\n",
        "\n",
        "s[AA].compute_at(s[OL], rxo)\n",
        "s[WW].compute_at(s[OL], rxo)\n",
        "s[AL].compute_at(s[OL], rxm)\n",
        "s[WL].compute_at(s[OL], rxm)\n",
        "\n",
        "print(tvm.lower(s, [input, kernel, output], simple_mode=True))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "produce compute {\n",
            "  // attr [iter_var(blockIdx.z, , blockIdx.z)] thread_extent = 16\n",
            "  // attr [compute.local] storage_scope = \"local\"\n",
            "  allocate compute.local[float32 * 1]\n",
            "  // attr [pad_temp.shared] storage_scope = \"shared\"\n",
            "  allocate pad_temp.shared[float32 * 224]\n",
            "  // attr [kernel.shared] storage_scope = \"shared\"\n",
            "  allocate kernel.shared[float32 * 1024]\n",
            "  // attr [pad_temp.shared.local] storage_scope = \"local\"\n",
            "  allocate pad_temp.shared.local[float32 * 1]\n",
            "  // attr [kernel.shared.local] storage_scope = \"local\"\n",
            "  allocate kernel.shared.local[float32 * 1]\n",
            "  // attr [iter_var(blockIdx.y, , blockIdx.y)] thread_extent = 1\n",
            "  // attr [iter_var(blockIdx.x, , blockIdx.x)] thread_extent = 7\n",
            "  // attr [iter_var(threadIdx.z, , threadIdx.z)] thread_extent = 32\n",
            "  // attr [iter_var(threadIdx.y, , threadIdx.y)] thread_extent = 7\n",
            "  // attr [iter_var(threadIdx.x, , threadIdx.x)] thread_extent = 1\n",
            "  produce compute.local {\n",
            "    compute.local[0] = 0.000000f\n",
            "    for (rc.outer, 0, 16) {\n",
            "      for (ry.outer, 0, 3) {\n",
            "        for (rx.outer, 0, 3) {\n",
            "          produce pad_temp.shared {\n",
            "            for (ax1, 0, 32) {\n",
            "              for (ax2, 0, 7) {\n",
            "                pad_temp.shared[((ax1*7) + ax2)] = tvm_if_then_else((((((1 - ry.outer) <= ax2) && (ax2 < (8 - ry.outer))) && ((1 - rx.outer) <= blockIdx.x)) && (blockIdx.x < (8 - rx.outer))), data[(((((((((rc.outer*32) + ax1)*7) + ax2) + ry.outer)*7) + blockIdx.x) + rx.outer) + -8)], 0.000000f)\n",
            "              }\n",
            "            }\n",
            "          }\n",
            "          produce kernel.shared {\n",
            "            for (ax0, 0, 32) {\n",
            "              for (ax1, 0, 32) {\n",
            "                kernel.shared[((ax0*32) + ax1)] = kernel[((((((((((blockIdx.z*32) + ax0)*16) + rc.outer)*32) + ax1)*3) + ry.outer)*3) + rx.outer)]\n",
            "              }\n",
            "            }\n",
            "          }\n",
            "          for (rc.inner.outer, 0, 32) {\n",
            "            produce pad_temp.shared.local {\n",
            "              pad_temp.shared.local[0] = pad_temp.shared[((rc.inner.outer*7) + threadIdx.y)]\n",
            "            }\n",
            "            produce kernel.shared.local {\n",
            "              kernel.shared.local[0] = kernel.shared[((threadIdx.z*32) + rc.inner.outer)]\n",
            "            }\n",
            "            compute.local[0] = (compute.local[0] + (pad_temp.shared.local[0]*kernel.shared.local[0]))\n",
            "          }\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "  compute[((((((blockIdx.z*32) + threadIdx.z)*7) + threadIdx.y)*7) + blockIdx.x)] = compute.local[0]\n",
            "}\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NfBPrhkRmMKC",
        "colab_type": "text"
      },
      "source": [
        "Cooperative fetching:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XdW3ZMB9mMVA",
        "colab_type": "code",
        "outputId": "ab837add-f8eb-4972-cfca-224cc2aa16a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1139
        }
      },
      "source": [
        "# cooperative fetching\n",
        "for load in [AA, WW]:\n",
        "    n, f, y, x = s[load].op.axis \n",
        "    fused = s[load].fuse(n, f, y, x)\n",
        "    tz, fused = s[load].split(fused, nparts=tile_f_factors[2])\n",
        "    ty, fused = s[load].split(fused, nparts=tile_y_factors[2])\n",
        "    tx, fused = s[load].split(fused, nparts=tile_x_factors[2])\n",
        "    s[load].bind(tz, tvm.thread_axis(\"threadIdx.z\"))\n",
        "    s[load].bind(ty, tvm.thread_axis(\"threadIdx.y\"))\n",
        "    s[load].bind(tx, tvm.thread_axis(\"threadIdx.x\"))\n",
        "print(tvm.lower(s, [input, kernel, output], simple_mode=True))\n",
        "# tune unroll\n",
        "#s[output].pragma(kernel_scope, 'auto_unroll_max_step', cfg['auto_unroll_max_step'].val)\n",
        "#s[output].pragma(kernel_scope, 'unroll_explicit', cfg['unroll_explicit'].val)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "produce compute {\n",
            "  // attr [iter_var(blockIdx.z, , blockIdx.z)] thread_extent = 16\n",
            "  // attr [compute.local] storage_scope = \"local\"\n",
            "  allocate compute.local[float32 * 1]\n",
            "  // attr [pad_temp.shared] storage_scope = \"shared\"\n",
            "  allocate pad_temp.shared[float32 * 224]\n",
            "  // attr [kernel.shared] storage_scope = \"shared\"\n",
            "  allocate kernel.shared[float32 * 1024]\n",
            "  // attr [pad_temp.shared.local] storage_scope = \"local\"\n",
            "  allocate pad_temp.shared.local[float32 * 1]\n",
            "  // attr [kernel.shared.local] storage_scope = \"local\"\n",
            "  allocate kernel.shared.local[float32 * 1]\n",
            "  // attr [iter_var(blockIdx.y, , blockIdx.y)] thread_extent = 1\n",
            "  // attr [iter_var(blockIdx.x, , blockIdx.x)] thread_extent = 7\n",
            "  // attr [iter_var(threadIdx.z, , threadIdx.z)] thread_extent = 32\n",
            "  // attr [iter_var(threadIdx.y, , threadIdx.y)] thread_extent = 7\n",
            "  // attr [iter_var(threadIdx.x, , threadIdx.x)] thread_extent = 1\n",
            "  produce compute.local {\n",
            "    compute.local[0] = 0.000000f\n",
            "    for (rc.outer, 0, 16) {\n",
            "      for (ry.outer, 0, 3) {\n",
            "        for (rx.outer, 0, 3) {\n",
            "          produce pad_temp.shared {\n",
            "            // attr [iter_var(threadIdx.z, , threadIdx.z)] thread_extent = 32\n",
            "            // attr [iter_var(threadIdx.y, , threadIdx.y)] thread_extent = 7\n",
            "            // attr [iter_var(threadIdx.x, , threadIdx.x)] thread_extent = 1\n",
            "            pad_temp.shared[((threadIdx.z*7) + threadIdx.y)] = tvm_if_then_else((((((1 - ry.outer) <= threadIdx.y) && (threadIdx.y < (8 - ry.outer))) && ((1 - rx.outer) <= blockIdx.x)) && (blockIdx.x < (8 - rx.outer))), data[(((((((((rc.outer*32) + threadIdx.z)*7) + threadIdx.y) + ry.outer)*7) + blockIdx.x) + rx.outer) + -8)], 0.000000f)\n",
            "          }\n",
            "          produce kernel.shared {\n",
            "            // attr [iter_var(threadIdx.z, , threadIdx.z)] thread_extent = 32\n",
            "            // attr [iter_var(threadIdx.y, , threadIdx.y)] thread_extent = 7\n",
            "            // attr [iter_var(threadIdx.x, , threadIdx.x)] thread_extent = 1\n",
            "            for (ax0.ax1.fused.ax2.fused.ax3.fused.inner.inner.inner, 0, 5) {\n",
            "              if (likely(((((threadIdx.y*5) + ax0.ax1.fused.ax2.fused.ax3.fused.inner.inner.inner)/32) < (32 - threadIdx.z)))) {\n",
            "                if (likely(((threadIdx.z*32) < ((1024 - ax0.ax1.fused.ax2.fused.ax3.fused.inner.inner.inner) - (threadIdx.y*5))))) {\n",
            "                  if (likely(((threadIdx.y*5) < (32 - ax0.ax1.fused.ax2.fused.ax3.fused.inner.inner.inner)))) {\n",
            "                    if (likely(((blockIdx.z*32) < ((512 - threadIdx.z) - (((threadIdx.y*5) + ax0.ax1.fused.ax2.fused.ax3.fused.inner.inner.inner)/32))))) {\n",
            "                      kernel.shared[((threadIdx.z*32) + ((threadIdx.y*5) + ax0.ax1.fused.ax2.fused.ax3.fused.inner.inner.inner))] = kernel[(((((((((((blockIdx.z*32) + (((threadIdx.y*5) + ax0.ax1.fused.ax2.fused.ax3.fused.inner.inner.inner)/32)) + threadIdx.z)*16) + rc.outer)*32) + (((threadIdx.y*5) + ax0.ax1.fused.ax2.fused.ax3.fused.inner.inner.inner) % 32))*3) + ry.outer)*3) + rx.outer)]\n",
            "                    }\n",
            "                  }\n",
            "                }\n",
            "              }\n",
            "            }\n",
            "          }\n",
            "          for (rc.inner.outer, 0, 32) {\n",
            "            produce pad_temp.shared.local {\n",
            "              pad_temp.shared.local[0] = pad_temp.shared[((rc.inner.outer*7) + threadIdx.y)]\n",
            "            }\n",
            "            produce kernel.shared.local {\n",
            "              kernel.shared.local[0] = kernel.shared[((threadIdx.z*32) + rc.inner.outer)]\n",
            "            }\n",
            "            compute.local[0] = (compute.local[0] + (pad_temp.shared.local[0]*kernel.shared.local[0]))\n",
            "          }\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "  compute[((((((blockIdx.z*32) + threadIdx.z)*7) + threadIdx.y)*7) + blockIdx.x)] = compute.local[0]\n",
            "}\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRZZDR5wrRA_",
        "colab_type": "text"
      },
      "source": [
        "Compile and run:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jmYLCxlbrRKW",
        "colab_type": "code",
        "outputId": "dd9236c9-546b-4576-fb9f-768dfea05df2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# check correctness\n",
        "a_np = np.random.uniform(size=(N, CI, H, W)).astype(np.float32)\n",
        "w_np = np.random.uniform(size=(CO, CI, KH, KW)).astype(np.float32)\n",
        "c_np = conv2d_nchw_python(a_np, w_np, stride, padding)\n",
        "\n",
        "with tvm.target.create('cuda'):\n",
        "    manual_conv2d = tvm.build(s, [input, kernel, output])\n",
        "    \n",
        "ctx = tvm.gpu()\n",
        "a_tvm = tvm.nd.array(a_np, ctx=ctx)\n",
        "w_tvm = tvm.nd.array(w_np, ctx=ctx)\n",
        "c_tvm = tvm.nd.empty(c_np.shape, ctx=ctx)\n",
        "manual_conv2d(a_tvm, w_tvm, c_tvm)\n",
        "\n",
        "tvm.testing.assert_allclose(c_np, c_tvm.asnumpy(), rtol=1e-2)\n",
        "\n",
        "evaluator = manual_conv2d.time_evaluator(manual_conv2d.entry_name, ctx, number=400)\n",
        "mean = evaluator(a_tvm, w_tvm, c_tvm).mean\n",
        "print(\"Time cost of this operator: %f\" % mean)\n",
        "print(\"GFLOPS:\", (autotvm.task.task.compute_flop(s)/mean)/10e9)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Time cost of this operator: 0.001189\n",
            "GFLOPS: 19.475072378822485\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}