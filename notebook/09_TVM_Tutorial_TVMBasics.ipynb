{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "09_TVM_Tutorial_TVMBasics.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
                "<a href=\"https://colab.research.google.com/github/uwsampl/tutorial/blob/master/notebook/09_TVM_Tutorial_TVMBasics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"

      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8T0-Z4MasbY",
        "colab_type": "text"
      },
      "source": [
        "TVM Basics\n",
        "=============================================\n",
        "**Author**:  `Eddie Yan <https://github.com/eqy>`_\n",
        "\n",
        "This tutorial introduces the basics of declaring and scheduling an operator (2D convolution) in TVM.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9soFF1q_cV-y",
        "colab_type": "text"
      },
      "source": [
        "Please run the following block to ensure TVM is setup for *this notebook*, each notebook may have its own runtime.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G_lzOGuFcUgG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! gsutil cp \"gs://tvm-fcrc-binariesd5fce43e-8373-11e9-bfb6-0242ac1c0002/tvm.tar.gz\" /tmp/tvm.tar.gz\n",
        "! mkdir -p /tvm\n",
        "! tar -xf /tmp/tvm.tar.gz --strip-components=4 --directory /tvm\n",
        "! ls -la /tvm\n",
        "# Move this block after we are done with pkg step\n",
        "! bash /tvm/package.sh\n",
        "import sys\n",
        "sys.path.append('/tvm/python')\n",
        "sys.path.append('/tvm/topi/python')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "agr421w8a49k",
        "colab_type": "text"
      },
      "source": [
        "Import packages:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C9JnukA4aTLJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import tvm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u5YlgPYma_5r",
        "colab_type": "text"
      },
      "source": [
        "Define the convolution shape\n",
        "---------------------------------------------------------------------------------------------\n",
        "\n",
        "We will implement a convolution operator similar to those found in networks like ResNet-18.\n",
        "We begin by defining the shape for the specific convolution we will implement:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DhtW-j3bbMLI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_channels = 64\n",
        "output_channels = 64\n",
        "kernel_size = 3\n",
        "input_height = 56\n",
        "input_width = 56\n",
        "padding = (1,1)\n",
        "\n",
        "output_height = (input_height + 2*padding[0] - kernel_size + 1)\n",
        "output_width = (input_width + 2*padding[1] - kernel_size + 1)\n",
        "\n",
        "# We define the input in H, W, C (height, width, channels) layout\n",
        "input_shape = (input_height+2*padding[0], input_width+2*padding[1], input_channels)\n",
        "# We define the kernel weights in H, W, I, O (kernel height, kernel width, input\n",
        "# channel, output channel) layout\n",
        "weight_shape = (kernel_size, kernel_size, input_channels, output_channels)\n",
        "# Wed define the output in H, W, C (height, width, channels) layout\n",
        "output_shape = (output_height, output_width, output_channels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PYacZkkNd_xG",
        "colab_type": "text"
      },
      "source": [
        "Define the convolution\n",
        "---------------------------------------------------------------------------------------------\n",
        "\n",
        "To define the convolution operator, we first declare placeholders using the shape dimensions we just defined. We then define the reduction axes for the summation that occurs for each output position of the convolution. There are three reduction axes in this case: the vertical spatial axis, the horizontal spatial axis, and the channel axis.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "afD69lgUd-QG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_placeholder = tvm.placeholder(input_shape, name='data')\n",
        "weight_placeholder = tvm.placeholder(weight_shape, name='weight')\n",
        "rc = tvm.reduce_axis((0, input_channels), name='rc')\n",
        "ry = tvm.reduce_axis((0, kernel_size), name='ry')\n",
        "rx = tvm.reduce_axis((0, kernel_size), name='rx')\n",
        "comp = tvm.compute((output_height, output_width, output_channels),\n",
        "    lambda output_y, output_x, output_channel:\n",
        "    tvm.sum(input_placeholder[output_y + ry][output_x + rx][rc]\n",
        "    * weight_placeholder[ry][rx][rc][output_channel],\n",
        "        axis=[ry, rx, rc])\n",
        "    )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iDGX2K6MeWH6",
        "colab_type": "text"
      },
      "source": [
        "Generate the Default Schedule\n",
        "---------------------------------------------------------------------------------------------\n",
        "\n",
        "After declaring the convolution we can use TVM to generate a default schedule for the convolution. The default schedule is the generic multi-level loop nest implementation without any optimizations applied. We also evaluate the performance of the default schedule.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yLaKLAH7eWRH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "s = tvm.create_schedule(comp.op)\n",
        "# print the loop nest (default schedule)\n",
        "print(tvm.lower(s, [input_placeholder, weight_placeholder, comp], simple_mode=True))\n",
        "# compile the function\n",
        "default_func = tvm.build(s, [input_placeholder, weight_placeholder, comp], target='llvm -mcpu=core-avx2', name='conv')\n",
        "# create a time evaluator instance for measuring the run time\n",
        "timer = default_func.time_evaluator(default_func.entry_name, tvm.cpu(0), min_repeat_ms=100)\n",
        "\n",
        "# data arrays for inputs and outputs to the function\n",
        "data = np.random.random(input_shape).astype('float32')\n",
        "weight = np.random.random(weight_shape).astype('float32')\n",
        "data_tvm = tvm.nd.array(data)\n",
        "weight_tvm = tvm.nd.array(weight)\n",
        "output_tvm = tvm.nd.array(np.empty(output_shape).astype('float32'))\n",
        "# time the execution of the function\n",
        "res = timer(data_tvm, weight_tvm, output_tvm)\n",
        "print(\"Conv2D with default schedule finished in:\", res.mean, \"seconds\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RH6gor8XjvXM",
        "colab_type": "text"
      },
      "source": [
        "Scheduling the Convolution\n",
        "---------------------------------------------------------------------------------------------\n",
        "\n",
        "Next, we apply some scheduling primitives to the convolution.\n",
        "A useful transformation for many operators is to reorder the loop axes for better locality.\n",
        "Here, we can grab references to the spatial and reduction axes of the computation and reorder them.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_yIhMYiQjvhQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "yo, xo, co = comp.op.axis\n",
        "ry, rx, rc = s[comp].op.reduce_axis\n",
        "s[comp].reorder(rx, ry, yo, xo, rc, co)\n",
        "\n",
        "# print the loop nest (reordered schedule)\n",
        "print(tvm.lower(s, [input_placeholder, weight_placeholder, comp], simple_mode=True))\n",
        "\n",
        "reordered_func = tvm.build(s, [input_placeholder, weight_placeholder, comp], target='llvm -mcpu=core-avx2', name='conv')\n",
        "timer = reordered_func.time_evaluator(reordered_func.entry_name, tvm.cpu(0), min_repeat_ms=100)\n",
        "res = timer(data_tvm, weight_tvm, output_tvm)\n",
        "print(\"Conv2D with reordered schedule finished in:\", res.mean, \"seconds\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QSicMUUukiu2",
        "colab_type": "text"
      },
      "source": [
        "More schedule transformations\n",
        "---------------------------------------------------\n",
        "Next, we can vectorize a loop with the `vectorize` schedule primitive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VT992Qu6ki6n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# print the loop nest (parallelized and reordered schedule)\n",
        "s[comp].vectorize(co)\n",
        "print(tvm.lower(s, [input_placeholder, weight_placeholder, comp], simple_mode=True))\n",
        "\n",
        "\n",
        "reordered_parallel_func = tvm.build(s, [input_placeholder, weight_placeholder, comp], target='llvm -mcpu=core-avx2', name='conv')\n",
        "timer = reordered_parallel_func.time_evaluator(reordered_parallel_func.entry_name, tvm.cpu(0), min_repeat_ms=100)\n",
        "res = timer(data_tvm, weight_tvm, output_tvm)\n",
        "print(\"Conv2D with reordered and vectorized schedule finished in:\", res.mean, \"seconds\")"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
