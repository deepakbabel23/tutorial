{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SeiRi-zc0NuZ"
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/uwsampl/tutorial/blob/master/notebook/02_TVM_Tutorial_Relay.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BYNJGUSR0Nub"
   },
   "source": [
    "Please run the following block to ensure TVM is setup for *this notebook*, each notebook may have its own runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1020
    },
    "colab_type": "code",
    "id": "bdiA6WVx0Nuc",
    "outputId": "949d0040-80a4-43e5-b1e9-04445c3307c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook executing locally, skipping Colab setup ...\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "  import google.colab\n",
    "  IN_COLAB = True\n",
    "except:\n",
    "  IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "    ! gsutil cp \"gs://tvm-fcrc-binariesd5fce43e-8373-11e9-bfb6-0242ac1c0002/tvm.tar.gz\" /tmp/tvm.tar.gz\n",
    "    ! mkdir -p /tvm\n",
    "    ! tar -xf /tmp/tvm.tar.gz --strip-components=4 --directory /tvm\n",
    "    ! ls -la /tvm\n",
    "    ! bash /tvm/package.sh\n",
    "    # Add TVM to the Python path.\n",
    "    import sys\n",
    "    sys.path.append('/tvm/python')\n",
    "    sys.path.append('/tvm/topi/python')\n",
    "else:\n",
    "    print(\"Notebook executing locally, skipping Colab setup ...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7Jg5xV0v0Nuh"
   },
   "source": [
    "# Relay: an Extensible Deep Learning IR\n",
    "\n",
    "Last year TVM introduced Relay IR â€“ a second generation high-level IR for deep learning. \n",
    "\n",
    "Relay's design comes from a simple insight that the critical difference between regular IRs\n",
    "and deep learning IRs are the primitive values they manipulate. Relay is designed using well\n",
    "known insights from the programming languages community coupled with TVM's existing \n",
    "infrastructure to provide state of the art performance. \n",
    "\n",
    "If you are familiar with ideas from programming languages or existing computation graph\n",
    "representations we will connect Relay to your existing knowledge during this tutorial.\n",
    "\n",
    "We will first cover the design of Relay, then elaborate on how one can use it to \n",
    "accomplish a wide variety of tasks. This piece of the tutorial focused directly \n",
    "on Relay but Relay will be present throughout all of the content today, and serves\n",
    "as the interface layer to TVM.\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tvm\n",
    "from tvm import relay\n",
    "import tvm.relay.testing\n",
    "from tvm.relay.expr_functor import ExprMutator\n",
    "import torch\n",
    "import torchvision\n",
    "import onnx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language \n",
    "\n",
    "We will briefly introduce the concepts of Relay below before showing how to use Relay to accomplish specific tasks.\n",
    "You can find a full language specification [here](https://docs.tvm.ai/langref/index.html).\n",
    "\n",
    "### Variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A single Relay variable, the string is just a hint\n",
    "x = relay.var('x')\n",
    "\n",
    "# A Relay variable with a different dtype, defaults to float32.\n",
    "x = relay.var('x', dtype='int32')\n",
    "\n",
    "# A Relay variable with a different shape.\n",
    "x = relay.var('x', shape=(10, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Operators\n",
    "\n",
    "Relay provides high performance operators defined in TVM that implement the primitive operations needed by deep learning applications. Operators can be applied to arguments just like regular Python or C++ functions. Common arithemetic operations are provided both via names and operator overloading.\n",
    "\n",
    "Variables can be used to construct Relay *expressions* which replace the concept of graphs present in previous frameworks. A Relay expression can be viewed much like a graph with extra functionality as we will see as we go\n",
    "forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v0.0.1\n",
      "free_var %x: Tensor[(10, 1), float32]\n",
      "add(%x, %x)\n"
     ]
    }
   ],
   "source": [
    "w = relay.op.add(x, x)\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v0.0.1\n",
      "free_var %x: Tensor[(10, 1), float32]\n",
      "add(%x, %x)\n"
     ]
    }
   ],
   "source": [
    "z = x + x\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions\n",
    "\n",
    "The fundamental packaging of computation in Relay is the function. A function is a combination of a set of inputs,\n",
    "and a Relay expression. One view is a function is no different than the ones in programming languages today, and another is that it replaces named subgraphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v0.0.1\n",
      "fn (%x: Tensor[(10, 1), float32]) {\n",
      "  add(%x, %x)\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "f = relay.Function([x], z)\n",
    "print(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Module\n",
    "\n",
    "Finally we can give functions a global name and package many of them together into a module. When we add a function to the module, it will be type checked before hand.\n",
    "\n",
    "When we print the module you can see the program annotated with all type information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v0.0.1\n",
      "def @f(%x: Tensor[(10, 1), float32]) -> Tensor[(10, 1), float32] {\n",
      "  add(%x, %x) /* ty=Tensor[(10, 1), float32] */\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mod = relay.Module({})\n",
    "fname = relay.GlobalVar('f')\n",
    "mod[fname] = f\n",
    "\n",
    "print(mod)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frontends\n",
    "\n",
    "Relay comes with a variety of frontends and supports most major frameworks including TensorFlow, PyTorch, MxNet, ONNX, Keras and Caffe2.\n",
    "\n",
    "Below we provide a couple examples of using these frontends to import models into Relay.\n",
    "\n",
    "You can find specific tutorials on deploying pretrained models below:\n",
    "    - [ONNX](https://docs.tvm.ai/tutorials/frontend/from_onnx.html#sphx-glr-tutorials-frontend-from-onnx-py)\n",
    "    - [TensorFlow](https://docs.tvm.ai/tutorials/frontend/from_tensorflow.html#sphx-glr-tutorials-frontend-from-tensorflow-py)\n",
    "    - [Keras](https://docs.tvm.ai/tutorials/frontend/from_keras.html#sphx-glr-tutorials-frontend-from-keras-py)\n",
    "    - [PyTorch](https://tvm.ai/2019/05/30/pytorch-frontend)\n",
    "    - [Caffe2](https://docs.tvm.ai/tutorials/frontend/from_caffe2.html#sphx-glr-tutorials-frontend-from-caffe2-py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph(%0 : Float(10, 3, 224, 224)\n",
      "      %1 : Float(64, 3, 7, 7)\n",
      "      %2 : Float(64)\n",
      "      %3 : Float(64)\n",
      "      %4 : Float(64)\n",
      "      %5 : Float(64)\n",
      "      %6 : Long()\n",
      "      %7 : Float(64, 64, 3, 3)\n",
      "      %8 : Float(64)\n",
      "      %9 : Float(64)\n",
      "      %10 : Float(64)\n",
      "      %11 : Float(64)\n",
      "      %12 : Long()\n",
      "      %13 : Float(64, 64, 3, 3)\n",
      "      %14 : Float(64)\n",
      "      %15 : Float(64)\n",
      "      %16 : Float(64)\n",
      "      %17 : Float(64)\n",
      "      %18 : Long()\n",
      "      %19 : Float(64, 64, 3, 3)\n",
      "      %20 : Float(64)\n",
      "      %21 : Float(64)\n",
      "      %22 : Float(64)\n",
      "      %23 : Float(64)\n",
      "      %24 : Long()\n",
      "      %25 : Float(64, 64, 3, 3)\n",
      "      %26 : Float(64)\n",
      "      %27 : Float(64)\n",
      "      %28 : Float(64)\n",
      "      %29 : Float(64)\n",
      "      %30 : Long()\n",
      "      %31 : Float(128, 64, 3, 3)\n",
      "      %32 : Float(128)\n",
      "      %33 : Float(128)\n",
      "      %34 : Float(128)\n",
      "      %35 : Float(128)\n",
      "      %36 : Long()\n",
      "      %37 : Float(128, 128, 3, 3)\n",
      "      %38 : Float(128)\n",
      "      %39 : Float(128)\n",
      "      %40 : Float(128)\n",
      "      %41 : Float(128)\n",
      "      %42 : Long()\n",
      "      %43 : Float(128, 64, 1, 1)\n",
      "      %44 : Float(128)\n",
      "      %45 : Float(128)\n",
      "      %46 : Float(128)\n",
      "      %47 : Float(128)\n",
      "      %48 : Long()\n",
      "      %49 : Float(128, 128, 3, 3)\n",
      "      %50 : Float(128)\n",
      "      %51 : Float(128)\n",
      "      %52 : Float(128)\n",
      "      %53 : Float(128)\n",
      "      %54 : Long()\n",
      "      %55 : Float(128, 128, 3, 3)\n",
      "      %56 : Float(128)\n",
      "      %57 : Float(128)\n",
      "      %58 : Float(128)\n",
      "      %59 : Float(128)\n",
      "      %60 : Long()\n",
      "      %61 : Float(256, 128, 3, 3)\n",
      "      %62 : Float(256)\n",
      "      %63 : Float(256)\n",
      "      %64 : Float(256)\n",
      "      %65 : Float(256)\n",
      "      %66 : Long()\n",
      "      %67 : Float(256, 256, 3, 3)\n",
      "      %68 : Float(256)\n",
      "      %69 : Float(256)\n",
      "      %70 : Float(256)\n",
      "      %71 : Float(256)\n",
      "      %72 : Long()\n",
      "      %73 : Float(256, 128, 1, 1)\n",
      "      %74 : Float(256)\n",
      "      %75 : Float(256)\n",
      "      %76 : Float(256)\n",
      "      %77 : Float(256)\n",
      "      %78 : Long()\n",
      "      %79 : Float(256, 256, 3, 3)\n",
      "      %80 : Float(256)\n",
      "      %81 : Float(256)\n",
      "      %82 : Float(256)\n",
      "      %83 : Float(256)\n",
      "      %84 : Long()\n",
      "      %85 : Float(256, 256, 3, 3)\n",
      "      %86 : Float(256)\n",
      "      %87 : Float(256)\n",
      "      %88 : Float(256)\n",
      "      %89 : Float(256)\n",
      "      %90 : Long()\n",
      "      %91 : Float(512, 256, 3, 3)\n",
      "      %92 : Float(512)\n",
      "      %93 : Float(512)\n",
      "      %94 : Float(512)\n",
      "      %95 : Float(512)\n",
      "      %96 : Long()\n",
      "      %97 : Float(512, 512, 3, 3)\n",
      "      %98 : Float(512)\n",
      "      %99 : Float(512)\n",
      "      %100 : Float(512)\n",
      "      %101 : Float(512)\n",
      "      %102 : Long()\n",
      "      %103 : Float(512, 256, 1, 1)\n",
      "      %104 : Float(512)\n",
      "      %105 : Float(512)\n",
      "      %106 : Float(512)\n",
      "      %107 : Float(512)\n",
      "      %108 : Long()\n",
      "      %109 : Float(512, 512, 3, 3)\n",
      "      %110 : Float(512)\n",
      "      %111 : Float(512)\n",
      "      %112 : Float(512)\n",
      "      %113 : Float(512)\n",
      "      %114 : Long()\n",
      "      %115 : Float(512, 512, 3, 3)\n",
      "      %116 : Float(512)\n",
      "      %117 : Float(512)\n",
      "      %118 : Float(512)\n",
      "      %119 : Float(512)\n",
      "      %120 : Long()\n",
      "      %121 : Float(1000, 512)\n",
      "      %122 : Float(1000)) {\n",
      "  %123 : Float(10, 64, 112, 112) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[7, 7], pads=[3, 3, 3, 3], strides=[2, 2]](%0, %1), scope: ResNet/Conv2d[conv1]\n",
      "  %124 : Float(10, 64, 112, 112) = onnx::BatchNormalization[epsilon=1e-05, momentum=1](%123, %2, %3, %4, %5), scope: ResNet/BatchNorm2d[bn1]\n",
      "  %125 : Float(10, 64, 112, 112) = onnx::Relu(%124), scope: ResNet/ReLU[relu]\n",
      "  %126 : Float(10, 64, 56, 56) = onnx::MaxPool[kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[2, 2]](%125), scope: ResNet/MaxPool2d[maxpool]\n",
      "  %127 : Float(10, 64, 56, 56) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%126, %7), scope: ResNet/Sequential[layer1]/BasicBlock[0]/Conv2d[conv1]\n",
      "  %128 : Float(10, 64, 56, 56) = onnx::BatchNormalization[epsilon=1e-05, momentum=1](%127, %8, %9, %10, %11), scope: ResNet/Sequential[layer1]/BasicBlock[0]/BatchNorm2d[bn1]\n",
      "  %129 : Float(10, 64, 56, 56) = onnx::Relu(%128), scope: ResNet/Sequential[layer1]/BasicBlock[0]/ReLU[relu]\n",
      "  %130 : Float(10, 64, 56, 56) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%129, %13), scope: ResNet/Sequential[layer1]/BasicBlock[0]/Conv2d[conv2]\n",
      "  %131 : Float(10, 64, 56, 56) = onnx::BatchNormalization[epsilon=1e-05, momentum=1](%130, %14, %15, %16, %17), scope: ResNet/Sequential[layer1]/BasicBlock[0]/BatchNorm2d[bn2]\n",
      "  %132 : Float(10, 64, 56, 56) = onnx::Add(%131, %126), scope: ResNet/Sequential[layer1]/BasicBlock[0]\n",
      "  %133 : Float(10, 64, 56, 56) = onnx::Relu(%132), scope: ResNet/Sequential[layer1]/BasicBlock[0]/ReLU[relu]\n",
      "  %134 : Float(10, 64, 56, 56) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%133, %19), scope: ResNet/Sequential[layer1]/BasicBlock[1]/Conv2d[conv1]\n",
      "  %135 : Float(10, 64, 56, 56) = onnx::BatchNormalization[epsilon=1e-05, momentum=1](%134, %20, %21, %22, %23), scope: ResNet/Sequential[layer1]/BasicBlock[1]/BatchNorm2d[bn1]\n",
      "  %136 : Float(10, 64, 56, 56) = onnx::Relu(%135), scope: ResNet/Sequential[layer1]/BasicBlock[1]/ReLU[relu]\n",
      "  %137 : Float(10, 64, 56, 56) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%136, %25), scope: ResNet/Sequential[layer1]/BasicBlock[1]/Conv2d[conv2]\n",
      "  %138 : Float(10, 64, 56, 56) = onnx::BatchNormalization[epsilon=1e-05, momentum=1](%137, %26, %27, %28, %29), scope: ResNet/Sequential[layer1]/BasicBlock[1]/BatchNorm2d[bn2]\n",
      "  %139 : Float(10, 64, 56, 56) = onnx::Add(%138, %133), scope: ResNet/Sequential[layer1]/BasicBlock[1]\n",
      "  %140 : Float(10, 64, 56, 56) = onnx::Relu(%139), scope: ResNet/Sequential[layer1]/BasicBlock[1]/ReLU[relu]\n",
      "  %141 : Float(10, 128, 28, 28) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[2, 2]](%140, %31), scope: ResNet/Sequential[layer2]/BasicBlock[0]/Conv2d[conv1]\n",
      "  %142 : Float(10, 128, 28, 28) = onnx::BatchNormalization[epsilon=1e-05, momentum=1](%141, %32, %33, %34, %35), scope: ResNet/Sequential[layer2]/BasicBlock[0]/BatchNorm2d[bn1]\n",
      "  %143 : Float(10, 128, 28, 28) = onnx::Relu(%142), scope: ResNet/Sequential[layer2]/BasicBlock[0]/ReLU[relu]\n",
      "  %144 : Float(10, 128, 28, 28) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%143, %37), scope: ResNet/Sequential[layer2]/BasicBlock[0]/Conv2d[conv2]\n",
      "  %145 : Float(10, 128, 28, 28) = onnx::BatchNormalization[epsilon=1e-05, momentum=1](%144, %38, %39, %40, %41), scope: ResNet/Sequential[layer2]/BasicBlock[0]/BatchNorm2d[bn2]\n",
      "  %146 : Float(10, 128, 28, 28) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[2, 2]](%140, %43), scope: ResNet/Sequential[layer2]/BasicBlock[0]/Sequential[downsample]/Conv2d[0]\n",
      "  %147 : Float(10, 128, 28, 28) = onnx::BatchNormalization[epsilon=1e-05, momentum=1](%146, %44, %45, %46, %47), scope: ResNet/Sequential[layer2]/BasicBlock[0]/Sequential[downsample]/BatchNorm2d[1]\n",
      "  %148 : Float(10, 128, 28, 28) = onnx::Add(%145, %147), scope: ResNet/Sequential[layer2]/BasicBlock[0]\n",
      "  %149 : Float(10, 128, 28, 28) = onnx::Relu(%148), scope: ResNet/Sequential[layer2]/BasicBlock[0]/ReLU[relu]\n",
      "  %150 : Float(10, 128, 28, 28) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%149, %49), scope: ResNet/Sequential[layer2]/BasicBlock[1]/Conv2d[conv1]\n",
      "  %151 : Float(10, 128, 28, 28) = onnx::BatchNormalization[epsilon=1e-05, momentum=1](%150, %50, %51, %52, %53), scope: ResNet/Sequential[layer2]/BasicBlock[1]/BatchNorm2d[bn1]\n",
      "  %152 : Float(10, 128, 28, 28) = onnx::Relu(%151), scope: ResNet/Sequential[layer2]/BasicBlock[1]/ReLU[relu]\n",
      "  %153 : Float(10, 128, 28, 28) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%152, %55), scope: ResNet/Sequential[layer2]/BasicBlock[1]/Conv2d[conv2]\n",
      "  %154 : Float(10, 128, 28, 28) = onnx::BatchNormalization[epsilon=1e-05, momentum=1](%153, %56, %57, %58, %59), scope: ResNet/Sequential[layer2]/BasicBlock[1]/BatchNorm2d[bn2]\n",
      "  %155 : Float(10, 128, 28, 28) = onnx::Add(%154, %149), scope: ResNet/Sequential[layer2]/BasicBlock[1]\n",
      "  %156 : Float(10, 128, 28, 28) = onnx::Relu(%155), scope: ResNet/Sequential[layer2]/BasicBlock[1]/ReLU[relu]\n",
      "  %157 : Float(10, 256, 14, 14) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[2, 2]](%156, %61), scope: ResNet/Sequential[layer3]/BasicBlock[0]/Conv2d[conv1]\n",
      "  %158 : Float(10, 256, 14, 14) = onnx::BatchNormalization[epsilon=1e-05, momentum=1](%157, %62, %63, %64, %65), scope: ResNet/Sequential[layer3]/BasicBlock[0]/BatchNorm2d[bn1]\n",
      "  %159 : Float(10, 256, 14, 14) = onnx::Relu(%158), scope: ResNet/Sequential[layer3]/BasicBlock[0]/ReLU[relu]\n",
      "  %160 : Float(10, 256, 14, 14) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%159, %67), scope: ResNet/Sequential[layer3]/BasicBlock[0]/Conv2d[conv2]\n",
      "  %161 : Float(10, 256, 14, 14) = onnx::BatchNormalization[epsilon=1e-05, momentum=1](%160, %68, %69, %70, %71), scope: ResNet/Sequential[layer3]/BasicBlock[0]/BatchNorm2d[bn2]\n",
      "  %162 : Float(10, 256, 14, 14) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[2, 2]](%156, %73), scope: ResNet/Sequential[layer3]/BasicBlock[0]/Sequential[downsample]/Conv2d[0]\n",
      "  %163 : Float(10, 256, 14, 14) = onnx::BatchNormalization[epsilon=1e-05, momentum=1](%162, %74, %75, %76, %77), scope: ResNet/Sequential[layer3]/BasicBlock[0]/Sequential[downsample]/BatchNorm2d[1]\n",
      "  %164 : Float(10, 256, 14, 14) = onnx::Add(%161, %163), scope: ResNet/Sequential[layer3]/BasicBlock[0]\n",
      "  %165 : Float(10, 256, 14, 14) = onnx::Relu(%164), scope: ResNet/Sequential[layer3]/BasicBlock[0]/ReLU[relu]\n",
      "  %166 : Float(10, 256, 14, 14) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%165, %79), scope: ResNet/Sequential[layer3]/BasicBlock[1]/Conv2d[conv1]\n",
      "  %167 : Float(10, 256, 14, 14) = onnx::BatchNormalization[epsilon=1e-05, momentum=1](%166, %80, %81, %82, %83), scope: ResNet/Sequential[layer3]/BasicBlock[1]/BatchNorm2d[bn1]\n",
      "  %168 : Float(10, 256, 14, 14) = onnx::Relu(%167), scope: ResNet/Sequential[layer3]/BasicBlock[1]/ReLU[relu]\n",
      "  %169 : Float(10, 256, 14, 14) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%168, %85), scope: ResNet/Sequential[layer3]/BasicBlock[1]/Conv2d[conv2]\n",
      "  %170 : Float(10, 256, 14, 14) = onnx::BatchNormalization[epsilon=1e-05, momentum=1](%169, %86, %87, %88, %89), scope: ResNet/Sequential[layer3]/BasicBlock[1]/BatchNorm2d[bn2]\n",
      "  %171 : Float(10, 256, 14, 14) = onnx::Add(%170, %165), scope: ResNet/Sequential[layer3]/BasicBlock[1]\n",
      "  %172 : Float(10, 256, 14, 14) = onnx::Relu(%171), scope: ResNet/Sequential[layer3]/BasicBlock[1]/ReLU[relu]\n",
      "  %173 : Float(10, 512, 7, 7) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[2, 2]](%172, %91), scope: ResNet/Sequential[layer4]/BasicBlock[0]/Conv2d[conv1]\n",
      "  %174 : Float(10, 512, 7, 7) = onnx::BatchNormalization[epsilon=1e-05, momentum=1](%173, %92, %93, %94, %95), scope: ResNet/Sequential[layer4]/BasicBlock[0]/BatchNorm2d[bn1]\n",
      "  %175 : Float(10, 512, 7, 7) = onnx::Relu(%174), scope: ResNet/Sequential[layer4]/BasicBlock[0]/ReLU[relu]\n",
      "  %176 : Float(10, 512, 7, 7) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%175, %97), scope: ResNet/Sequential[layer4]/BasicBlock[0]/Conv2d[conv2]\n",
      "  %177 : Float(10, 512, 7, 7) = onnx::BatchNormalization[epsilon=1e-05, momentum=1](%176, %98, %99, %100, %101), scope: ResNet/Sequential[layer4]/BasicBlock[0]/BatchNorm2d[bn2]\n",
      "  %178 : Float(10, 512, 7, 7) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[2, 2]](%172, %103), scope: ResNet/Sequential[layer4]/BasicBlock[0]/Sequential[downsample]/Conv2d[0]\n",
      "  %179 : Float(10, 512, 7, 7) = onnx::BatchNormalization[epsilon=1e-05, momentum=1](%178, %104, %105, %106, %107), scope: ResNet/Sequential[layer4]/BasicBlock[0]/Sequential[downsample]/BatchNorm2d[1]\n",
      "  %180 : Float(10, 512, 7, 7) = onnx::Add(%177, %179), scope: ResNet/Sequential[layer4]/BasicBlock[0]\n",
      "  %181 : Float(10, 512, 7, 7) = onnx::Relu(%180), scope: ResNet/Sequential[layer4]/BasicBlock[0]/ReLU[relu]\n",
      "  %182 : Float(10, 512, 7, 7) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%181, %109), scope: ResNet/Sequential[layer4]/BasicBlock[1]/Conv2d[conv1]\n",
      "  %183 : Float(10, 512, 7, 7) = onnx::BatchNormalization[epsilon=1e-05, momentum=1](%182, %110, %111, %112, %113), scope: ResNet/Sequential[layer4]/BasicBlock[1]/BatchNorm2d[bn1]\n",
      "  %184 : Float(10, 512, 7, 7) = onnx::Relu(%183), scope: ResNet/Sequential[layer4]/BasicBlock[1]/ReLU[relu]\n",
      "  %185 : Float(10, 512, 7, 7) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%184, %115), scope: ResNet/Sequential[layer4]/BasicBlock[1]/Conv2d[conv2]\n",
      "  %186 : Float(10, 512, 7, 7) = onnx::BatchNormalization[epsilon=1e-05, momentum=1](%185, %116, %117, %118, %119), scope: ResNet/Sequential[layer4]/BasicBlock[1]/BatchNorm2d[bn2]\n",
      "  %187 : Float(10, 512, 7, 7) = onnx::Add(%186, %181), scope: ResNet/Sequential[layer4]/BasicBlock[1]\n",
      "  %188 : Float(10, 512, 7, 7) = onnx::Relu(%187), scope: ResNet/Sequential[layer4]/BasicBlock[1]/ReLU[relu]\n",
      "  %189 : Float(10, 512, 1, 1) = onnx::GlobalAveragePool(%188), scope: ResNet/AdaptiveAvgPool2d[avgpool]\n",
      "  %190 : Long() = onnx::Constant[value={0}](), scope: ResNet\n",
      "  %191 : Tensor = onnx::Shape(%189), scope: ResNet\n",
      "  %192 : Long() = onnx::Gather[axis=0](%191, %190), scope: ResNet\n",
      "  %193 : Long() = onnx::Constant[value={-1}](), scope: ResNet\n",
      "  %194 : Tensor = onnx::Unsqueeze[axes=[0]](%192)\n",
      "  %195 : Tensor = onnx::Unsqueeze[axes=[0]](%193)\n",
      "  %196 : Tensor = onnx::Concat[axis=0](%194, %195)\n",
      "  %197 : Float(10, 512) = onnx::Reshape(%189, %196), scope: ResNet\n",
      "  %198 : Float(10, 1000) = onnx::Gemm[alpha=1, beta=1, transB=1](%197, %121, %122), scope: ResNet/Linear[fc]\n",
      "  return (%198);\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Attribute momentum is ignored in relay.sym.batch_norm\n",
      "WARNING:root:Attribute momentum is ignored in relay.sym.batch_norm\n",
      "WARNING:root:Attribute momentum is ignored in relay.sym.batch_norm\n",
      "WARNING:root:Attribute momentum is ignored in relay.sym.batch_norm\n",
      "WARNING:root:Attribute momentum is ignored in relay.sym.batch_norm\n",
      "WARNING:root:Attribute momentum is ignored in relay.sym.batch_norm\n",
      "WARNING:root:Attribute momentum is ignored in relay.sym.batch_norm\n",
      "WARNING:root:Attribute momentum is ignored in relay.sym.batch_norm\n",
      "WARNING:root:Attribute momentum is ignored in relay.sym.batch_norm\n",
      "WARNING:root:Attribute momentum is ignored in relay.sym.batch_norm\n",
      "WARNING:root:Attribute momentum is ignored in relay.sym.batch_norm\n",
      "WARNING:root:Attribute momentum is ignored in relay.sym.batch_norm\n",
      "WARNING:root:Attribute momentum is ignored in relay.sym.batch_norm\n",
      "WARNING:root:Attribute momentum is ignored in relay.sym.batch_norm\n",
      "WARNING:root:Attribute momentum is ignored in relay.sym.batch_norm\n",
      "WARNING:root:Attribute momentum is ignored in relay.sym.batch_norm\n",
      "WARNING:root:Attribute momentum is ignored in relay.sym.batch_norm\n",
      "WARNING:root:Attribute momentum is ignored in relay.sym.batch_norm\n",
      "WARNING:root:Attribute momentum is ignored in relay.sym.batch_norm\n",
      "WARNING:root:Attribute momentum is ignored in relay.sym.batch_norm\n",
      "WARNING:root:Infering Reshape argument by precompute\n"
     ]
    },
    {
     "ename": "TVMError",
     "evalue": "Traceback (most recent call last):\n  [bt] (8) 9   libtvm.dylib                        0x0000000115285fdd tvm::relay::backend::RelayBuildModule::GetFunction(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::shared_ptr<tvm::runtime::ModuleNode> const&)::'lambda1'(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const + 429\n  [bt] (7) 8   libtvm.dylib                        0x00000001152861f2 tvm::relay::backend::RelayBuildModule::Build(tvm::relay::Function, tvm::Map<tvm::Integer, tvm::Target, void, void> const&, tvm::Target const&) + 130\n  [bt] (6) 7   libtvm.dylib                        0x00000001152863f9 tvm::relay::backend::RelayBuildModule::BuildRelay(tvm::relay::Function, std::__1::unordered_map<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, tvm::runtime::NDArray, std::__1::hash<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > >, std::__1::equal_to<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > >, std::__1::allocator<std::__1::pair<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const, tvm::runtime::NDArray> > > const&) + 345\n  [bt] (5) 6   libtvm.dylib                        0x00000001153542ca tvm::relay::ModuleNode::FromExpr(tvm::relay::Expr const&, tvm::Map<tvm::relay::GlobalVar, tvm::relay::Function, void, void> const&) + 938\n  [bt] (4) 5   libtvm.dylib                        0x0000000115352b17 tvm::relay::ModuleNode::Add(tvm::relay::GlobalVar const&, tvm::relay::Function const&, bool) + 151\n  [bt] (3) 4   libtvm.dylib                        0x00000001156315b8 tvm::relay::InferType(tvm::relay::Function const&, tvm::relay::Module const&, tvm::relay::GlobalVar const&) + 472\n  [bt] (2) 3   libtvm.dylib                        0x0000000115630687 tvm::relay::TypeInferencer::Infer(tvm::relay::Expr) + 135\n  [bt] (1) 2   libtvm.dylib                        0x000000011531ee23 tvm::relay::ErrorReporter::RenderErrors(tvm::relay::Module const&, bool) + 5555\n  [bt] (0) 1   libtvm.dylib                        0x0000000114ed5949 dmlc::LogMessageFatal::~LogMessageFatal() + 57\n  [bt] (8) 9   libtvm.dylib                        0x0000000115352b17 tvm::relay::ModuleNode::Add(tvm::relay::GlobalVar const&, tvm::relay::Function const&, bool) + 151\n  [bt] (7) 8   libtvm.dylib                        0x00000001156315b8 tvm::relay::InferType(tvm::relay::Function const&, tvm::relay::Module const&, tvm::relay::GlobalVar const&) + 472\n  [bt] (6) 7   libtvm.dylib                        0x000000011563066b tvm::relay::TypeInferencer::Infer(tvm::relay::Expr) + 107\n  [bt] (5) 6   libtvm.dylib                        0x000000011564d05a tvm::relay::TypeSolver::Solve() + 1114\n  [bt] (4) 5   libtvm.dylib                        0x000000011564d6f8 tvm::TypedEnvFunc<bool (tvm::Array<tvm::relay::Type, void> const&, int, tvm::Attrs const&, tvm::relay::TypeReporter const&)>::operator()(tvm::Array<tvm::relay::Type, void> const&, int, tvm::Attrs const&, tvm::relay::TypeReporter const&) const + 328\n  [bt] (3) 4   libtvm.dylib                        0x00000001153a6ba9 std::__1::__function::__func<void tvm::runtime::TypedPackedFunc<bool (tvm::Array<tvm::relay::Type, void> const&, int, tvm::Attrs const&, tvm::relay::TypeReporter const&)>::AssignTypedLambda<bool (*)(tvm::Array<tvm::relay::Type, void> const&, int, tvm::Attrs const&, tvm::relay::TypeReporter const&)>(bool (*)(tvm::Array<tvm::relay::Type, void> const&, int, tvm::Attrs const&, tvm::relay::TypeReporter const&))::'lambda'(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*), std::__1::allocator<void tvm::runtime::TypedPackedFunc<bool (tvm::Array<tvm::relay::Type, void> const&, int, tvm::Attrs const&, tvm::relay::TypeReporter const&)>::AssignTypedLambda<bool (*)(tvm::Array<tvm::relay::Type, void> const&, int, tvm::Attrs const&, tvm::relay::TypeReporter const&)>(bool (*)(tvm::Array<tvm::relay::Type, void> const&, int, tvm::Attrs const&, tvm::relay::TypeReporter const&))::'lambda'(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)>, void (tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)>::operator()(tvm::runtime::TVMArgs&&, tvm::runtime::TVMRetValue*&&) + 137\n  [bt] (2) 3   libtvm.dylib                        0x00000001153a6c4f void tvm::runtime::detail::unpack_call_dispatcher<bool, 0, 4, bool (*)(tvm::Array<tvm::relay::Type, void> const&, int, tvm::Attrs const&, tvm::relay::TypeReporter const&)>::run<tvm::runtime::TVMArgValue, tvm::runtime::TVMArgValue, tvm::runtime::TVMArgValue, tvm::runtime::TVMArgValue>(bool (* const&)(tvm::Array<tvm::relay::Type, void> const&, int, tvm::Attrs const&, tvm::relay::TypeReporter const&), tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*, tvm::runtime::TVMArgValue&&, tvm::runtime::TVMArgValue&&, tvm::runtime::TVMArgValue&&, tvm::runtime::TVMArgValue&&) + 95\n  [bt] (1) 2   libtvm.dylib                        0x00000001154b479e tvm::relay::ConcatenateRel(tvm::Array<tvm::relay::Type, void> const&, int, tvm::Attrs const&, tvm::relay::TypeReporter const&) + 1918\n  [bt] (0) 1   libtvm.dylib                        0x0000000114ed5949 dmlc::LogMessageFatal::~LogMessageFatal() + 57\n  File \"/Users/jroesch/Git/tvm/src/relay/ir/error.cc\", line 132\nTVMError: \u001b[1m\nError(s) have occurred. We have annotated the program with them:\n\n\u001b[0m\u001b[1mIn `main`: \n\u001b[0mv0.0.1\nfn (%v0: Tensor[(10, 3, 224, 224), float32]) {\n  %0 = nn.conv2d(%v0, meta[relay.Constant][0], strides=[2, 2], padding=[3, 3], kernel_size=[7, 7])\n  %1 = nn.batch_norm(%0, meta[relay.Constant][1], meta[relay.Constant][2], meta[relay.Constant][3], meta[relay.Constant][4], epsilon=1e-05)\n  %2 = %1.0\n  %3 = nn.relu(%2)\n  %4 = nn.max_pool2d(%3, pool_size=[3, 3], strides=[2, 2], padding=[1, 1])\n  %5 = nn.conv2d(%4, meta[relay.Constant][5], padding=[1, 1], kernel_size=[3, 3])\n  %6 = nn.batch_norm(%5, meta[relay.Constant][6], meta[relay.Constant][7], meta[relay.Constant][8], meta[relay.Constant][9], epsilon=1e-05)\n  %7 = %6.0\n  %8 = nn.relu(%7)\n  %9 = nn.conv2d(%8, meta[relay.Constant][10], padding=[1, 1], kernel_size=[3, 3])\n  %10 = nn.batch_norm(%9, meta[relay.Constant][11], meta[relay.Constant][12], meta[relay.Constant][13], meta[relay.Constant][14], epsilon=1e-05)\n  %11 = %10.0\n  %12 = add(%11, %4)\n  %13 = nn.relu(%12)\n  %14 = nn.conv2d(%13, meta[relay.Constant][15], padding=[1, 1], kernel_size=[3, 3])\n  %15 = nn.batch_norm(%14, meta[relay.Constant][16], meta[relay.Constant][17], meta[relay.Constant][18], meta[relay.Constant][19], epsilon=1e-05)\n  %16 = %15.0\n  %17 = nn.relu(%16)\n  %18 = nn.conv2d(%17, meta[relay.Constant][20], padding=[1, 1], kernel_size=[3, 3])\n  %19 = nn.batch_norm(%18, meta[relay.Constant][21], meta[relay.Constant][22], meta[relay.Constant][23], meta[relay.Constant][24], epsilon=1e-05)\n  %20 = %19.0\n  %21 = add(%20, %13)\n  %22 = nn.relu(%21)\n  %23 = nn.conv2d(%22, meta[relay.Constant][25], strides=[2, 2], padding=[1, 1], kernel_size=[3, 3])\n  %24 = nn.batch_norm(%23, meta[relay.Constant][26], meta[relay.Constant][27], meta[relay.Constant][28], meta[relay.Constant][29], epsilon=1e-05)\n  %25 = %24.0\n  %26 = nn.relu(%25)\n  %27 = nn.conv2d(%26, meta[relay.Constant][30], padding=[1, 1], kernel_size=[3, 3])\n  %28 = nn.batch_norm(%27, meta[relay.Constant][31], meta[relay.Constant][32], meta[relay.Constant][33], meta[relay.Constant][34], epsilon=1e-05)\n  %29 = %28.0\n  %30 = nn.conv2d(%22, meta[relay.Constant][35], strides=[2, 2], kernel_size=[1, 1])\n  %31 = nn.batch_norm(%30, meta[relay.Constant][36], meta[relay.Constant][37], meta[relay.Constant][38], meta[relay.Constant][39], epsilon=1e-05)\n  %32 = %31.0\n  %33 = add(%29, %32)\n  %34 = nn.relu(%33)\n  %35 = nn.conv2d(%34, meta[relay.Constant][40], padding=[1, 1], kernel_size=[3, 3])\n  %36 = nn.batch_norm(%35, meta[relay.Constant][41], meta[relay.Constant][42], meta[relay.Constant][43], meta[relay.Constant][44], epsilon=1e-05)\n  %37 = %36.0\n  %38 = nn.relu(%37)\n  %39 = nn.conv2d(%38, meta[relay.Constant][45], padding=[1, 1], kernel_size=[3, 3])\n  %40 = nn.batch_norm(%39, meta[relay.Constant][46], meta[relay.Constant][47], meta[relay.Constant][48], meta[relay.Constant][49], epsilon=1e-05)\n  %41 = %40.0\n  %42 = add(%41, %34)\n  %43 = nn.relu(%42)\n  %44 = nn.conv2d(%43, meta[relay.Constant][50], strides=[2, 2], padding=[1, 1], kernel_size=[3, 3])\n  %45 = nn.batch_norm(%44, meta[relay.Constant][51], meta[relay.Constant][52], meta[relay.Constant][53], meta[relay.Constant][54], epsilon=1e-05)\n  %46 = %45.0\n  %47 = nn.relu(%46)\n  %48 = nn.conv2d(%47, meta[relay.Constant][55], padding=[1, 1], kernel_size=[3, 3])\n  %49 = nn.batch_norm(%48, meta[relay.Constant][56], meta[relay.Constant][57], meta[relay.Constant][58], meta[relay.Constant][59], epsilon=1e-05)\n  %50 = %49.0\n  %51 = nn.conv2d(%43, meta[relay.Constant][60], strides=[2, 2], kernel_size=[1, 1])\n  %52 = nn.batch_norm(%51, meta[relay.Constant][61], meta[relay.Constant][62], meta[relay.Constant][63], meta[relay.Constant][64], epsilon=1e-05)\n  %53 = %52.0\n  %54 = add(%50, %53)\n  %55 = nn.relu(%54)\n  %56 = nn.conv2d(%55, meta[relay.Constant][65], padding=[1, 1], kernel_size=[3, 3])\n  %57 = nn.batch_norm(%56, meta[relay.Constant][66], meta[relay.Constant][67], meta[relay.Constant][68], meta[relay.Constant][69], epsilon=1e-05)\n  %58 = %57.0\n  %59 = nn.relu(%58)\n  %60 = nn.conv2d(%59, meta[relay.Constant][70], padding=[1, 1], kernel_size=[3, 3])\n  %61 = nn.batch_norm(%60, meta[relay.Constant][71], meta[relay.Constant][72], meta[relay.Constant][73], meta[relay.Constant][74], epsilon=1e-05)\n  %62 = %61.0\n  %63 = add(%62, %55)\n  %64 = nn.relu(%63)\n  %65 = nn.conv2d(%64, meta[relay.Constant][75], strides=[2, 2], padding=[1, 1], kernel_size=[3, 3])\n  %66 = nn.batch_norm(%65, meta[relay.Constant][76], meta[relay.Constant][77], meta[relay.Constant][78], meta[relay.Constant][79], epsilon=1e-05)\n  %67 = %66.0\n  %68 = nn.relu(%67)\n  %69 = nn.conv2d(%68, meta[relay.Constant][80], padding=[1, 1], kernel_size=[3, 3])\n  %70 = nn.batch_norm(%69, meta[relay.Constant][81], meta[relay.Constant][82], meta[relay.Constant][83], meta[relay.Constant][84], epsilon=1e-05)\n  %71 = %70.0\n  %72 = nn.conv2d(%64, meta[relay.Constant][85], strides=[2, 2], kernel_size=[1, 1])\n  %73 = nn.batch_norm(%72, meta[relay.Constant][86], meta[relay.Constant][87], meta[relay.Constant][88], meta[relay.Constant][89], epsilon=1e-05)\n  %74 = %73.0\n  %75 = add(%71, %74)\n  %76 = nn.relu(%75)\n  %77 = nn.conv2d(%76, meta[relay.Constant][90], padding=[1, 1], kernel_size=[3, 3])\n  %78 = nn.batch_norm(%77, meta[relay.Constant][91], meta[relay.Constant][92], meta[relay.Constant][93], meta[relay.Constant][94], epsilon=1e-05)\n  %79 = %78.0\n  %80 = nn.relu(%79)\n  %81 = nn.conv2d(%80, meta[relay.Constant][95], padding=[1, 1], kernel_size=[3, 3])\n  %82 = nn.batch_norm(%81, meta[relay.Constant][96], meta[relay.Constant][97], meta[relay.Constant][98], meta[relay.Constant][99], epsilon=1e-05)\n  %83 = %82.0\n  %84 = add(%83, %76)\n  %85 = nn.relu(%84)\n  %86 = nn.global_avg_pool2d(%85)\n  %87 = shape_of(%86, dtype=\"int32\")\n  %88 = take(%87, int64(0), axis=0)\n  %89 = expand_dims(%88, axis=0)\n  %90 = expand_dims(int64(-1), axis=0)\n  %91 = (%89, %90)\n  concatenate(%91)\u001b[31man internal invariant was violated while typechecking your program [14:25:05] /Users/jroesch/Git/tvm/src/relay/op/tensor/transform.cc:204: Check failed: e_dtype == dtype (int64 vs. int32) : relay.concatenate requires all tensors have the same dtype\n; \u001b[39m\n}\n// meta data omitted. you can use show_meta_data=True to include meta data\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTVMError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-66-bf4b6f9be8c0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0monnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexport\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch_resnet18\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdummy_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"resnet.onnx\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0monnx_resnet18\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0monnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'resnet.onnx'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrelay\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrontend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_onnx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0monnx_resnet18\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m \u001b[0;34m'0'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m224\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Git/tvm/python/tvm/relay/frontend/onnx.py\u001b[0m in \u001b[0;36mfrom_onnx\u001b[0;34m(model, shape, dtype)\u001b[0m\n\u001b[1;32m   1244\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1245\u001b[0m         \u001b[0mopset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1246\u001b[0;31m     \u001b[0msym\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_onnx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1247\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msym\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Git/tvm/python/tvm/relay/frontend/onnx.py\u001b[0m in \u001b[0;36mfrom_onnx\u001b[0;34m(self, graph, opset)\u001b[0m\n\u001b[1;32m   1072\u001b[0m                 \u001b[0mattr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tvm_custom'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'name'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1073\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1074\u001b[0;31m                 \u001b[0mop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_operator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1075\u001b[0m                 \u001b[0mnode_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fix_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1076\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_expr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTupleWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Git/tvm/python/tvm/relay/frontend/onnx.py\u001b[0m in \u001b[0;36m_convert_operator\u001b[0;34m(self, op_name, inputs, attrs, opset)\u001b[0m\n\u001b[1;32m   1178\u001b[0m             \u001b[0msym\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_relay_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mop_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconvert_map\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0msym\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mop_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m             raise NotImplementedError(\n",
      "\u001b[0;32m~/Git/tvm/python/tvm/relay/frontend/onnx.py\u001b[0m in \u001b[0;36m_impl_v1\u001b[0;34m(cls, inputs, attr, params)\u001b[0m\n\u001b[1;32m    415\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_expr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mir_pass\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfree_vars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelay\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt_level\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 417\u001b[0;31m                 \u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelay\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"llvm\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    418\u001b[0m             \u001b[0mctx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"llvm\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m             \u001b[0;32mfrom\u001b[0m \u001b[0mtvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgraph_runtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Git/tvm/python/tvm/relay/build_module.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(func, target, target_host, params)\u001b[0m\n\u001b[1;32m    194\u001b[0m         \u001b[0mbld_mod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBuildModule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m         graph_json, mod, params = bld_mod.build(func, target, target_host,\n\u001b[0;32m--> 196\u001b[0;31m                                                 params)\n\u001b[0m\u001b[1;32m    197\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_json\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Git/tvm/python/tvm/relay/build_module.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, func, target, target_host, params)\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0;31m# Build the function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_host\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m         \u001b[0;31m# Get artifacts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0mgraph_json\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Git/tvm/python/tvm/_ffi/_ctypes/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    207\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtcodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m                 ctypes.byref(ret_val), ctypes.byref(ret_tcode)) != 0:\n\u001b[0;32m--> 209\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mget_last_ffi_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m         \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m         \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTVMError\u001b[0m: Traceback (most recent call last):\n  [bt] (8) 9   libtvm.dylib                        0x0000000115285fdd tvm::relay::backend::RelayBuildModule::GetFunction(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::shared_ptr<tvm::runtime::ModuleNode> const&)::'lambda1'(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const + 429\n  [bt] (7) 8   libtvm.dylib                        0x00000001152861f2 tvm::relay::backend::RelayBuildModule::Build(tvm::relay::Function, tvm::Map<tvm::Integer, tvm::Target, void, void> const&, tvm::Target const&) + 130\n  [bt] (6) 7   libtvm.dylib                        0x00000001152863f9 tvm::relay::backend::RelayBuildModule::BuildRelay(tvm::relay::Function, std::__1::unordered_map<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, tvm::runtime::NDArray, std::__1::hash<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > >, std::__1::equal_to<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > >, std::__1::allocator<std::__1::pair<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const, tvm::runtime::NDArray> > > const&) + 345\n  [bt] (5) 6   libtvm.dylib                        0x00000001153542ca tvm::relay::ModuleNode::FromExpr(tvm::relay::Expr const&, tvm::Map<tvm::relay::GlobalVar, tvm::relay::Function, void, void> const&) + 938\n  [bt] (4) 5   libtvm.dylib                        0x0000000115352b17 tvm::relay::ModuleNode::Add(tvm::relay::GlobalVar const&, tvm::relay::Function const&, bool) + 151\n  [bt] (3) 4   libtvm.dylib                        0x00000001156315b8 tvm::relay::InferType(tvm::relay::Function const&, tvm::relay::Module const&, tvm::relay::GlobalVar const&) + 472\n  [bt] (2) 3   libtvm.dylib                        0x0000000115630687 tvm::relay::TypeInferencer::Infer(tvm::relay::Expr) + 135\n  [bt] (1) 2   libtvm.dylib                        0x000000011531ee23 tvm::relay::ErrorReporter::RenderErrors(tvm::relay::Module const&, bool) + 5555\n  [bt] (0) 1   libtvm.dylib                        0x0000000114ed5949 dmlc::LogMessageFatal::~LogMessageFatal() + 57\n  [bt] (8) 9   libtvm.dylib                        0x0000000115352b17 tvm::relay::ModuleNode::Add(tvm::relay::GlobalVar const&, tvm::relay::Function const&, bool) + 151\n  [bt] (7) 8   libtvm.dylib                        0x00000001156315b8 tvm::relay::InferType(tvm::relay::Function const&, tvm::relay::Module const&, tvm::relay::GlobalVar const&) + 472\n  [bt] (6) 7   libtvm.dylib                        0x000000011563066b tvm::relay::TypeInferencer::Infer(tvm::relay::Expr) + 107\n  [bt] (5) 6   libtvm.dylib                        0x000000011564d05a tvm::relay::TypeSolver::Solve() + 1114\n  [bt] (4) 5   libtvm.dylib                        0x000000011564d6f8 tvm::TypedEnvFunc<bool (tvm::Array<tvm::relay::Type, void> const&, int, tvm::Attrs const&, tvm::relay::TypeReporter const&)>::operator()(tvm::Array<tvm::relay::Type, void> const&, int, tvm::Attrs const&, tvm::relay::TypeReporter const&) const + 328\n  [bt] (3) 4   libtvm.dylib                        0x00000001153a6ba9 std::__1::__function::__func<void tvm::runtime::TypedPackedFunc<bool (tvm::Array<tvm::relay::Type, void> const&, int, tvm::Attrs const&, tvm::relay::TypeReporter const&)>::AssignTypedLambda<bool (*)(tvm::Array<tvm::relay::Type, void> const&, int, tvm::Attrs const&, tvm::relay::TypeReporter const&)>(bool (*)(tvm::Array<tvm::relay::Type, void> const&, int, tvm::Attrs const&, tvm::relay::TypeReporter const&))::'lambda'(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*), std::__1::allocator<void tvm::runtime::TypedPackedFunc<bool (tvm::Array<tvm::relay::Type, void> const&, int, tvm::Attrs const&, tvm::relay::TypeReporter const&)>::AssignTypedLambda<bool (*)(tvm::Array<tvm::relay::Type, void> const&, int, tvm::Attrs const&, tvm::relay::TypeReporter const&)>(bool (*)(tvm::Array<tvm::relay::Type, void> const&, int, tvm::Attrs const&, tvm::relay::TypeReporter const&))::'lambda'(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)>, void (tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)>::operator()(tvm::runtime::TVMArgs&&, tvm::runtime::TVMRetValue*&&) + 137\n  [bt] (2) 3   libtvm.dylib                        0x00000001153a6c4f void tvm::runtime::detail::unpack_call_dispatcher<bool, 0, 4, bool (*)(tvm::Array<tvm::relay::Type, void> const&, int, tvm::Attrs const&, tvm::relay::TypeReporter const&)>::run<tvm::runtime::TVMArgValue, tvm::runtime::TVMArgValue, tvm::runtime::TVMArgValue, tvm::runtime::TVMArgValue>(bool (* const&)(tvm::Array<tvm::relay::Type, void> const&, int, tvm::Attrs const&, tvm::relay::TypeReporter const&), tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*, tvm::runtime::TVMArgValue&&, tvm::runtime::TVMArgValue&&, tvm::runtime::TVMArgValue&&, tvm::runtime::TVMArgValue&&) + 95\n  [bt] (1) 2   libtvm.dylib                        0x00000001154b479e tvm::relay::ConcatenateRel(tvm::Array<tvm::relay::Type, void> const&, int, tvm::Attrs const&, tvm::relay::TypeReporter const&) + 1918\n  [bt] (0) 1   libtvm.dylib                        0x0000000114ed5949 dmlc::LogMessageFatal::~LogMessageFatal() + 57\n  File \"/Users/jroesch/Git/tvm/src/relay/ir/error.cc\", line 132\nTVMError: \u001b[1m\nError(s) have occurred. We have annotated the program with them:\n\n\u001b[0m\u001b[1mIn `main`: \n\u001b[0mv0.0.1\nfn (%v0: Tensor[(10, 3, 224, 224), float32]) {\n  %0 = nn.conv2d(%v0, meta[relay.Constant][0], strides=[2, 2], padding=[3, 3], kernel_size=[7, 7])\n  %1 = nn.batch_norm(%0, meta[relay.Constant][1], meta[relay.Constant][2], meta[relay.Constant][3], meta[relay.Constant][4], epsilon=1e-05)\n  %2 = %1.0\n  %3 = nn.relu(%2)\n  %4 = nn.max_pool2d(%3, pool_size=[3, 3], strides=[2, 2], padding=[1, 1])\n  %5 = nn.conv2d(%4, meta[relay.Constant][5], padding=[1, 1], kernel_size=[3, 3])\n  %6 = nn.batch_norm(%5, meta[relay.Constant][6], meta[relay.Constant][7], meta[relay.Constant][8], meta[relay.Constant][9], epsilon=1e-05)\n  %7 = %6.0\n  %8 = nn.relu(%7)\n  %9 = nn.conv2d(%8, meta[relay.Constant][10], padding=[1, 1], kernel_size=[3, 3])\n  %10 = nn.batch_norm(%9, meta[relay.Constant][11], meta[relay.Constant][12], meta[relay.Constant][13], meta[relay.Constant][14], epsilon=1e-05)\n  %11 = %10.0\n  %12 = add(%11, %4)\n  %13 = nn.relu(%12)\n  %14 = nn.conv2d(%13, meta[relay.Constant][15], padding=[1, 1], kernel_size=[3, 3])\n  %15 = nn.batch_norm(%14, meta[relay.Constant][16], meta[relay.Constant][17], meta[relay.Constant][18], meta[relay.Constant][19], epsilon=1e-05)\n  %16 = %15.0\n  %17 = nn.relu(%16)\n  %18 = nn.conv2d(%17, meta[relay.Constant][20], padding=[1, 1], kernel_size=[3, 3])\n  %19 = nn.batch_norm(%18, meta[relay.Constant][21], meta[relay.Constant][22], meta[relay.Constant][23], meta[relay.Constant][24], epsilon=1e-05)\n  %20 = %19.0\n  %21 = add(%20, %13)\n  %22 = nn.relu(%21)\n  %23 = nn.conv2d(%22, meta[relay.Constant][25], strides=[2, 2], padding=[1, 1], kernel_size=[3, 3])\n  %24 = nn.batch_norm(%23, meta[relay.Constant][26], meta[relay.Constant][27], meta[relay.Constant][28], meta[relay.Constant][29], epsilon=1e-05)\n  %25 = %24.0\n  %26 = nn.relu(%25)\n  %27 = nn.conv2d(%26, meta[relay.Constant][30], padding=[1, 1], kernel_size=[3, 3])\n  %28 = nn.batch_norm(%27, meta[relay.Constant][31], meta[relay.Constant][32], meta[relay.Constant][33], meta[relay.Constant][34], epsilon=1e-05)\n  %29 = %28.0\n  %30 = nn.conv2d(%22, meta[relay.Constant][35], strides=[2, 2], kernel_size=[1, 1])\n  %31 = nn.batch_norm(%30, meta[relay.Constant][36], meta[relay.Constant][37], meta[relay.Constant][38], meta[relay.Constant][39], epsilon=1e-05)\n  %32 = %31.0\n  %33 = add(%29, %32)\n  %34 = nn.relu(%33)\n  %35 = nn.conv2d(%34, meta[relay.Constant][40], padding=[1, 1], kernel_size=[3, 3])\n  %36 = nn.batch_norm(%35, meta[relay.Constant][41], meta[relay.Constant][42], meta[relay.Constant][43], meta[relay.Constant][44], epsilon=1e-05)\n  %37 = %36.0\n  %38 = nn.relu(%37)\n  %39 = nn.conv2d(%38, meta[relay.Constant][45], padding=[1, 1], kernel_size=[3, 3])\n  %40 = nn.batch_norm(%39, meta[relay.Constant][46], meta[relay.Constant][47], meta[relay.Constant][48], meta[relay.Constant][49], epsilon=1e-05)\n  %41 = %40.0\n  %42 = add(%41, %34)\n  %43 = nn.relu(%42)\n  %44 = nn.conv2d(%43, meta[relay.Constant][50], strides=[2, 2], padding=[1, 1], kernel_size=[3, 3])\n  %45 = nn.batch_norm(%44, meta[relay.Constant][51], meta[relay.Constant][52], meta[relay.Constant][53], meta[relay.Constant][54], epsilon=1e-05)\n  %46 = %45.0\n  %47 = nn.relu(%46)\n  %48 = nn.conv2d(%47, meta[relay.Constant][55], padding=[1, 1], kernel_size=[3, 3])\n  %49 = nn.batch_norm(%48, meta[relay.Constant][56], meta[relay.Constant][57], meta[relay.Constant][58], meta[relay.Constant][59], epsilon=1e-05)\n  %50 = %49.0\n  %51 = nn.conv2d(%43, meta[relay.Constant][60], strides=[2, 2], kernel_size=[1, 1])\n  %52 = nn.batch_norm(%51, meta[relay.Constant][61], meta[relay.Constant][62], meta[relay.Constant][63], meta[relay.Constant][64], epsilon=1e-05)\n  %53 = %52.0\n  %54 = add(%50, %53)\n  %55 = nn.relu(%54)\n  %56 = nn.conv2d(%55, meta[relay.Constant][65], padding=[1, 1], kernel_size=[3, 3])\n  %57 = nn.batch_norm(%56, meta[relay.Constant][66], meta[relay.Constant][67], meta[relay.Constant][68], meta[relay.Constant][69], epsilon=1e-05)\n  %58 = %57.0\n  %59 = nn.relu(%58)\n  %60 = nn.conv2d(%59, meta[relay.Constant][70], padding=[1, 1], kernel_size=[3, 3])\n  %61 = nn.batch_norm(%60, meta[relay.Constant][71], meta[relay.Constant][72], meta[relay.Constant][73], meta[relay.Constant][74], epsilon=1e-05)\n  %62 = %61.0\n  %63 = add(%62, %55)\n  %64 = nn.relu(%63)\n  %65 = nn.conv2d(%64, meta[relay.Constant][75], strides=[2, 2], padding=[1, 1], kernel_size=[3, 3])\n  %66 = nn.batch_norm(%65, meta[relay.Constant][76], meta[relay.Constant][77], meta[relay.Constant][78], meta[relay.Constant][79], epsilon=1e-05)\n  %67 = %66.0\n  %68 = nn.relu(%67)\n  %69 = nn.conv2d(%68, meta[relay.Constant][80], padding=[1, 1], kernel_size=[3, 3])\n  %70 = nn.batch_norm(%69, meta[relay.Constant][81], meta[relay.Constant][82], meta[relay.Constant][83], meta[relay.Constant][84], epsilon=1e-05)\n  %71 = %70.0\n  %72 = nn.conv2d(%64, meta[relay.Constant][85], strides=[2, 2], kernel_size=[1, 1])\n  %73 = nn.batch_norm(%72, meta[relay.Constant][86], meta[relay.Constant][87], meta[relay.Constant][88], meta[relay.Constant][89], epsilon=1e-05)\n  %74 = %73.0\n  %75 = add(%71, %74)\n  %76 = nn.relu(%75)\n  %77 = nn.conv2d(%76, meta[relay.Constant][90], padding=[1, 1], kernel_size=[3, 3])\n  %78 = nn.batch_norm(%77, meta[relay.Constant][91], meta[relay.Constant][92], meta[relay.Constant][93], meta[relay.Constant][94], epsilon=1e-05)\n  %79 = %78.0\n  %80 = nn.relu(%79)\n  %81 = nn.conv2d(%80, meta[relay.Constant][95], padding=[1, 1], kernel_size=[3, 3])\n  %82 = nn.batch_norm(%81, meta[relay.Constant][96], meta[relay.Constant][97], meta[relay.Constant][98], meta[relay.Constant][99], epsilon=1e-05)\n  %83 = %82.0\n  %84 = add(%83, %76)\n  %85 = nn.relu(%84)\n  %86 = nn.global_avg_pool2d(%85)\n  %87 = shape_of(%86, dtype=\"int32\")\n  %88 = take(%87, int64(0), axis=0)\n  %89 = expand_dims(%88, axis=0)\n  %90 = expand_dims(int64(-1), axis=0)\n  %91 = (%89, %90)\n  concatenate(%91)\u001b[31man internal invariant was violated while typechecking your program [14:25:05] /Users/jroesch/Git/tvm/src/relay/op/tensor/transform.cc:204: Check failed: e_dtype == dtype (int64 vs. int32) : relay.concatenate requires all tensors have the same dtype\n; \u001b[39m\n}\n// meta data omitted. you can use show_meta_data=True to include meta data\n"
     ]
    }
   ],
   "source": [
    "torch_resnet18 = torchvision.models.resnet18()\n",
    "dummy_input = torch.randn(10, 3, 224, 224)\n",
    "torch.onnx.export(torch_resnet18, dummy_input, \"resnet.onnx\", verbose=True)\n",
    "onnx_resnet18 = onnx.load('resnet.onnx')\n",
    "func, params = relay.frontend.from_onnx(onnx_resnet18, shape={ '0': (10, 3, 224, 224) })\n",
    "print(func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Executing Relay\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pass Manager\n",
    "\n",
    "Relay has a flexible and configurable pass manager with an elegant API which be used to easily compose and schedule pass pipelines. We believe an easy to configure pipeline is important to enable intelligent exploration between a variety of \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizations\n",
    "\n",
    "Defining optimizations to transform your program is straight forward and easy to do in Relay.\n",
    "\n",
    "For example let's define a constant evaluator for Relay."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "puVpH87g0Nui"
   },
   "source": [
    "## Heterogeneous Execution\n",
    "\n",
    "Relay supports a high-level interface for scheduling computation across multiple heterogeneous devices. An interesting property of this pass is that it is not special, it is built using Relay's standard machinery for\n",
    "passes. \n",
    "\n",
    "We implement this by using an annotation to mark which computations we would like to schedule on which device, \n",
    "and a pass inserts all the appropriate calls to synchronize memory across devices. \n",
    "\n",
    "The below pass uses this machinery to schedule all convolutions onto the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 6276
    },
    "colab_type": "code",
    "id": "5sfd3Svu0Nui",
    "outputId": "bf451b82-ece9-429e-d2cf-395e1ee1b026"
   },
   "outputs": [],
   "source": [
    "class ScheduleConv2d(ExprMutator):\n",
    "    def __init__(self, device):\n",
    "        self.device = device\n",
    "        super().__init__()\n",
    "\n",
    "    def visit_call(self, expr):\n",
    "        visit = super().visit_call(expr)\n",
    "        if expr.op == tvm.relay.op.get(\"nn.conv2d\"):\n",
    "            return relay.annotation.on_device(visit, self.device)\n",
    "        else:\n",
    "            return visit\n",
    "\n",
    "def schedule_conv2d_on_gpu(expr):\n",
    "    sched = ScheduleConv2d(tvm.gpu(0))\n",
    "    return sched.visit(expr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v0.0.1\n",
      "fn (%data: Tensor[(1, 3, 224, 224), float32], %bn_data_gamma: Tensor[(3,), float32], %bn_data_beta: Tensor[(3,), float32], %bn_data_moving_mean: Tensor[(3,), float32], %bn_data_moving_var: Tensor[(3,), float32], %conv0_weight: Tensor[(64, 3, 7, 7), float32], %bn0_gamma: Tensor[(64,), float32], %bn0_beta: Tensor[(64,), float32], %bn0_moving_mean: Tensor[(64,), float32], %bn0_moving_var: Tensor[(64,), float32], %stage1_unit1_bn1_gamma: Tensor[(64,), float32], %stage1_unit1_bn1_beta: Tensor[(64,), float32], %stage1_unit1_bn1_moving_mean: Tensor[(64,), float32], %stage1_unit1_bn1_moving_var: Tensor[(64,), float32], %stage1_unit1_conv1_weight: Tensor[(64, 64, 3, 3), float32], %stage1_unit1_bn2_gamma: Tensor[(64,), float32], %stage1_unit1_bn2_beta: Tensor[(64,), float32], %stage1_unit1_bn2_moving_mean: Tensor[(64,), float32], %stage1_unit1_bn2_moving_var: Tensor[(64,), float32], %stage1_unit1_conv2_weight: Tensor[(64, 64, 3, 3), float32], %stage1_unit1_sc_weight: Tensor[(64, 64, 1, 1), float32], %stage1_unit2_bn1_gamma: Tensor[(64,), float32], %stage1_unit2_bn1_beta: Tensor[(64,), float32], %stage1_unit2_bn1_moving_mean: Tensor[(64,), float32], %stage1_unit2_bn1_moving_var: Tensor[(64,), float32], %stage1_unit2_conv1_weight: Tensor[(64, 64, 3, 3), float32], %stage1_unit2_bn2_gamma: Tensor[(64,), float32], %stage1_unit2_bn2_beta: Tensor[(64,), float32], %stage1_unit2_bn2_moving_mean: Tensor[(64,), float32], %stage1_unit2_bn2_moving_var: Tensor[(64,), float32], %stage1_unit2_conv2_weight: Tensor[(64, 64, 3, 3), float32], %stage2_unit1_bn1_gamma: Tensor[(64,), float32], %stage2_unit1_bn1_beta: Tensor[(64,), float32], %stage2_unit1_bn1_moving_mean: Tensor[(64,), float32], %stage2_unit1_bn1_moving_var: Tensor[(64,), float32], %stage2_unit1_conv1_weight: Tensor[(128, 64, 3, 3), float32], %stage2_unit1_bn2_gamma: Tensor[(128,), float32], %stage2_unit1_bn2_beta: Tensor[(128,), float32], %stage2_unit1_bn2_moving_mean: Tensor[(128,), float32], %stage2_unit1_bn2_moving_var: Tensor[(128,), float32], %stage2_unit1_conv2_weight: Tensor[(128, 128, 3, 3), float32], %stage2_unit1_sc_weight: Tensor[(128, 64, 1, 1), float32], %stage2_unit2_bn1_gamma: Tensor[(128,), float32], %stage2_unit2_bn1_beta: Tensor[(128,), float32], %stage2_unit2_bn1_moving_mean: Tensor[(128,), float32], %stage2_unit2_bn1_moving_var: Tensor[(128,), float32], %stage2_unit2_conv1_weight: Tensor[(128, 128, 3, 3), float32], %stage2_unit2_bn2_gamma: Tensor[(128,), float32], %stage2_unit2_bn2_beta: Tensor[(128,), float32], %stage2_unit2_bn2_moving_mean: Tensor[(128,), float32], %stage2_unit2_bn2_moving_var: Tensor[(128,), float32], %stage2_unit2_conv2_weight: Tensor[(128, 128, 3, 3), float32], %stage3_unit1_bn1_gamma: Tensor[(128,), float32], %stage3_unit1_bn1_beta: Tensor[(128,), float32], %stage3_unit1_bn1_moving_mean: Tensor[(128,), float32], %stage3_unit1_bn1_moving_var: Tensor[(128,), float32], %stage3_unit1_conv1_weight: Tensor[(256, 128, 3, 3), float32], %stage3_unit1_bn2_gamma: Tensor[(256,), float32], %stage3_unit1_bn2_beta: Tensor[(256,), float32], %stage3_unit1_bn2_moving_mean: Tensor[(256,), float32], %stage3_unit1_bn2_moving_var: Tensor[(256,), float32], %stage3_unit1_conv2_weight: Tensor[(256, 256, 3, 3), float32], %stage3_unit1_sc_weight: Tensor[(256, 128, 1, 1), float32], %stage3_unit2_bn1_gamma: Tensor[(256,), float32], %stage3_unit2_bn1_beta: Tensor[(256,), float32], %stage3_unit2_bn1_moving_mean: Tensor[(256,), float32], %stage3_unit2_bn1_moving_var: Tensor[(256,), float32], %stage3_unit2_conv1_weight: Tensor[(256, 256, 3, 3), float32], %stage3_unit2_bn2_gamma: Tensor[(256,), float32], %stage3_unit2_bn2_beta: Tensor[(256,), float32], %stage3_unit2_bn2_moving_mean: Tensor[(256,), float32], %stage3_unit2_bn2_moving_var: Tensor[(256,), float32], %stage3_unit2_conv2_weight: Tensor[(256, 256, 3, 3), float32], %stage4_unit1_bn1_gamma: Tensor[(256,), float32], %stage4_unit1_bn1_beta: Tensor[(256,), float32], %stage4_unit1_bn1_moving_mean: Tensor[(256,), float32], %stage4_unit1_bn1_moving_var: Tensor[(256,), float32], %stage4_unit1_conv1_weight: Tensor[(512, 256, 3, 3), float32], %stage4_unit1_bn2_gamma: Tensor[(512,), float32], %stage4_unit1_bn2_beta: Tensor[(512,), float32], %stage4_unit1_bn2_moving_mean: Tensor[(512,), float32], %stage4_unit1_bn2_moving_var: Tensor[(512,), float32], %stage4_unit1_conv2_weight: Tensor[(512, 512, 3, 3), float32], %stage4_unit1_sc_weight: Tensor[(512, 256, 1, 1), float32], %stage4_unit2_bn1_gamma: Tensor[(512,), float32], %stage4_unit2_bn1_beta: Tensor[(512,), float32], %stage4_unit2_bn1_moving_mean: Tensor[(512,), float32], %stage4_unit2_bn1_moving_var: Tensor[(512,), float32], %stage4_unit2_conv1_weight: Tensor[(512, 512, 3, 3), float32], %stage4_unit2_bn2_gamma: Tensor[(512,), float32], %stage4_unit2_bn2_beta: Tensor[(512,), float32], %stage4_unit2_bn2_moving_mean: Tensor[(512,), float32], %stage4_unit2_bn2_moving_var: Tensor[(512,), float32], %stage4_unit2_conv2_weight: Tensor[(512, 512, 3, 3), float32], %bn1_gamma: Tensor[(512,), float32], %bn1_beta: Tensor[(512,), float32], %bn1_moving_mean: Tensor[(512,), float32], %bn1_moving_var: Tensor[(512,), float32], %fc1_weight: Tensor[(1000, 512), float32], %fc1_bias: Tensor[(1000,), float32]) -> Tensor[(1, 1000), float32] {\n",
      "  %0 = nn.batch_norm(%data, %bn_data_gamma, %bn_data_beta, %bn_data_moving_mean, %bn_data_moving_var, epsilon=2e-05, scale=False) /* ty=(Tensor[(1, 3, 224, 224), float32], Tensor[(3,), float32], Tensor[(3,), float32]) */\n",
      "  %1 = %0.0\n",
      "  %2 = nn.conv2d(%1, %conv0_weight, strides=[2, 2], padding=[3, 3], channels=64, kernel_size=[7, 7]) /* ty=Tensor[(1, 64, 112, 112), float32] */\n",
      "  %3 = nn.batch_norm(%2, %bn0_gamma, %bn0_beta, %bn0_moving_mean, %bn0_moving_var, epsilon=2e-05) /* ty=(Tensor[(1, 64, 112, 112), float32], Tensor[(64,), float32], Tensor[(64,), float32]) */\n",
      "  %4 = %3.0\n",
      "  %5 = nn.relu(%4) /* ty=Tensor[(1, 64, 112, 112), float32] */\n",
      "  %6 = nn.max_pool2d(%5, pool_size=[3, 3], strides=[2, 2], padding=[1, 1]) /* ty=Tensor[(1, 64, 56, 56), float32] */\n",
      "  %7 = nn.batch_norm(%6, %stage1_unit1_bn1_gamma, %stage1_unit1_bn1_beta, %stage1_unit1_bn1_moving_mean, %stage1_unit1_bn1_moving_var, epsilon=2e-05) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64,), float32], Tensor[(64,), float32]) */\n",
      "  %8 = %7.0\n",
      "  %9 = nn.relu(%8) /* ty=Tensor[(1, 64, 56, 56), float32] */\n",
      "  %10 = nn.conv2d(%9, %stage1_unit1_conv1_weight, padding=[1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(1, 64, 56, 56), float32] */\n",
      "  %11 = nn.batch_norm(%10, %stage1_unit1_bn2_gamma, %stage1_unit1_bn2_beta, %stage1_unit1_bn2_moving_mean, %stage1_unit1_bn2_moving_var, epsilon=2e-05) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64,), float32], Tensor[(64,), float32]) */\n",
      "  %12 = %11.0\n",
      "  %13 = nn.relu(%12) /* ty=Tensor[(1, 64, 56, 56), float32] */\n",
      "  %14 = nn.conv2d(%13, %stage1_unit1_conv2_weight, padding=[1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(1, 64, 56, 56), float32] */\n",
      "  %15 = nn.conv2d(%9, %stage1_unit1_sc_weight, channels=64, kernel_size=[1, 1]) /* ty=Tensor[(1, 64, 56, 56), float32] */\n",
      "  %16 = add(%14, %15) /* ty=Tensor[(1, 64, 56, 56), float32] */\n",
      "  %17 = nn.batch_norm(%16, %stage1_unit2_bn1_gamma, %stage1_unit2_bn1_beta, %stage1_unit2_bn1_moving_mean, %stage1_unit2_bn1_moving_var, epsilon=2e-05) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64,), float32], Tensor[(64,), float32]) */\n",
      "  %18 = %17.0\n",
      "  %19 = nn.relu(%18) /* ty=Tensor[(1, 64, 56, 56), float32] */\n",
      "  %20 = nn.conv2d(%19, %stage1_unit2_conv1_weight, padding=[1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(1, 64, 56, 56), float32] */\n",
      "  %21 = nn.batch_norm(%20, %stage1_unit2_bn2_gamma, %stage1_unit2_bn2_beta, %stage1_unit2_bn2_moving_mean, %stage1_unit2_bn2_moving_var, epsilon=2e-05) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64,), float32], Tensor[(64,), float32]) */\n",
      "  %22 = %21.0\n",
      "  %23 = nn.relu(%22) /* ty=Tensor[(1, 64, 56, 56), float32] */\n",
      "  %24 = nn.conv2d(%23, %stage1_unit2_conv2_weight, padding=[1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(1, 64, 56, 56), float32] */\n",
      "  %25 = add(%24, %16) /* ty=Tensor[(1, 64, 56, 56), float32] */\n",
      "  %26 = nn.batch_norm(%25, %stage2_unit1_bn1_gamma, %stage2_unit1_bn1_beta, %stage2_unit1_bn1_moving_mean, %stage2_unit1_bn1_moving_var, epsilon=2e-05) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64,), float32], Tensor[(64,), float32]) */\n",
      "  %27 = %26.0\n",
      "  %28 = nn.relu(%27) /* ty=Tensor[(1, 64, 56, 56), float32] */\n",
      "  %29 = nn.conv2d(%28, %stage2_unit1_conv1_weight, strides=[2, 2], padding=[1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(1, 128, 28, 28), float32] */\n",
      "  %30 = nn.batch_norm(%29, %stage2_unit1_bn2_gamma, %stage2_unit1_bn2_beta, %stage2_unit1_bn2_moving_mean, %stage2_unit1_bn2_moving_var, epsilon=2e-05) /* ty=(Tensor[(1, 128, 28, 28), float32], Tensor[(128,), float32], Tensor[(128,), float32]) */\n",
      "  %31 = %30.0\n",
      "  %32 = nn.relu(%31) /* ty=Tensor[(1, 128, 28, 28), float32] */\n",
      "  %33 = nn.conv2d(%32, %stage2_unit1_conv2_weight, padding=[1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(1, 128, 28, 28), float32] */\n",
      "  %34 = nn.conv2d(%28, %stage2_unit1_sc_weight, strides=[2, 2], channels=128, kernel_size=[1, 1]) /* ty=Tensor[(1, 128, 28, 28), float32] */\n",
      "  %35 = add(%33, %34) /* ty=Tensor[(1, 128, 28, 28), float32] */\n",
      "  %36 = nn.batch_norm(%35, %stage2_unit2_bn1_gamma, %stage2_unit2_bn1_beta, %stage2_unit2_bn1_moving_mean, %stage2_unit2_bn1_moving_var, epsilon=2e-05) /* ty=(Tensor[(1, 128, 28, 28), float32], Tensor[(128,), float32], Tensor[(128,), float32]) */\n",
      "  %37 = %36.0\n",
      "  %38 = nn.relu(%37) /* ty=Tensor[(1, 128, 28, 28), float32] */\n",
      "  %39 = nn.conv2d(%38, %stage2_unit2_conv1_weight, padding=[1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(1, 128, 28, 28), float32] */\n",
      "  %40 = nn.batch_norm(%39, %stage2_unit2_bn2_gamma, %stage2_unit2_bn2_beta, %stage2_unit2_bn2_moving_mean, %stage2_unit2_bn2_moving_var, epsilon=2e-05) /* ty=(Tensor[(1, 128, 28, 28), float32], Tensor[(128,), float32], Tensor[(128,), float32]) */\n",
      "  %41 = %40.0\n",
      "  %42 = nn.relu(%41) /* ty=Tensor[(1, 128, 28, 28), float32] */\n",
      "  %43 = nn.conv2d(%42, %stage2_unit2_conv2_weight, padding=[1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(1, 128, 28, 28), float32] */\n",
      "  %44 = add(%43, %35) /* ty=Tensor[(1, 128, 28, 28), float32] */\n",
      "  %45 = nn.batch_norm(%44, %stage3_unit1_bn1_gamma, %stage3_unit1_bn1_beta, %stage3_unit1_bn1_moving_mean, %stage3_unit1_bn1_moving_var, epsilon=2e-05) /* ty=(Tensor[(1, 128, 28, 28), float32], Tensor[(128,), float32], Tensor[(128,), float32]) */\n",
      "  %46 = %45.0\n",
      "  %47 = nn.relu(%46) /* ty=Tensor[(1, 128, 28, 28), float32] */\n",
      "  %48 = nn.conv2d(%47, %stage3_unit1_conv1_weight, strides=[2, 2], padding=[1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(1, 256, 14, 14), float32] */\n",
      "  %49 = nn.batch_norm(%48, %stage3_unit1_bn2_gamma, %stage3_unit1_bn2_beta, %stage3_unit1_bn2_moving_mean, %stage3_unit1_bn2_moving_var, epsilon=2e-05) /* ty=(Tensor[(1, 256, 14, 14), float32], Tensor[(256,), float32], Tensor[(256,), float32]) */\n",
      "  %50 = %49.0\n",
      "  %51 = nn.relu(%50) /* ty=Tensor[(1, 256, 14, 14), float32] */\n",
      "  %52 = nn.conv2d(%51, %stage3_unit1_conv2_weight, padding=[1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(1, 256, 14, 14), float32] */\n",
      "  %53 = nn.conv2d(%47, %stage3_unit1_sc_weight, strides=[2, 2], channels=256, kernel_size=[1, 1]) /* ty=Tensor[(1, 256, 14, 14), float32] */\n",
      "  %54 = add(%52, %53) /* ty=Tensor[(1, 256, 14, 14), float32] */\n",
      "  %55 = nn.batch_norm(%54, %stage3_unit2_bn1_gamma, %stage3_unit2_bn1_beta, %stage3_unit2_bn1_moving_mean, %stage3_unit2_bn1_moving_var, epsilon=2e-05) /* ty=(Tensor[(1, 256, 14, 14), float32], Tensor[(256,), float32], Tensor[(256,), float32]) */\n",
      "  %56 = %55.0\n",
      "  %57 = nn.relu(%56) /* ty=Tensor[(1, 256, 14, 14), float32] */\n",
      "  %58 = nn.conv2d(%57, %stage3_unit2_conv1_weight, padding=[1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(1, 256, 14, 14), float32] */\n",
      "  %59 = nn.batch_norm(%58, %stage3_unit2_bn2_gamma, %stage3_unit2_bn2_beta, %stage3_unit2_bn2_moving_mean, %stage3_unit2_bn2_moving_var, epsilon=2e-05) /* ty=(Tensor[(1, 256, 14, 14), float32], Tensor[(256,), float32], Tensor[(256,), float32]) */\n",
      "  %60 = %59.0\n",
      "  %61 = nn.relu(%60) /* ty=Tensor[(1, 256, 14, 14), float32] */\n",
      "  %62 = nn.conv2d(%61, %stage3_unit2_conv2_weight, padding=[1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(1, 256, 14, 14), float32] */\n",
      "  %63 = add(%62, %54) /* ty=Tensor[(1, 256, 14, 14), float32] */\n",
      "  %64 = nn.batch_norm(%63, %stage4_unit1_bn1_gamma, %stage4_unit1_bn1_beta, %stage4_unit1_bn1_moving_mean, %stage4_unit1_bn1_moving_var, epsilon=2e-05) /* ty=(Tensor[(1, 256, 14, 14), float32], Tensor[(256,), float32], Tensor[(256,), float32]) */\n",
      "  %65 = %64.0\n",
      "  %66 = nn.relu(%65) /* ty=Tensor[(1, 256, 14, 14), float32] */\n",
      "  %67 = nn.conv2d(%66, %stage4_unit1_conv1_weight, strides=[2, 2], padding=[1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(1, 512, 7, 7), float32] */\n",
      "  %68 = nn.batch_norm(%67, %stage4_unit1_bn2_gamma, %stage4_unit1_bn2_beta, %stage4_unit1_bn2_moving_mean, %stage4_unit1_bn2_moving_var, epsilon=2e-05) /* ty=(Tensor[(1, 512, 7, 7), float32], Tensor[(512,), float32], Tensor[(512,), float32]) */\n",
      "  %69 = %68.0\n",
      "  %70 = nn.relu(%69) /* ty=Tensor[(1, 512, 7, 7), float32] */\n",
      "  %71 = nn.conv2d(%70, %stage4_unit1_conv2_weight, padding=[1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(1, 512, 7, 7), float32] */\n",
      "  %72 = nn.conv2d(%66, %stage4_unit1_sc_weight, strides=[2, 2], channels=512, kernel_size=[1, 1]) /* ty=Tensor[(1, 512, 7, 7), float32] */\n",
      "  %73 = add(%71, %72) /* ty=Tensor[(1, 512, 7, 7), float32] */\n",
      "  %74 = nn.batch_norm(%73, %stage4_unit2_bn1_gamma, %stage4_unit2_bn1_beta, %stage4_unit2_bn1_moving_mean, %stage4_unit2_bn1_moving_var, epsilon=2e-05) /* ty=(Tensor[(1, 512, 7, 7), float32], Tensor[(512,), float32], Tensor[(512,), float32]) */\n",
      "  %75 = %74.0\n",
      "  %76 = nn.relu(%75) /* ty=Tensor[(1, 512, 7, 7), float32] */\n",
      "  %77 = nn.conv2d(%76, %stage4_unit2_conv1_weight, padding=[1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(1, 512, 7, 7), float32] */\n",
      "  %78 = nn.batch_norm(%77, %stage4_unit2_bn2_gamma, %stage4_unit2_bn2_beta, %stage4_unit2_bn2_moving_mean, %stage4_unit2_bn2_moving_var, epsilon=2e-05) /* ty=(Tensor[(1, 512, 7, 7), float32], Tensor[(512,), float32], Tensor[(512,), float32]) */\n",
      "  %79 = %78.0\n",
      "  %80 = nn.relu(%79) /* ty=Tensor[(1, 512, 7, 7), float32] */\n",
      "  %81 = nn.conv2d(%80, %stage4_unit2_conv2_weight, padding=[1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(1, 512, 7, 7), float32] */\n",
      "  %82 = add(%81, %73) /* ty=Tensor[(1, 512, 7, 7), float32] */\n",
      "  %83 = nn.batch_norm(%82, %bn1_gamma, %bn1_beta, %bn1_moving_mean, %bn1_moving_var, epsilon=2e-05) /* ty=(Tensor[(1, 512, 7, 7), float32], Tensor[(512,), float32], Tensor[(512,), float32]) */\n",
      "  %84 = %83.0\n",
      "  %85 = nn.relu(%84) /* ty=Tensor[(1, 512, 7, 7), float32] */\n",
      "  %86 = nn.global_avg_pool2d(%85) /* ty=Tensor[(1, 512, 1, 1), float32] */\n",
      "  %87 = nn.batch_flatten(%86) /* ty=Tensor[(1, 512), float32] */\n",
      "  %88 = nn.dense(%87, %fc1_weight, units=1000) /* ty=Tensor[(1, 1000), float32] */\n",
      "  %89 = nn.bias_add(%88, %fc1_bias, axis=-1) /* ty=Tensor[(1, 1000), float32] */\n",
      "  nn.softmax(%89) /* ty=Tensor[(1, 1000), float32] */\n",
      "}\n",
      "v0.0.1\n",
      "fn (%data: Tensor[(1, 3, 224, 224), float32], %bn_data_gamma: Tensor[(3,), float32], %bn_data_beta: Tensor[(3,), float32], %bn_data_moving_mean: Tensor[(3,), float32], %bn_data_moving_var: Tensor[(3,), float32], %conv0_weight: Tensor[(64, 3, 7, 7), float32], %bn0_gamma: Tensor[(64,), float32], %bn0_beta: Tensor[(64,), float32], %bn0_moving_mean: Tensor[(64,), float32], %bn0_moving_var: Tensor[(64,), float32], %stage1_unit1_bn1_gamma: Tensor[(64,), float32], %stage1_unit1_bn1_beta: Tensor[(64,), float32], %stage1_unit1_bn1_moving_mean: Tensor[(64,), float32], %stage1_unit1_bn1_moving_var: Tensor[(64,), float32], %stage1_unit1_conv1_weight: Tensor[(64, 64, 3, 3), float32], %stage1_unit1_bn2_gamma: Tensor[(64,), float32], %stage1_unit1_bn2_beta: Tensor[(64,), float32], %stage1_unit1_bn2_moving_mean: Tensor[(64,), float32], %stage1_unit1_bn2_moving_var: Tensor[(64,), float32], %stage1_unit1_conv2_weight: Tensor[(64, 64, 3, 3), float32], %stage1_unit1_sc_weight: Tensor[(64, 64, 1, 1), float32], %stage1_unit2_bn1_gamma: Tensor[(64,), float32], %stage1_unit2_bn1_beta: Tensor[(64,), float32], %stage1_unit2_bn1_moving_mean: Tensor[(64,), float32], %stage1_unit2_bn1_moving_var: Tensor[(64,), float32], %stage1_unit2_conv1_weight: Tensor[(64, 64, 3, 3), float32], %stage1_unit2_bn2_gamma: Tensor[(64,), float32], %stage1_unit2_bn2_beta: Tensor[(64,), float32], %stage1_unit2_bn2_moving_mean: Tensor[(64,), float32], %stage1_unit2_bn2_moving_var: Tensor[(64,), float32], %stage1_unit2_conv2_weight: Tensor[(64, 64, 3, 3), float32], %stage2_unit1_bn1_gamma: Tensor[(64,), float32], %stage2_unit1_bn1_beta: Tensor[(64,), float32], %stage2_unit1_bn1_moving_mean: Tensor[(64,), float32], %stage2_unit1_bn1_moving_var: Tensor[(64,), float32], %stage2_unit1_conv1_weight: Tensor[(128, 64, 3, 3), float32], %stage2_unit1_bn2_gamma: Tensor[(128,), float32], %stage2_unit1_bn2_beta: Tensor[(128,), float32], %stage2_unit1_bn2_moving_mean: Tensor[(128,), float32], %stage2_unit1_bn2_moving_var: Tensor[(128,), float32], %stage2_unit1_conv2_weight: Tensor[(128, 128, 3, 3), float32], %stage2_unit1_sc_weight: Tensor[(128, 64, 1, 1), float32], %stage2_unit2_bn1_gamma: Tensor[(128,), float32], %stage2_unit2_bn1_beta: Tensor[(128,), float32], %stage2_unit2_bn1_moving_mean: Tensor[(128,), float32], %stage2_unit2_bn1_moving_var: Tensor[(128,), float32], %stage2_unit2_conv1_weight: Tensor[(128, 128, 3, 3), float32], %stage2_unit2_bn2_gamma: Tensor[(128,), float32], %stage2_unit2_bn2_beta: Tensor[(128,), float32], %stage2_unit2_bn2_moving_mean: Tensor[(128,), float32], %stage2_unit2_bn2_moving_var: Tensor[(128,), float32], %stage2_unit2_conv2_weight: Tensor[(128, 128, 3, 3), float32], %stage3_unit1_bn1_gamma: Tensor[(128,), float32], %stage3_unit1_bn1_beta: Tensor[(128,), float32], %stage3_unit1_bn1_moving_mean: Tensor[(128,), float32], %stage3_unit1_bn1_moving_var: Tensor[(128,), float32], %stage3_unit1_conv1_weight: Tensor[(256, 128, 3, 3), float32], %stage3_unit1_bn2_gamma: Tensor[(256,), float32], %stage3_unit1_bn2_beta: Tensor[(256,), float32], %stage3_unit1_bn2_moving_mean: Tensor[(256,), float32], %stage3_unit1_bn2_moving_var: Tensor[(256,), float32], %stage3_unit1_conv2_weight: Tensor[(256, 256, 3, 3), float32], %stage3_unit1_sc_weight: Tensor[(256, 128, 1, 1), float32], %stage3_unit2_bn1_gamma: Tensor[(256,), float32], %stage3_unit2_bn1_beta: Tensor[(256,), float32], %stage3_unit2_bn1_moving_mean: Tensor[(256,), float32], %stage3_unit2_bn1_moving_var: Tensor[(256,), float32], %stage3_unit2_conv1_weight: Tensor[(256, 256, 3, 3), float32], %stage3_unit2_bn2_gamma: Tensor[(256,), float32], %stage3_unit2_bn2_beta: Tensor[(256,), float32], %stage3_unit2_bn2_moving_mean: Tensor[(256,), float32], %stage3_unit2_bn2_moving_var: Tensor[(256,), float32], %stage3_unit2_conv2_weight: Tensor[(256, 256, 3, 3), float32], %stage4_unit1_bn1_gamma: Tensor[(256,), float32], %stage4_unit1_bn1_beta: Tensor[(256,), float32], %stage4_unit1_bn1_moving_mean: Tensor[(256,), float32], %stage4_unit1_bn1_moving_var: Tensor[(256,), float32], %stage4_unit1_conv1_weight: Tensor[(512, 256, 3, 3), float32], %stage4_unit1_bn2_gamma: Tensor[(512,), float32], %stage4_unit1_bn2_beta: Tensor[(512,), float32], %stage4_unit1_bn2_moving_mean: Tensor[(512,), float32], %stage4_unit1_bn2_moving_var: Tensor[(512,), float32], %stage4_unit1_conv2_weight: Tensor[(512, 512, 3, 3), float32], %stage4_unit1_sc_weight: Tensor[(512, 256, 1, 1), float32], %stage4_unit2_bn1_gamma: Tensor[(512,), float32], %stage4_unit2_bn1_beta: Tensor[(512,), float32], %stage4_unit2_bn1_moving_mean: Tensor[(512,), float32], %stage4_unit2_bn1_moving_var: Tensor[(512,), float32], %stage4_unit2_conv1_weight: Tensor[(512, 512, 3, 3), float32], %stage4_unit2_bn2_gamma: Tensor[(512,), float32], %stage4_unit2_bn2_beta: Tensor[(512,), float32], %stage4_unit2_bn2_moving_mean: Tensor[(512,), float32], %stage4_unit2_bn2_moving_var: Tensor[(512,), float32], %stage4_unit2_conv2_weight: Tensor[(512, 512, 3, 3), float32], %bn1_gamma: Tensor[(512,), float32], %bn1_beta: Tensor[(512,), float32], %bn1_moving_mean: Tensor[(512,), float32], %bn1_moving_var: Tensor[(512,), float32], %fc1_weight: Tensor[(1000, 512), float32], %fc1_bias: Tensor[(1000,), float32]) -> Tensor[(1, 1000), float32] {\n",
      "  %0 = nn.batch_norm(%data, %bn_data_gamma, %bn_data_beta, %bn_data_moving_mean, %bn_data_moving_var, epsilon=2e-05, scale=False)\n",
      "  %1 = %0.0\n",
      "  %2 = nn.conv2d(%1, %conv0_weight, strides=[2, 2], padding=[3, 3], channels=64, kernel_size=[7, 7])\n",
      "  %3 = on_device(%2, meta[relay.attrs.OnDeviceAttrs][0])\n",
      "  %4 = nn.batch_norm(%3, %bn0_gamma, %bn0_beta, %bn0_moving_mean, %bn0_moving_var, epsilon=2e-05)\n",
      "  %5 = %4.0\n",
      "  %6 = nn.relu(%5)\n",
      "  %7 = nn.max_pool2d(%6, pool_size=[3, 3], strides=[2, 2], padding=[1, 1])\n",
      "  %8 = nn.batch_norm(%7, %stage1_unit1_bn1_gamma, %stage1_unit1_bn1_beta, %stage1_unit1_bn1_moving_mean, %stage1_unit1_bn1_moving_var, epsilon=2e-05)\n",
      "  %9 = %8.0\n",
      "  %10 = nn.relu(%9)\n",
      "  %11 = nn.conv2d(%10, %stage1_unit1_conv1_weight, padding=[1, 1], channels=64, kernel_size=[3, 3])\n",
      "  %12 = on_device(%11, meta[relay.attrs.OnDeviceAttrs][1])\n",
      "  %13 = nn.batch_norm(%12, %stage1_unit1_bn2_gamma, %stage1_unit1_bn2_beta, %stage1_unit1_bn2_moving_mean, %stage1_unit1_bn2_moving_var, epsilon=2e-05)\n",
      "  %14 = %13.0\n",
      "  %15 = nn.relu(%14)\n",
      "  %16 = nn.conv2d(%15, %stage1_unit1_conv2_weight, padding=[1, 1], channels=64, kernel_size=[3, 3])\n",
      "  %17 = on_device(%16, meta[relay.attrs.OnDeviceAttrs][2])\n",
      "  %18 = nn.conv2d(%10, %stage1_unit1_sc_weight, channels=64, kernel_size=[1, 1])\n",
      "  %19 = on_device(%18, meta[relay.attrs.OnDeviceAttrs][3])\n",
      "  %20 = add(%17, %19)\n",
      "  %21 = nn.batch_norm(%20, %stage1_unit2_bn1_gamma, %stage1_unit2_bn1_beta, %stage1_unit2_bn1_moving_mean, %stage1_unit2_bn1_moving_var, epsilon=2e-05)\n",
      "  %22 = %21.0\n",
      "  %23 = nn.relu(%22)\n",
      "  %24 = nn.conv2d(%23, %stage1_unit2_conv1_weight, padding=[1, 1], channels=64, kernel_size=[3, 3])\n",
      "  %25 = on_device(%24, meta[relay.attrs.OnDeviceAttrs][4])\n",
      "  %26 = nn.batch_norm(%25, %stage1_unit2_bn2_gamma, %stage1_unit2_bn2_beta, %stage1_unit2_bn2_moving_mean, %stage1_unit2_bn2_moving_var, epsilon=2e-05)\n",
      "  %27 = %26.0\n",
      "  %28 = nn.relu(%27)\n",
      "  %29 = nn.conv2d(%28, %stage1_unit2_conv2_weight, padding=[1, 1], channels=64, kernel_size=[3, 3])\n",
      "  %30 = on_device(%29, meta[relay.attrs.OnDeviceAttrs][5])\n",
      "  %31 = add(%30, %20)\n",
      "  %32 = nn.batch_norm(%31, %stage2_unit1_bn1_gamma, %stage2_unit1_bn1_beta, %stage2_unit1_bn1_moving_mean, %stage2_unit1_bn1_moving_var, epsilon=2e-05)\n",
      "  %33 = %32.0\n",
      "  %34 = nn.relu(%33)\n",
      "  %35 = nn.conv2d(%34, %stage2_unit1_conv1_weight, strides=[2, 2], padding=[1, 1], channels=128, kernel_size=[3, 3])\n",
      "  %36 = on_device(%35, meta[relay.attrs.OnDeviceAttrs][6])\n",
      "  %37 = nn.batch_norm(%36, %stage2_unit1_bn2_gamma, %stage2_unit1_bn2_beta, %stage2_unit1_bn2_moving_mean, %stage2_unit1_bn2_moving_var, epsilon=2e-05)\n",
      "  %38 = %37.0\n",
      "  %39 = nn.relu(%38)\n",
      "  %40 = nn.conv2d(%39, %stage2_unit1_conv2_weight, padding=[1, 1], channels=128, kernel_size=[3, 3])\n",
      "  %41 = on_device(%40, meta[relay.attrs.OnDeviceAttrs][7])\n",
      "  %42 = nn.conv2d(%34, %stage2_unit1_sc_weight, strides=[2, 2], channels=128, kernel_size=[1, 1])\n",
      "  %43 = on_device(%42, meta[relay.attrs.OnDeviceAttrs][8])\n",
      "  %44 = add(%41, %43)\n",
      "  %45 = nn.batch_norm(%44, %stage2_unit2_bn1_gamma, %stage2_unit2_bn1_beta, %stage2_unit2_bn1_moving_mean, %stage2_unit2_bn1_moving_var, epsilon=2e-05)\n",
      "  %46 = %45.0\n",
      "  %47 = nn.relu(%46)\n",
      "  %48 = nn.conv2d(%47, %stage2_unit2_conv1_weight, padding=[1, 1], channels=128, kernel_size=[3, 3])\n",
      "  %49 = on_device(%48, meta[relay.attrs.OnDeviceAttrs][9])\n",
      "  %50 = nn.batch_norm(%49, %stage2_unit2_bn2_gamma, %stage2_unit2_bn2_beta, %stage2_unit2_bn2_moving_mean, %stage2_unit2_bn2_moving_var, epsilon=2e-05)\n",
      "  %51 = %50.0\n",
      "  %52 = nn.relu(%51)\n",
      "  %53 = nn.conv2d(%52, %stage2_unit2_conv2_weight, padding=[1, 1], channels=128, kernel_size=[3, 3])\n",
      "  %54 = on_device(%53, meta[relay.attrs.OnDeviceAttrs][10])\n",
      "  %55 = add(%54, %44)\n",
      "  %56 = nn.batch_norm(%55, %stage3_unit1_bn1_gamma, %stage3_unit1_bn1_beta, %stage3_unit1_bn1_moving_mean, %stage3_unit1_bn1_moving_var, epsilon=2e-05)\n",
      "  %57 = %56.0\n",
      "  %58 = nn.relu(%57)\n",
      "  %59 = nn.conv2d(%58, %stage3_unit1_conv1_weight, strides=[2, 2], padding=[1, 1], channels=256, kernel_size=[3, 3])\n",
      "  %60 = on_device(%59, meta[relay.attrs.OnDeviceAttrs][11])\n",
      "  %61 = nn.batch_norm(%60, %stage3_unit1_bn2_gamma, %stage3_unit1_bn2_beta, %stage3_unit1_bn2_moving_mean, %stage3_unit1_bn2_moving_var, epsilon=2e-05)\n",
      "  %62 = %61.0\n",
      "  %63 = nn.relu(%62)\n",
      "  %64 = nn.conv2d(%63, %stage3_unit1_conv2_weight, padding=[1, 1], channels=256, kernel_size=[3, 3])\n",
      "  %65 = on_device(%64, meta[relay.attrs.OnDeviceAttrs][12])\n",
      "  %66 = nn.conv2d(%58, %stage3_unit1_sc_weight, strides=[2, 2], channels=256, kernel_size=[1, 1])\n",
      "  %67 = on_device(%66, meta[relay.attrs.OnDeviceAttrs][13])\n",
      "  %68 = add(%65, %67)\n",
      "  %69 = nn.batch_norm(%68, %stage3_unit2_bn1_gamma, %stage3_unit2_bn1_beta, %stage3_unit2_bn1_moving_mean, %stage3_unit2_bn1_moving_var, epsilon=2e-05)\n",
      "  %70 = %69.0\n",
      "  %71 = nn.relu(%70)\n",
      "  %72 = nn.conv2d(%71, %stage3_unit2_conv1_weight, padding=[1, 1], channels=256, kernel_size=[3, 3])\n",
      "  %73 = on_device(%72, meta[relay.attrs.OnDeviceAttrs][14])\n",
      "  %74 = nn.batch_norm(%73, %stage3_unit2_bn2_gamma, %stage3_unit2_bn2_beta, %stage3_unit2_bn2_moving_mean, %stage3_unit2_bn2_moving_var, epsilon=2e-05)\n",
      "  %75 = %74.0\n",
      "  %76 = nn.relu(%75)\n",
      "  %77 = nn.conv2d(%76, %stage3_unit2_conv2_weight, padding=[1, 1], channels=256, kernel_size=[3, 3])\n",
      "  %78 = on_device(%77, meta[relay.attrs.OnDeviceAttrs][15])\n",
      "  %79 = add(%78, %68)\n",
      "  %80 = nn.batch_norm(%79, %stage4_unit1_bn1_gamma, %stage4_unit1_bn1_beta, %stage4_unit1_bn1_moving_mean, %stage4_unit1_bn1_moving_var, epsilon=2e-05)\n",
      "  %81 = %80.0\n",
      "  %82 = nn.relu(%81)\n",
      "  %83 = nn.conv2d(%82, %stage4_unit1_conv1_weight, strides=[2, 2], padding=[1, 1], channels=512, kernel_size=[3, 3])\n",
      "  %84 = on_device(%83, meta[relay.attrs.OnDeviceAttrs][16])\n",
      "  %85 = nn.batch_norm(%84, %stage4_unit1_bn2_gamma, %stage4_unit1_bn2_beta, %stage4_unit1_bn2_moving_mean, %stage4_unit1_bn2_moving_var, epsilon=2e-05)\n",
      "  %86 = %85.0\n",
      "  %87 = nn.relu(%86)\n",
      "  %88 = nn.conv2d(%87, %stage4_unit1_conv2_weight, padding=[1, 1], channels=512, kernel_size=[3, 3])\n",
      "  %89 = on_device(%88, meta[relay.attrs.OnDeviceAttrs][17])\n",
      "  %90 = nn.conv2d(%82, %stage4_unit1_sc_weight, strides=[2, 2], channels=512, kernel_size=[1, 1])\n",
      "  %91 = on_device(%90, meta[relay.attrs.OnDeviceAttrs][18])\n",
      "  %92 = add(%89, %91)\n",
      "  %93 = nn.batch_norm(%92, %stage4_unit2_bn1_gamma, %stage4_unit2_bn1_beta, %stage4_unit2_bn1_moving_mean, %stage4_unit2_bn1_moving_var, epsilon=2e-05)\n",
      "  %94 = %93.0\n",
      "  %95 = nn.relu(%94)\n",
      "  %96 = nn.conv2d(%95, %stage4_unit2_conv1_weight, padding=[1, 1], channels=512, kernel_size=[3, 3])\n",
      "  %97 = on_device(%96, meta[relay.attrs.OnDeviceAttrs][19])\n",
      "  %98 = nn.batch_norm(%97, %stage4_unit2_bn2_gamma, %stage4_unit2_bn2_beta, %stage4_unit2_bn2_moving_mean, %stage4_unit2_bn2_moving_var, epsilon=2e-05)\n",
      "  %99 = %98.0\n",
      "  %100 = nn.relu(%99)\n",
      "  %101 = nn.conv2d(%100, %stage4_unit2_conv2_weight, padding=[1, 1], channels=512, kernel_size=[3, 3])\n",
      "  %102 = on_device(%101, meta[relay.attrs.OnDeviceAttrs][20])\n",
      "  %103 = add(%102, %92)\n",
      "  %104 = nn.batch_norm(%103, %bn1_gamma, %bn1_beta, %bn1_moving_mean, %bn1_moving_var, epsilon=2e-05)\n",
      "  %105 = %104.0\n",
      "  %106 = nn.relu(%105)\n",
      "  %107 = nn.global_avg_pool2d(%106)\n",
      "  %108 = nn.batch_flatten(%107)\n",
      "  %109 = nn.dense(%108, %fc1_weight, units=1000)\n",
      "  %110 = nn.bias_add(%109, %fc1_bias, axis=-1)\n",
      "  nn.softmax(%110)\n",
      "}\n",
      "// meta data omitted. you can use show_meta_data=True to include meta data\n",
      "v0.0.1\n",
      "fn (%data: Tensor[(1, 3, 224, 224), float32], %bn_data_gamma: Tensor[(3,), float32], %bn_data_beta: Tensor[(3,), float32], %bn_data_moving_mean: Tensor[(3,), float32], %bn_data_moving_var: Tensor[(3,), float32], %conv0_weight: Tensor[(64, 3, 7, 7), float32], %bn0_gamma: Tensor[(64,), float32], %bn0_beta: Tensor[(64,), float32], %bn0_moving_mean: Tensor[(64,), float32], %bn0_moving_var: Tensor[(64,), float32], %stage1_unit1_bn1_gamma: Tensor[(64,), float32], %stage1_unit1_bn1_beta: Tensor[(64,), float32], %stage1_unit1_bn1_moving_mean: Tensor[(64,), float32], %stage1_unit1_bn1_moving_var: Tensor[(64,), float32], %stage1_unit1_conv1_weight: Tensor[(64, 64, 3, 3), float32], %stage1_unit1_bn2_gamma: Tensor[(64,), float32], %stage1_unit1_bn2_beta: Tensor[(64,), float32], %stage1_unit1_bn2_moving_mean: Tensor[(64,), float32], %stage1_unit1_bn2_moving_var: Tensor[(64,), float32], %stage1_unit1_conv2_weight: Tensor[(64, 64, 3, 3), float32], %stage1_unit1_sc_weight: Tensor[(64, 64, 1, 1), float32], %stage1_unit2_bn1_gamma: Tensor[(64,), float32], %stage1_unit2_bn1_beta: Tensor[(64,), float32], %stage1_unit2_bn1_moving_mean: Tensor[(64,), float32], %stage1_unit2_bn1_moving_var: Tensor[(64,), float32], %stage1_unit2_conv1_weight: Tensor[(64, 64, 3, 3), float32], %stage1_unit2_bn2_gamma: Tensor[(64,), float32], %stage1_unit2_bn2_beta: Tensor[(64,), float32], %stage1_unit2_bn2_moving_mean: Tensor[(64,), float32], %stage1_unit2_bn2_moving_var: Tensor[(64,), float32], %stage1_unit2_conv2_weight: Tensor[(64, 64, 3, 3), float32], %stage2_unit1_bn1_gamma: Tensor[(64,), float32], %stage2_unit1_bn1_beta: Tensor[(64,), float32], %stage2_unit1_bn1_moving_mean: Tensor[(64,), float32], %stage2_unit1_bn1_moving_var: Tensor[(64,), float32], %stage2_unit1_conv1_weight: Tensor[(128, 64, 3, 3), float32], %stage2_unit1_bn2_gamma: Tensor[(128,), float32], %stage2_unit1_bn2_beta: Tensor[(128,), float32], %stage2_unit1_bn2_moving_mean: Tensor[(128,), float32], %stage2_unit1_bn2_moving_var: Tensor[(128,), float32], %stage2_unit1_conv2_weight: Tensor[(128, 128, 3, 3), float32], %stage2_unit1_sc_weight: Tensor[(128, 64, 1, 1), float32], %stage2_unit2_bn1_gamma: Tensor[(128,), float32], %stage2_unit2_bn1_beta: Tensor[(128,), float32], %stage2_unit2_bn1_moving_mean: Tensor[(128,), float32], %stage2_unit2_bn1_moving_var: Tensor[(128,), float32], %stage2_unit2_conv1_weight: Tensor[(128, 128, 3, 3), float32], %stage2_unit2_bn2_gamma: Tensor[(128,), float32], %stage2_unit2_bn2_beta: Tensor[(128,), float32], %stage2_unit2_bn2_moving_mean: Tensor[(128,), float32], %stage2_unit2_bn2_moving_var: Tensor[(128,), float32], %stage2_unit2_conv2_weight: Tensor[(128, 128, 3, 3), float32], %stage3_unit1_bn1_gamma: Tensor[(128,), float32], %stage3_unit1_bn1_beta: Tensor[(128,), float32], %stage3_unit1_bn1_moving_mean: Tensor[(128,), float32], %stage3_unit1_bn1_moving_var: Tensor[(128,), float32], %stage3_unit1_conv1_weight: Tensor[(256, 128, 3, 3), float32], %stage3_unit1_bn2_gamma: Tensor[(256,), float32], %stage3_unit1_bn2_beta: Tensor[(256,), float32], %stage3_unit1_bn2_moving_mean: Tensor[(256,), float32], %stage3_unit1_bn2_moving_var: Tensor[(256,), float32], %stage3_unit1_conv2_weight: Tensor[(256, 256, 3, 3), float32], %stage3_unit1_sc_weight: Tensor[(256, 128, 1, 1), float32], %stage3_unit2_bn1_gamma: Tensor[(256,), float32], %stage3_unit2_bn1_beta: Tensor[(256,), float32], %stage3_unit2_bn1_moving_mean: Tensor[(256,), float32], %stage3_unit2_bn1_moving_var: Tensor[(256,), float32], %stage3_unit2_conv1_weight: Tensor[(256, 256, 3, 3), float32], %stage3_unit2_bn2_gamma: Tensor[(256,), float32], %stage3_unit2_bn2_beta: Tensor[(256,), float32], %stage3_unit2_bn2_moving_mean: Tensor[(256,), float32], %stage3_unit2_bn2_moving_var: Tensor[(256,), float32], %stage3_unit2_conv2_weight: Tensor[(256, 256, 3, 3), float32], %stage4_unit1_bn1_gamma: Tensor[(256,), float32], %stage4_unit1_bn1_beta: Tensor[(256,), float32], %stage4_unit1_bn1_moving_mean: Tensor[(256,), float32], %stage4_unit1_bn1_moving_var: Tensor[(256,), float32], %stage4_unit1_conv1_weight: Tensor[(512, 256, 3, 3), float32], %stage4_unit1_bn2_gamma: Tensor[(512,), float32], %stage4_unit1_bn2_beta: Tensor[(512,), float32], %stage4_unit1_bn2_moving_mean: Tensor[(512,), float32], %stage4_unit1_bn2_moving_var: Tensor[(512,), float32], %stage4_unit1_conv2_weight: Tensor[(512, 512, 3, 3), float32], %stage4_unit1_sc_weight: Tensor[(512, 256, 1, 1), float32], %stage4_unit2_bn1_gamma: Tensor[(512,), float32], %stage4_unit2_bn1_beta: Tensor[(512,), float32], %stage4_unit2_bn1_moving_mean: Tensor[(512,), float32], %stage4_unit2_bn1_moving_var: Tensor[(512,), float32], %stage4_unit2_conv1_weight: Tensor[(512, 512, 3, 3), float32], %stage4_unit2_bn2_gamma: Tensor[(512,), float32], %stage4_unit2_bn2_beta: Tensor[(512,), float32], %stage4_unit2_bn2_moving_mean: Tensor[(512,), float32], %stage4_unit2_bn2_moving_var: Tensor[(512,), float32], %stage4_unit2_conv2_weight: Tensor[(512, 512, 3, 3), float32], %bn1_gamma: Tensor[(512,), float32], %bn1_beta: Tensor[(512,), float32], %bn1_moving_mean: Tensor[(512,), float32], %bn1_moving_var: Tensor[(512,), float32], %fc1_weight: Tensor[(1000, 512), float32], %fc1_bias: Tensor[(1000,), float32]) -> Tensor[(1, 1000), float32] {\n",
      "  %0 = nn.batch_norm(%data, %bn_data_gamma, %bn_data_beta, %bn_data_moving_mean, %bn_data_moving_var, epsilon=2e-05, scale=False)\n",
      "  %1 = %0.0\n",
      "  %2 = device_copy(%1, meta[relay.attrs.DeviceCopyAttrs][0])\n",
      "  %3 = nn.conv2d(%2, %conv0_weight, strides=[2, 2], padding=[3, 3], channels=64, kernel_size=[7, 7])\n",
      "  %4 = device_copy(%3, meta[relay.attrs.DeviceCopyAttrs][1])\n",
      "  %5 = nn.batch_norm(%4, %bn0_gamma, %bn0_beta, %bn0_moving_mean, %bn0_moving_var, epsilon=2e-05)\n",
      "  %6 = %5.0\n",
      "  %7 = nn.relu(%6)\n",
      "  %8 = nn.max_pool2d(%7, pool_size=[3, 3], strides=[2, 2], padding=[1, 1])\n",
      "  %9 = nn.batch_norm(%8, %stage1_unit1_bn1_gamma, %stage1_unit1_bn1_beta, %stage1_unit1_bn1_moving_mean, %stage1_unit1_bn1_moving_var, epsilon=2e-05)\n",
      "  %10 = %9.0\n",
      "  %11 = nn.relu(%10)\n",
      "  %12 = device_copy(%11, meta[relay.attrs.DeviceCopyAttrs][2])\n",
      "  %13 = nn.conv2d(%12, %stage1_unit1_conv1_weight, padding=[1, 1], channels=64, kernel_size=[3, 3])\n",
      "  %14 = device_copy(%13, meta[relay.attrs.DeviceCopyAttrs][3])\n",
      "  %15 = nn.batch_norm(%14, %stage1_unit1_bn2_gamma, %stage1_unit1_bn2_beta, %stage1_unit1_bn2_moving_mean, %stage1_unit1_bn2_moving_var, epsilon=2e-05)\n",
      "  %16 = %15.0\n",
      "  %17 = nn.relu(%16)\n",
      "  %18 = device_copy(%17, meta[relay.attrs.DeviceCopyAttrs][4])\n",
      "  %19 = nn.conv2d(%18, %stage1_unit1_conv2_weight, padding=[1, 1], channels=64, kernel_size=[3, 3])\n",
      "  %20 = device_copy(%19, meta[relay.attrs.DeviceCopyAttrs][5])\n",
      "  %21 = device_copy(%11, meta[relay.attrs.DeviceCopyAttrs][6])\n",
      "  %22 = nn.conv2d(%21, %stage1_unit1_sc_weight, channels=64, kernel_size=[1, 1])\n",
      "  %23 = device_copy(%22, meta[relay.attrs.DeviceCopyAttrs][7])\n",
      "  %24 = add(%20, %23)\n",
      "  %25 = nn.batch_norm(%24, %stage1_unit2_bn1_gamma, %stage1_unit2_bn1_beta, %stage1_unit2_bn1_moving_mean, %stage1_unit2_bn1_moving_var, epsilon=2e-05)\n",
      "  %26 = %25.0\n",
      "  %27 = nn.relu(%26)\n",
      "  %28 = device_copy(%27, meta[relay.attrs.DeviceCopyAttrs][8])\n",
      "  %29 = nn.conv2d(%28, %stage1_unit2_conv1_weight, padding=[1, 1], channels=64, kernel_size=[3, 3])\n",
      "  %30 = device_copy(%29, meta[relay.attrs.DeviceCopyAttrs][9])\n",
      "  %31 = nn.batch_norm(%30, %stage1_unit2_bn2_gamma, %stage1_unit2_bn2_beta, %stage1_unit2_bn2_moving_mean, %stage1_unit2_bn2_moving_var, epsilon=2e-05)\n",
      "  %32 = %31.0\n",
      "  %33 = nn.relu(%32)\n",
      "  %34 = device_copy(%33, meta[relay.attrs.DeviceCopyAttrs][10])\n",
      "  %35 = nn.conv2d(%34, %stage1_unit2_conv2_weight, padding=[1, 1], channels=64, kernel_size=[3, 3])\n",
      "  %36 = device_copy(%35, meta[relay.attrs.DeviceCopyAttrs][11])\n",
      "  %37 = add(%36, %24)\n",
      "  %38 = nn.batch_norm(%37, %stage2_unit1_bn1_gamma, %stage2_unit1_bn1_beta, %stage2_unit1_bn1_moving_mean, %stage2_unit1_bn1_moving_var, epsilon=2e-05)\n",
      "  %39 = %38.0\n",
      "  %40 = nn.relu(%39)\n",
      "  %41 = device_copy(%40, meta[relay.attrs.DeviceCopyAttrs][12])\n",
      "  %42 = nn.conv2d(%41, %stage2_unit1_conv1_weight, strides=[2, 2], padding=[1, 1], channels=128, kernel_size=[3, 3])\n",
      "  %43 = device_copy(%42, meta[relay.attrs.DeviceCopyAttrs][13])\n",
      "  %44 = nn.batch_norm(%43, %stage2_unit1_bn2_gamma, %stage2_unit1_bn2_beta, %stage2_unit1_bn2_moving_mean, %stage2_unit1_bn2_moving_var, epsilon=2e-05)\n",
      "  %45 = %44.0\n",
      "  %46 = nn.relu(%45)\n",
      "  %47 = device_copy(%46, meta[relay.attrs.DeviceCopyAttrs][14])\n",
      "  %48 = nn.conv2d(%47, %stage2_unit1_conv2_weight, padding=[1, 1], channels=128, kernel_size=[3, 3])\n",
      "  %49 = device_copy(%48, meta[relay.attrs.DeviceCopyAttrs][15])\n",
      "  %50 = device_copy(%40, meta[relay.attrs.DeviceCopyAttrs][16])\n",
      "  %51 = nn.conv2d(%50, %stage2_unit1_sc_weight, strides=[2, 2], channels=128, kernel_size=[1, 1])\n",
      "  %52 = device_copy(%51, meta[relay.attrs.DeviceCopyAttrs][17])\n",
      "  %53 = add(%49, %52)\n",
      "  %54 = nn.batch_norm(%53, %stage2_unit2_bn1_gamma, %stage2_unit2_bn1_beta, %stage2_unit2_bn1_moving_mean, %stage2_unit2_bn1_moving_var, epsilon=2e-05)\n",
      "  %55 = %54.0\n",
      "  %56 = nn.relu(%55)\n",
      "  %57 = device_copy(%56, meta[relay.attrs.DeviceCopyAttrs][18])\n",
      "  %58 = nn.conv2d(%57, %stage2_unit2_conv1_weight, padding=[1, 1], channels=128, kernel_size=[3, 3])\n",
      "  %59 = device_copy(%58, meta[relay.attrs.DeviceCopyAttrs][19])\n",
      "  %60 = nn.batch_norm(%59, %stage2_unit2_bn2_gamma, %stage2_unit2_bn2_beta, %stage2_unit2_bn2_moving_mean, %stage2_unit2_bn2_moving_var, epsilon=2e-05)\n",
      "  %61 = %60.0\n",
      "  %62 = nn.relu(%61)\n",
      "  %63 = device_copy(%62, meta[relay.attrs.DeviceCopyAttrs][20])\n",
      "  %64 = nn.conv2d(%63, %stage2_unit2_conv2_weight, padding=[1, 1], channels=128, kernel_size=[3, 3])\n",
      "  %65 = device_copy(%64, meta[relay.attrs.DeviceCopyAttrs][21])\n",
      "  %66 = add(%65, %53)\n",
      "  %67 = nn.batch_norm(%66, %stage3_unit1_bn1_gamma, %stage3_unit1_bn1_beta, %stage3_unit1_bn1_moving_mean, %stage3_unit1_bn1_moving_var, epsilon=2e-05)\n",
      "  %68 = %67.0\n",
      "  %69 = nn.relu(%68)\n",
      "  %70 = device_copy(%69, meta[relay.attrs.DeviceCopyAttrs][22])\n",
      "  %71 = nn.conv2d(%70, %stage3_unit1_conv1_weight, strides=[2, 2], padding=[1, 1], channels=256, kernel_size=[3, 3])\n",
      "  %72 = device_copy(%71, meta[relay.attrs.DeviceCopyAttrs][23])\n",
      "  %73 = nn.batch_norm(%72, %stage3_unit1_bn2_gamma, %stage3_unit1_bn2_beta, %stage3_unit1_bn2_moving_mean, %stage3_unit1_bn2_moving_var, epsilon=2e-05)\n",
      "  %74 = %73.0\n",
      "  %75 = nn.relu(%74)\n",
      "  %76 = device_copy(%75, meta[relay.attrs.DeviceCopyAttrs][24])\n",
      "  %77 = nn.conv2d(%76, %stage3_unit1_conv2_weight, padding=[1, 1], channels=256, kernel_size=[3, 3])\n",
      "  %78 = device_copy(%77, meta[relay.attrs.DeviceCopyAttrs][25])\n",
      "  %79 = device_copy(%69, meta[relay.attrs.DeviceCopyAttrs][26])\n",
      "  %80 = nn.conv2d(%79, %stage3_unit1_sc_weight, strides=[2, 2], channels=256, kernel_size=[1, 1])\n",
      "  %81 = device_copy(%80, meta[relay.attrs.DeviceCopyAttrs][27])\n",
      "  %82 = add(%78, %81)\n",
      "  %83 = nn.batch_norm(%82, %stage3_unit2_bn1_gamma, %stage3_unit2_bn1_beta, %stage3_unit2_bn1_moving_mean, %stage3_unit2_bn1_moving_var, epsilon=2e-05)\n",
      "  %84 = %83.0\n",
      "  %85 = nn.relu(%84)\n",
      "  %86 = device_copy(%85, meta[relay.attrs.DeviceCopyAttrs][28])\n",
      "  %87 = nn.conv2d(%86, %stage3_unit2_conv1_weight, padding=[1, 1], channels=256, kernel_size=[3, 3])\n",
      "  %88 = device_copy(%87, meta[relay.attrs.DeviceCopyAttrs][29])\n",
      "  %89 = nn.batch_norm(%88, %stage3_unit2_bn2_gamma, %stage3_unit2_bn2_beta, %stage3_unit2_bn2_moving_mean, %stage3_unit2_bn2_moving_var, epsilon=2e-05)\n",
      "  %90 = %89.0\n",
      "  %91 = nn.relu(%90)\n",
      "  %92 = device_copy(%91, meta[relay.attrs.DeviceCopyAttrs][30])\n",
      "  %93 = nn.conv2d(%92, %stage3_unit2_conv2_weight, padding=[1, 1], channels=256, kernel_size=[3, 3])\n",
      "  %94 = device_copy(%93, meta[relay.attrs.DeviceCopyAttrs][31])\n",
      "  %95 = add(%94, %82)\n",
      "  %96 = nn.batch_norm(%95, %stage4_unit1_bn1_gamma, %stage4_unit1_bn1_beta, %stage4_unit1_bn1_moving_mean, %stage4_unit1_bn1_moving_var, epsilon=2e-05)\n",
      "  %97 = %96.0\n",
      "  %98 = nn.relu(%97)\n",
      "  %99 = device_copy(%98, meta[relay.attrs.DeviceCopyAttrs][32])\n",
      "  %100 = nn.conv2d(%99, %stage4_unit1_conv1_weight, strides=[2, 2], padding=[1, 1], channels=512, kernel_size=[3, 3])\n",
      "  %101 = device_copy(%100, meta[relay.attrs.DeviceCopyAttrs][33])\n",
      "  %102 = nn.batch_norm(%101, %stage4_unit1_bn2_gamma, %stage4_unit1_bn2_beta, %stage4_unit1_bn2_moving_mean, %stage4_unit1_bn2_moving_var, epsilon=2e-05)\n",
      "  %103 = %102.0\n",
      "  %104 = nn.relu(%103)\n",
      "  %105 = device_copy(%104, meta[relay.attrs.DeviceCopyAttrs][34])\n",
      "  %106 = nn.conv2d(%105, %stage4_unit1_conv2_weight, padding=[1, 1], channels=512, kernel_size=[3, 3])\n",
      "  %107 = device_copy(%106, meta[relay.attrs.DeviceCopyAttrs][35])\n",
      "  %108 = device_copy(%98, meta[relay.attrs.DeviceCopyAttrs][36])\n",
      "  %109 = nn.conv2d(%108, %stage4_unit1_sc_weight, strides=[2, 2], channels=512, kernel_size=[1, 1])\n",
      "  %110 = device_copy(%109, meta[relay.attrs.DeviceCopyAttrs][37])\n",
      "  %111 = add(%107, %110)\n",
      "  %112 = nn.batch_norm(%111, %stage4_unit2_bn1_gamma, %stage4_unit2_bn1_beta, %stage4_unit2_bn1_moving_mean, %stage4_unit2_bn1_moving_var, epsilon=2e-05)\n",
      "  %113 = %112.0\n",
      "  %114 = nn.relu(%113)\n",
      "  %115 = device_copy(%114, meta[relay.attrs.DeviceCopyAttrs][38])\n",
      "  %116 = nn.conv2d(%115, %stage4_unit2_conv1_weight, padding=[1, 1], channels=512, kernel_size=[3, 3])\n",
      "  %117 = device_copy(%116, meta[relay.attrs.DeviceCopyAttrs][39])\n",
      "  %118 = nn.batch_norm(%117, %stage4_unit2_bn2_gamma, %stage4_unit2_bn2_beta, %stage4_unit2_bn2_moving_mean, %stage4_unit2_bn2_moving_var, epsilon=2e-05)\n",
      "  %119 = %118.0\n",
      "  %120 = nn.relu(%119)\n",
      "  %121 = device_copy(%120, meta[relay.attrs.DeviceCopyAttrs][40])\n",
      "  %122 = nn.conv2d(%121, %stage4_unit2_conv2_weight, padding=[1, 1], channels=512, kernel_size=[3, 3])\n",
      "  %123 = device_copy(%122, meta[relay.attrs.DeviceCopyAttrs][41])\n",
      "  %124 = add(%123, %111)\n",
      "  %125 = nn.batch_norm(%124, %bn1_gamma, %bn1_beta, %bn1_moving_mean, %bn1_moving_var, epsilon=2e-05)\n",
      "  %126 = %125.0\n",
      "  %127 = nn.relu(%126)\n",
      "  %128 = nn.global_avg_pool2d(%127)\n",
      "  %129 = nn.batch_flatten(%128)\n",
      "  %130 = nn.dense(%129, %fc1_weight, units=1000)\n",
      "  %131 = nn.bias_add(%130, %fc1_bias, axis=-1)\n",
      "  nn.softmax(%131)\n",
      "}\n",
      "// meta data omitted. you can use show_meta_data=True to include meta data\n"
     ]
    }
   ],
   "source": [
    "# TODO(@jroesch): use pass manager\n",
    "resnet, params = relay.testing.resnet.get_workload()\n",
    "print(resnet)\n",
    "resnet = schedule_conv2d_on_gpu(resnet)\n",
    "print(resnet)\n",
    "resnet = relay.ir_pass.rewrite_annotated_ops(resnet, 0)\n",
    "print(resnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Virtual Machine\n",
    "\n",
    "Relay supports three execution mechanisms, the VM provides the best balance between performance and expressivity out of them, but is also the newest and is still in alpha quality. We are working on shipping the remaining pieces of the VM over the next few weeks, with a beta version coming soon. \n",
    "\n",
    "Details about the VM can be found here..., but users can use the VM today from the high-level executor interface in Python, and directly in C++.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ahead of time compilation\n",
    "A final example of how what "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UPK5HmW30Nun"
   },
   "source": [
    "## VTA\n",
    "..."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "02_TVM_Tutorial_Relay.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
