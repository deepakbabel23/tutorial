{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SeiRi-zc0NuZ"
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/uwsampl/tutorial/blob/master/notebook/02_TVM_Tutorial_Relay.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BYNJGUSR0Nub"
   },
   "source": [
    "Please run the following block to ensure TVM is setup for *this notebook*, each notebook may have its own runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1972
    },
    "colab_type": "code",
    "id": "bdiA6WVx0Nuc",
    "outputId": "20b89448-5871-4940-d16f-4473310f3c4b"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "  import google.colab\n",
    "  IN_COLAB = True\n",
    "except:\n",
    "  IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "    ! gsutil cp \"gs://tvm-fcrc-binariesd5fce43e-8373-11e9-bfb6-0242ac1c0002/tvm.tar.gz\" /tmp/tvm.tar.gz\n",
    "    ! mkdir -p /tvm\n",
    "    ! tar -xf /tmp/tvm.tar.gz --strip-components=4 --directory /tvm\n",
    "    ! ls -la /tvm\n",
    "    ! bash /tvm/package.sh\n",
    "    ! pip install onnx \n",
    "    # ! pip install https://download.pytorch.org/whl/cu100/torch-1.1.0-cp36-cp36m-linux_x86_64.whl\n",
    "    # ! pip3 install https://download.pytorch.org/whl/cu100/torchvision-0.3.0-cp36-cp36m-linux_x86_64.whl\n",
    "    # Add TVM to the Python path.\n",
    "    import sys\n",
    "    import os\n",
    "    sys.path.append('/tvm/python')\n",
    "    sys.path.append('/tvm/topi/python')\n",
    "    sys.path.append('/tvm/vta/python')\n",
    "    os.environ['TVM_HOME'] = '/tvm'\n",
    "else:\n",
    "    print(\"Notebook executing locally, skipping Colab setup ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "LOr6_sEnU34L",
    "outputId": "2d483539-1ba2-46bd-babb-bd4c08eeab6b"
   },
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    ! cd /; git clone https://github.com/uwsampl/relay-aot\n",
    "    sys.path.append('/relay-aot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7Jg5xV0v0Nuh"
   },
   "source": [
    "# Relay: an Extensible Deep Learning IR\n",
    "\n",
    "Last year TVM introduced Relay IR â€“ a second generation high-level IR for deep learning. \n",
    "\n",
    "Relay's design comes from a simple insight that the critical difference between regular IRs\n",
    "and deep learning IRs are the primitive values they manipulate. Relay is designed using well\n",
    "known insights from the programming languages community coupled with TVM's existing \n",
    "infrastructure to provide state of the art performance. \n",
    "\n",
    "If you are familiar with ideas from programming languages or existing computation graph\n",
    "representations, we will connect Relay to your existing knowledge during this tutorial.\n",
    "\n",
    "We will first cover the design of Relay then elaborate on how one can use it to \n",
    "accomplish a wide variety of tasks. This portion of the tutorial is focused  \n",
    "on Relay, but Relay will be discussed throughout the day day. Relay serves\n",
    "as the interface layer to TVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TB0vjCD4U34O"
   },
   "outputs": [],
   "source": [
    "import tvm\n",
    "from tvm import relay\n",
    "import tvm.relay.testing\n",
    "from tvm.relay.expr_functor import ExprMutator, ExprVisitor\n",
    "import torch\n",
    "import torchvision\n",
    "import onnx\n",
    "import numpy\n",
    "import aot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2m7pb6CRU34Q"
   },
   "source": [
    "## Language \n",
    "\n",
    "We will first briefly introduce the concepts of Relay below.\n",
    "You can find a full language specification [here](https://docs.tvm.ai/langref/index.html).\n",
    "\n",
    "### Variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aJ1r0s_6U34R"
   },
   "outputs": [],
   "source": [
    "# A single Relay variable, the string is just a hint\n",
    "x = relay.var('x')\n",
    "\n",
    "# A Relay variable with a different dtype, defaults to float32.\n",
    "x = relay.var('x', dtype='int32')\n",
    "\n",
    "# A Relay variable with a different shape.\n",
    "x = relay.var('x', shape=(10, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_wXlQZW6U34T"
   },
   "source": [
    "### Operators\n",
    "\n",
    "Relay provides high performance operators defined in TVM that implement the primitive operations needed by deep learning applications. Operators can be applied to arguments just like regular Python or C++ functions. Common arithemetic operations are provided both via names and operator overloading.\n",
    "\n",
    "Variables can be used to construct Relay *expressions* which replace the concept of graphs present in previous frameworks. A Relay expression can be viewed much like a graph with extra functionality as we will see as we go\n",
    "forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "JfzpVxqXU34U",
    "outputId": "3a9180c9-558b-4227-f36c-c1c3627011d6"
   },
   "outputs": [],
   "source": [
    "w = relay.op.add(x, x)\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "OFbnGQ2eU34Y",
    "outputId": "eb226ce7-0570-465d-d14f-70254350ba47"
   },
   "outputs": [],
   "source": [
    "z = x + x\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dt8wxT3nU34a"
   },
   "source": [
    "### Functions\n",
    "\n",
    "The fundamental packaging of computation in Relay is the function. A function is a combination of a set of inputs\n",
    "and a Relay expression. Relay functions are no different than ones in programming languages today. They replace named subgraphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "iQZAveEQU34b",
    "outputId": "c39e3c46-1e34-4107-b330-73608ac46e88"
   },
   "outputs": [],
   "source": [
    "f = relay.Function([x], z)\n",
    "print(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QOStPwpiU34e"
   },
   "source": [
    "### Module\n",
    "\n",
    "Finally, we can give functions a global name and package many of them together into a module. When we add a function to the module, it will be type checked before hand.\n",
    "\n",
    "When we print the module, you can see the program annotated with all type information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "zKZXCIw2U34f",
    "outputId": "bacba3d9-c594-424f-c30a-6fc6e44ee584"
   },
   "outputs": [],
   "source": [
    "mod = relay.Module({})\n",
    "fname = relay.GlobalVar('f')\n",
    "mod[fname] = f\n",
    "\n",
    "print(mod)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ToSdC5UDU34h"
   },
   "source": [
    "## Frontends\n",
    "\n",
    "Relay comes with a variety of frontends and supports most major frameworks, including TensorFlow, PyTorch, MxNet, ONNX, Keras and Caffe2.\n",
    "\n",
    "Below we provide a couple examples of using these frontends to import models into Relay.\n",
    "\n",
    "You can find specific tutorials on deploying pretrained models below:  \n",
    "\n",
    "- [ONNX](https://docs.tvm.ai/tutorials/frontend/from_onnx.html#sphx-glr-tutorials-frontend-from-onnx-py)\n",
    "- [TensorFlow](https://docs.tvm.ai/tutorials/frontend/from_tensorflow.html#sphx-glr-tutorials-frontend-from-tensorflow-py)\n",
    "- [Keras](https://docs.tvm.ai/tutorials/frontend/from_keras.html#sphx-glr-tutorials-frontend-from-keras-py)\n",
    "- [PyTorch](https://tvm.ai/2019/05/30/pytorch-frontend)\n",
    "- [Caffe2](https://docs.tvm.ai/tutorials/frontend/from_caffe2.html#sphx-glr-tutorials-frontend-from-caffe2-py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 3454
    },
    "colab_type": "code",
    "id": "QAvjJ_8jU34i",
    "outputId": "6606d361-87d2-42bd-850e-c6347d409dae"
   },
   "outputs": [],
   "source": [
    "torch_resnet18 = torchvision.models.resnet18()\n",
    "dummy_input = torch.randn(1, 3, 224, 224)\n",
    "torch.onnx.export(torch_resnet18, dummy_input, \"resnet.onnx\",  export_params=True, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1466
    },
    "colab_type": "code",
    "id": "1cdUL9-QU34l",
    "outputId": "f6201f6c-7959-4fb0-fec2-1a151f6eaf69"
   },
   "outputs": [],
   "source": [
    "onnx_resnet18 = onnx.load('resnet.onnx')\n",
    "func, params = relay.frontend.from_onnx(onnx_resnet18, shape={ '0': (1, 3, 224, 224) })\n",
    "# print(func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OrlhhY-1U34p"
   },
   "source": [
    "## Text Format\n",
    "\n",
    "Relay has a textual representation that can be used to write and print programs. The textual format is still being stablized but can still be of great use today. For example, instead of providing inscrutable graph representations of programs, we can produce human readable output by default.\n",
    "\n",
    "There are a few different ways to interact with the textual format. The first is to just print out a Relay expression as we have seen above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 258
    },
    "colab_type": "code",
    "id": "ePXjh0_sU34p",
    "outputId": "6adaaec3-be0b-4adb-e4d7-a90d6a95eec8"
   },
   "outputs": [],
   "source": [
    "mlp, params = relay.testing.mlp.get_workload(1)\n",
    "print(mlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UR5GOFbuU34s"
   },
   "source": [
    "By default the pretty printer renders the code without metadata. The pretty printer omits metadata because the metadata, which contains information such as constants, is often unreadable. For instance, imagine we perform an optimization, such as inlining the parameters into the program for further optimization. The metadata would include 100s of megabytes of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1567
    },
    "colab_type": "code",
    "id": "oked2HUKU34t",
    "outputId": "220befce-e007-4354-e5d4-84afbddd50ec"
   },
   "outputs": [],
   "source": [
    "def inline_parameters(expr, params):\n",
    "    param_map = dict((p.name_hint, p) for p in expr.params)\n",
    "    params = dict((param_map[k], relay.const(params[k])) for k in params)\n",
    "    new_body = relay.bind(expr.body, params)\n",
    "    return relay.Function(relay.ir_pass.free_vars(new_body), new_body)\n",
    "\n",
    "inline_mlp = inline_parameters(mlp, params)\n",
    "print(inline_mlp.astext(show_meta_data=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lhGOXL7_U34x"
   },
   "source": [
    "Relay's pretty printer also allows users to attach debugging output and metadata to the IR. For example you can see the type information on the example above, but we can also customize the annotation process by passing a callback for annotating nodes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_j-DUyUJU34y"
   },
   "outputs": [],
   "source": [
    "i = 0 \n",
    "def ann(*args):\n",
    "    global i\n",
    "    i += 1\n",
    "    return f\" <expression: {i}>\"\n",
    "\n",
    "print(mlp.astext(show_meta_data=True, annotate=ann))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-glsaLErU341"
   },
   "source": [
    "Finally, an important part of the Relay text format is the ability to load Relay code \n",
    "like a normal programming language. We can use the Relay parser to parse code. We actually do this to define the Relay\n",
    "*prelude* the small standard library of utilities shipped in Relay. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fbZIbuM0U342"
   },
   "source": [
    "## Executing Relay\n",
    "\n",
    "Now that we have looked at how to write and manipulate a Relay program, we will show you how to run one. Relay has multiple execution mechanisms: a custom *debug interpreter* for Relay which can be used for experimentation and debugging, TVM's older graph runtime, the existing execution mechanism, and Relay VM. Relay VM is a newly designed execution mechanism with the goal to smoothly execute all of Relay efficiently. \n",
    "\n",
    "We provide a high level interface which imposes some wrapping overhead but enables quick experimentation with each API. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LeW_s703U343"
   },
   "outputs": [],
   "source": [
    "mod = relay.Module()\n",
    "debug_ex = relay.create_executor('debug', mod=mod)\n",
    "graph_ex = relay.create_executor('graph', mod=mod)\n",
    "vm_ex = relay.create_executor('vm', mod=mod)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oVEZhnJvU346"
   },
   "source": [
    "Each executor can be used to evaluate an expression given a Relay module, in this case we use an empty module, and will just evaluate the same expression, a MLP, using each one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MLx57gDlU347"
   },
   "outputs": [],
   "source": [
    "debug_mlp = debug_ex.evaluate(mlp)\n",
    "graph_mlp = graph_ex.evaluate(mlp)\n",
    "vm_mlp = vm_ex.evaluate(mlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rAhlzHJtU34-"
   },
   "source": [
    "Each one can be called like a normal Python function with the inputs passed as positional arguments and the parameters as keyword arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "--KiuJ3oU34_"
   },
   "outputs": [],
   "source": [
    "data = numpy.random.rand(1, 1, 28, 28).astype('float32')\n",
    "print(\"Debug: \", debug_mlp(data, **params))\n",
    "print(\"Graph: \", graph_mlp(data, **params))\n",
    "print(\"VM: \", vm_mlp(data, **params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9eOM1C7JU35C"
   },
   "source": [
    "### Virtual Machine\n",
    "\n",
    "The Relay virtual machine is worth highlighting, it is a brand new runtime mechanism for Relay which is beginning to stablize. We encourage new users to check it out and provide feedback on its design, perforomance, and more. The VM enables support for non-standard aspects of Relay including control-flow, closures, and data structures.\n",
    "\n",
    "For more details on the virtual machine see [here](https://github.com/dmlc/tvm/issues/2810)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AvSE40I2U35C"
   },
   "source": [
    "## Pass Manager\n",
    "\n",
    "Relay has a flexible and configurable pass manager with an elegant API that be used to easily compose and schedule pass pipelines. We believe an easy-to-configure pipeline is important to enable intelligent exploration between a variety of \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GPfL2d1XU35D"
   },
   "outputs": [],
   "source": [
    "resnet, params = relay.testing.resnet.get_workload()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_5EuQ9OYU35F"
   },
   "source": [
    "The Relay pass infrastructure takes inspiration from both traditional designs from the compiler world as well as APIs from deep learning. In particular our pass manager uses a uniform interface similar to the `Layer` abstraction provided by many deep learning frameworks. Our passes are defined as parametric functions from Module to Module.\n",
    "\n",
    "For example we define another type of pass, `Sequential` which runs a set of passes serially over a module. Below we construct a compiler pipeline from a set of transformations, we will first type check, perform cosntant folding, eliminate common subexpressions, and the perform layout transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JM5en7erU35G"
   },
   "outputs": [],
   "source": [
    "seq = relay.transform.Sequential([\n",
    "    relay.transform.InferType(),\n",
    "    relay.transform.FoldConstant(),\n",
    "    relay.transform.EliminateCommonSubexpr(),\n",
    "    relay.transform.AlterOpLayout()\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nn49gt_hU35I"
   },
   "source": [
    "Finally we will construct a module using the ResNet as our *main* function. To do so we:\n",
    "\n",
    "- Use `relay.build_config` to setup the scoped build config, these scoped configs can be used with Python's `with` statement. After the block ends the configuration will be reset to the defaults.\n",
    "- Use `tvm.target.create` in a scoped fashion as well to set the target to `llvm`.\n",
    "- We can now optimize the module with this information and get it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 232
    },
    "colab_type": "code",
    "id": "r96221MeU35I",
    "outputId": "2d6eface-c581-4ccb-81f6-f8362a43bd9a"
   },
   "outputs": [],
   "source": [
    "mod = relay.Module({\"main\": resnet})\n",
    "with relay.build_config(opt_level=3):\n",
    "    with tvm.target.create(\"llvm\"):\n",
    "        mod = seq(mod)\n",
    "        print(mod[\"main\"])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hHxw6IEyU35K"
   },
   "source": [
    "## Optimizations\n",
    "\n",
    "Defining optimizations to transform your program is straight forward and easy to do in Relay.\n",
    "\n",
    "For example let's define a simple constant evaluator for Relay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IsConstant(ExprVisitor):\n",
    "    def __init__(self):\n",
    "        self.is_const = True\n",
    "        super().__init__()\n",
    "    \n",
    "    def visit_const(self, const):\n",
    "        self.is_const = self.is_const and True\n",
    "    \n",
    "    def visit_op(self, op):\n",
    "        self.is_const = self.is_const and True\n",
    "        \n",
    "    def visit_call(self, call):\n",
    "        for arg in call.args:\n",
    "            self.visit(arg)\n",
    "        \n",
    "        self.visit(call.op)\n",
    "        \n",
    "    def visit(self, expr):\n",
    "        if not isinstance(expr, (relay.Op, relay.Call, relay.Constant)):\n",
    "            self.is_const = False\n",
    "        \n",
    "        return super().visit(expr)\n",
    "    \n",
    "def is_const(expr):\n",
    "    isc = IsConstant()\n",
    "    isc.visit(expr)\n",
    "    return isc.is_const"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q2rQZ_vwU35L"
   },
   "outputs": [],
   "source": [
    "class ConstantFolder(ExprMutator):\n",
    "    def visit_call(self, call):\n",
    "        if is_const(call):\n",
    "            ex = relay.create_executor('debug')\n",
    "            cexpr = ex.evaluate(call)\n",
    "            return relay.const(cexpr.asnumpy())\n",
    "        else:\n",
    "            return super().visit_call(call)\n",
    "\n",
    "def const_fold(expr):\n",
    "    return ConstantFolder().visit(expr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1 = relay.const(numpy.ones((10, 1), 'float32'))\n",
    "c2 = relay.const(numpy.ones((1, 1), 'float32'))\n",
    "e = c1 + c2\n",
    "print(\"Input Expr: \", e)\n",
    "const = const_fold(e)\n",
    "assert isinstance(const, relay.Constant)\n",
    "print(\"Output: \", const.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NDWqCNNxU35N"
   },
   "source": [
    "## Quantization\n",
    "\n",
    "Relay supports an automatic quantization framework which can be used to implement a variety of quantization schemes. You can find more details about it in our recent paper as well as it being used in action here.\n",
    "\n",
    "Below we will apply the default quantization scheme to ResNet-18."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I0nWbO3kU35N",
    "outputId": "08484804-e648-4ecf-9d8b-7f9e87e80a3c"
   },
   "outputs": [],
   "source": [
    "resnet, params = relay.testing.resnet.get_workload()\n",
    "\n",
    "with relay.quantize.qconfig():\n",
    "    qresnet = relay.quantize.quantize(resnet, params)\n",
    "    print(qresnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "puVpH87g0Nui"
   },
   "source": [
    "## Heterogeneous Execution\n",
    "\n",
    "Relay supports a high-level interface for scheduling computation across multiple heterogeneous devices. An interesting property of this pass is that it is not special. It is built using Relay's standard machinery for\n",
    "passes. \n",
    "\n",
    "We implement this by using an annotation to mark which computations we would like to schedule on each device, \n",
    "and a pass inserts all the appropriate calls to synchronize memory across devices. \n",
    "\n",
    "The below pass uses this machinery to schedule all convolutions onto the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5sfd3Svu0Nui"
   },
   "outputs": [],
   "source": [
    "class ScheduleConv2d(ExprMutator):\n",
    "    def __init__(self, device):\n",
    "        self.device = device\n",
    "        super().__init__()\n",
    "\n",
    "    def visit_call(self, expr):\n",
    "        visit = super().visit_call(expr)\n",
    "        if expr.op == tvm.relay.op.get(\"nn.conv2d\"):\n",
    "            return relay.annotation.on_device(visit, self.device)\n",
    "        else:\n",
    "            return visit\n",
    "\n",
    "def schedule_conv2d_on_gpu(expr):\n",
    "    sched = ScheduleConv2d(tvm.gpu(0))\n",
    "    return sched.visit(expr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c0LWbjv7U35S"
   },
   "source": [
    "We can grab a model, we provide a few basic models in Relay's testing library. By default when printing a model we will see it rendered in Relay's textual format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0W0PfkDuU35T"
   },
   "outputs": [],
   "source": [
    "# We can grab a model, we provide a few basic models in Relay's testing library.\n",
    "resnet, params = relay.testing.resnet.get_workload()\n",
    "print(resnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xekqKiGRU35V"
   },
   "source": [
    "We can now run the customized pass we defined above to schedule individual convolutions on the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iVwCKQ0zU35W"
   },
   "outputs": [],
   "source": [
    "resnet = schedule_conv2d_on_gpu(resnet)\n",
    "print(resnet)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J-L7ylTdU35Y"
   },
   "source": [
    "We can later rewrite away the device annotations to insert the copies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CZw0zQJYU35Y"
   },
   "outputs": [],
   "source": [
    "resnet = relay.ir_pass.rewrite_annotated_ops(resnet, 0)\n",
    "print(resnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5iuQyQ0xU35a"
   },
   "source": [
    "Finally we will look at a couple case studies of what can be built using Relay. We will first look at how Relay is used as a backend in PyTorch integration, then how Relay can be used to compile a model down to traditional hardware, and finally how it can be used to support a custom accelerator, VTA, which we will dicuss in detail today."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pfioMVRkU35a"
   },
   "source": [
    "## Ahead of time compilation\n",
    "\n",
    "An example of what can be built using Relay can be found with the [ahead of time compiler](https://github.com/uwsampl/relay-aot). \n",
    "\n",
    "We have already installed the compiler above, we can simply import it and use it below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OgJZf2PxU35b"
   },
   "outputs": [],
   "source": [
    "mlp, params = relay.testing.mlp.get_workload(1)\n",
    "cmlp = aot.compile(mlp, relay.Module({}), ctx=tvm.cpu(0), tgt=tvm.target.create('llvm'))\n",
    "data = numpy.ones((1, 1, 28, 28)).astype('float32')\n",
    "print(cmlp(data, *params.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VM-TQ1Y8U35e"
   },
   "source": [
    "## PyTorch Integration\n",
    "\n",
    "Recently Facebook engineers have begun to integrate TVM into PyTorch. The are using PyTorch's JIT functionality to generate Relay code which is then optimized and deployed. This support is currently being integrated into mainline TVM, and installed from [here](https://github.com/pytorch/tvm). \n",
    "\n",
    "Initial results are promising and a recent writeup on the design can be found on our [blog](https://tvm.ai/2019/05/30/pytorch-frontend). \n",
    "\n",
    "![results](https://i.imgur.com/KfJ7oas.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YQZoqcaXU35f"
   },
   "source": [
    "```python\n",
    "def add(a, b, c):\n",
    "    return a + b + c\n",
    "\n",
    "# via tracing\n",
    "relay_graph = torch_tvm.to_relay(add, inputs)\n",
    "\n",
    "@torch.jit.script\n",
    "def mul(a, b, c):\n",
    "    return a * b * c\n",
    "\n",
    "# via script\n",
    "relay_graph = torch_tvm.to_relay(mul, inputs)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "02_TVM_Tutorial_Relay.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
