{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SeiRi-zc0NuZ"
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/uwsampl/tutorial/blob/master/notebook/02_TVM_Tutorial_Relay.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BYNJGUSR0Nub"
   },
   "source": [
    "Please run the following block to ensure TVM is setup for *this notebook*, each notebook may have its own runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1020
    },
    "colab_type": "code",
    "id": "bdiA6WVx0Nuc",
    "outputId": "949d0040-80a4-43e5-b1e9-04445c3307c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook executing locally, skipping Colab setup ...\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "  import google.colab\n",
    "  IN_COLAB = True\n",
    "except:\n",
    "  IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "    ! gsutil cp \"gs://tvm-fcrc-binariesd5fce43e-8373-11e9-bfb6-0242ac1c0002/tvm.tar.gz\" /tmp/tvm.tar.gz\n",
    "    ! mkdir -p /tvm\n",
    "    ! tar -xf /tmp/tvm.tar.gz --strip-components=4 --directory /tvm\n",
    "    ! ls -la /tvm\n",
    "    ! bash /tvm/package.sh\n",
    "    ! export TVM_HOME=/tvm\n",
    "    # Add TVM to the Python path.\n",
    "    import sys\n",
    "    sys.path.append('/tvm/python')\n",
    "    sys.path.append('/tvm/topi/python')\n",
    "    sys.path.append('/tvm/vta/python')\n",
    "else:\n",
    "    print(\"Notebook executing locally, skipping Colab setup ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    ! cd /; git clone https://github.com/uwsampl/relay-aot\n",
    "    sys.path.append('/relay-aot')\n",
    "else:\n",
    "    import aot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7Jg5xV0v0Nuh"
   },
   "source": [
    "# Relay: an Extensible Deep Learning IR\n",
    "\n",
    "Last year TVM introduced Relay IR â€“ a second generation high-level IR for deep learning. \n",
    "\n",
    "Relay's design comes from a simple insight that the critical difference between regular IRs\n",
    "and deep learning IRs are the primitive values they manipulate. Relay is designed using well\n",
    "known insights from the programming languages community coupled with TVM's existing \n",
    "infrastructure to provide state of the art performance. \n",
    "\n",
    "If you are familiar with ideas from programming languages or existing computation graph\n",
    "representations, we will connect Relay to your existing knowledge during this tutorial.\n",
    "\n",
    "We will first cover the design of Relay then elaborate on how one can use it to \n",
    "accomplish a wide variety of tasks. This portion of the tutorial is focused  \n",
    "on Relay, but Relay will be discussed throughout the day day. Relay serves\n",
    "as the interface layer to TVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tvm\n",
    "from tvm import relay\n",
    "import tvm.relay.testing\n",
    "from tvm.relay.expr_functor import ExprMutator\n",
    "import torch\n",
    "import torchvision\n",
    "import onnx\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language \n",
    "\n",
    "We will first briefly introduce the concepts of Relay below.\n",
    "You can find a full language specification [here](https://docs.tvm.ai/langref/index.html).\n",
    "\n",
    "### Variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A single Relay variable, the string is just a hint\n",
    "x = relay.var('x')\n",
    "\n",
    "# A Relay variable with a different dtype, defaults to float32.\n",
    "x = relay.var('x', dtype='int32')\n",
    "\n",
    "# A Relay variable with a different shape.\n",
    "x = relay.var('x', shape=(10, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Operators\n",
    "\n",
    "Relay provides high performance operators defined in TVM that implement the primitive operations needed by deep learning applications. Operators can be applied to arguments just like regular Python or C++ functions. Common arithemetic operations are provided both via names and operator overloading.\n",
    "\n",
    "Variables can be used to construct Relay *expressions* which replace the concept of graphs present in previous frameworks. A Relay expression can be viewed much like a graph with extra functionality as we will see as we go\n",
    "forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v0.0.1\n",
      "free_var %x: Tensor[(10, 1), float32]\n",
      "add(%x, %x)\n"
     ]
    }
   ],
   "source": [
    "w = relay.op.add(x, x)\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v0.0.1\n",
      "free_var %x: Tensor[(10, 1), float32]\n",
      "add(%x, %x)\n"
     ]
    }
   ],
   "source": [
    "z = x + x\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions\n",
    "\n",
    "The fundamental packaging of computation in Relay is the function. A function is a combination of a set of inputs\n",
    "and a Relay expression. Relay functions are no different than ones in programming languages today. They replace named subgraphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v0.0.1\n",
      "fn (%x: Tensor[(10, 1), float32]) {\n",
      "  add(%x, %x)\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "f = relay.Function([x], z)\n",
    "print(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Module\n",
    "\n",
    "Finally, we can give functions a global name and package many of them together into a module. When we add a function to the module, it will be type checked before hand.\n",
    "\n",
    "When we print the module, you can see the program annotated with all type information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v0.0.1\n",
      "def @f(%x: Tensor[(10, 1), float32]) -> Tensor[(10, 1), float32] {\n",
      "  add(%x, %x) /* ty=Tensor[(10, 1), float32] */\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mod = relay.Module({})\n",
    "fname = relay.GlobalVar('f')\n",
    "mod[fname] = f\n",
    "\n",
    "print(mod)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frontends\n",
    "\n",
    "Relay comes with a variety of frontends and supports most major frameworks, including TensorFlow, PyTorch, MxNet, ONNX, Keras and Caffe2.\n",
    "\n",
    "Below we provide a couple examples of using these frontends to import models into Relay.\n",
    "\n",
    "You can find specific tutorials on deploying pretrained models below:  \n",
    "\n",
    "- [ONNX](https://docs.tvm.ai/tutorials/frontend/from_onnx.html#sphx-glr-tutorials-frontend-from-onnx-py)\n",
    "- [TensorFlow](https://docs.tvm.ai/tutorials/frontend/from_tensorflow.html#sphx-glr-tutorials-frontend-from-tensorflow-py)\n",
    "- [Keras](https://docs.tvm.ai/tutorials/frontend/from_keras.html#sphx-glr-tutorials-frontend-from-keras-py)\n",
    "- [PyTorch](https://tvm.ai/2019/05/30/pytorch-frontend)\n",
    "- [Caffe2](https://docs.tvm.ai/tutorials/frontend/from_caffe2.html#sphx-glr-tutorials-frontend-from-caffe2-py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph(%0 : Float(10, 3, 224, 224),\n",
      "      %conv1.weight : Float(64, 3, 7, 7),\n",
      "      %bn1.weight : Float(64),\n",
      "      %bn1.bias : Float(64),\n",
      "      %bn1.running_mean : Float(64),\n",
      "      %bn1.running_var : Float(64),\n",
      "      %bn1.num_batches_tracked : Long(),\n",
      "      %layer1.0.conv1.weight : Float(64, 64, 3, 3),\n",
      "      %layer1.0.bn1.weight : Float(64),\n",
      "      %layer1.0.bn1.bias : Float(64),\n",
      "      %layer1.0.bn1.running_mean : Float(64),\n",
      "      %layer1.0.bn1.running_var : Float(64),\n",
      "      %layer1.0.bn1.num_batches_tracked : Long(),\n",
      "      %layer1.0.conv2.weight : Float(64, 64, 3, 3),\n",
      "      %layer1.0.bn2.weight : Float(64),\n",
      "      %layer1.0.bn2.bias : Float(64),\n",
      "      %layer1.0.bn2.running_mean : Float(64),\n",
      "      %layer1.0.bn2.running_var : Float(64),\n",
      "      %layer1.0.bn2.num_batches_tracked : Long(),\n",
      "      %layer1.1.conv1.weight : Float(64, 64, 3, 3),\n",
      "      %layer1.1.bn1.weight : Float(64),\n",
      "      %layer1.1.bn1.bias : Float(64),\n",
      "      %layer1.1.bn1.running_mean : Float(64),\n",
      "      %layer1.1.bn1.running_var : Float(64),\n",
      "      %layer1.1.bn1.num_batches_tracked : Long(),\n",
      "      %layer1.1.conv2.weight : Float(64, 64, 3, 3),\n",
      "      %layer1.1.bn2.weight : Float(64),\n",
      "      %layer1.1.bn2.bias : Float(64),\n",
      "      %layer1.1.bn2.running_mean : Float(64),\n",
      "      %layer1.1.bn2.running_var : Float(64),\n",
      "      %layer1.1.bn2.num_batches_tracked : Long(),\n",
      "      %layer2.0.conv1.weight : Float(128, 64, 3, 3),\n",
      "      %layer2.0.bn1.weight : Float(128),\n",
      "      %layer2.0.bn1.bias : Float(128),\n",
      "      %layer2.0.bn1.running_mean : Float(128),\n",
      "      %layer2.0.bn1.running_var : Float(128),\n",
      "      %layer2.0.bn1.num_batches_tracked : Long(),\n",
      "      %layer2.0.conv2.weight : Float(128, 128, 3, 3),\n",
      "      %layer2.0.bn2.weight : Float(128),\n",
      "      %layer2.0.bn2.bias : Float(128),\n",
      "      %layer2.0.bn2.running_mean : Float(128),\n",
      "      %layer2.0.bn2.running_var : Float(128),\n",
      "      %layer2.0.bn2.num_batches_tracked : Long(),\n",
      "      %layer2.0.downsample.0.weight : Float(128, 64, 1, 1),\n",
      "      %layer2.0.downsample.1.weight : Float(128),\n",
      "      %layer2.0.downsample.1.bias : Float(128),\n",
      "      %layer2.0.downsample.1.running_mean : Float(128),\n",
      "      %layer2.0.downsample.1.running_var : Float(128),\n",
      "      %layer2.0.downsample.1.num_batches_tracked : Long(),\n",
      "      %layer2.1.conv1.weight : Float(128, 128, 3, 3),\n",
      "      %layer2.1.bn1.weight : Float(128),\n",
      "      %layer2.1.bn1.bias : Float(128),\n",
      "      %layer2.1.bn1.running_mean : Float(128),\n",
      "      %layer2.1.bn1.running_var : Float(128),\n",
      "      %layer2.1.bn1.num_batches_tracked : Long(),\n",
      "      %layer2.1.conv2.weight : Float(128, 128, 3, 3),\n",
      "      %layer2.1.bn2.weight : Float(128),\n",
      "      %layer2.1.bn2.bias : Float(128),\n",
      "      %layer2.1.bn2.running_mean : Float(128),\n",
      "      %layer2.1.bn2.running_var : Float(128),\n",
      "      %layer2.1.bn2.num_batches_tracked : Long(),\n",
      "      %layer3.0.conv1.weight : Float(256, 128, 3, 3),\n",
      "      %layer3.0.bn1.weight : Float(256),\n",
      "      %layer3.0.bn1.bias : Float(256),\n",
      "      %layer3.0.bn1.running_mean : Float(256),\n",
      "      %layer3.0.bn1.running_var : Float(256),\n",
      "      %layer3.0.bn1.num_batches_tracked : Long(),\n",
      "      %layer3.0.conv2.weight : Float(256, 256, 3, 3),\n",
      "      %layer3.0.bn2.weight : Float(256),\n",
      "      %layer3.0.bn2.bias : Float(256),\n",
      "      %layer3.0.bn2.running_mean : Float(256),\n",
      "      %layer3.0.bn2.running_var : Float(256),\n",
      "      %layer3.0.bn2.num_batches_tracked : Long(),\n",
      "      %layer3.0.downsample.0.weight : Float(256, 128, 1, 1),\n",
      "      %layer3.0.downsample.1.weight : Float(256),\n",
      "      %layer3.0.downsample.1.bias : Float(256),\n",
      "      %layer3.0.downsample.1.running_mean : Float(256),\n",
      "      %layer3.0.downsample.1.running_var : Float(256),\n",
      "      %layer3.0.downsample.1.num_batches_tracked : Long(),\n",
      "      %layer3.1.conv1.weight : Float(256, 256, 3, 3),\n",
      "      %layer3.1.bn1.weight : Float(256),\n",
      "      %layer3.1.bn1.bias : Float(256),\n",
      "      %layer3.1.bn1.running_mean : Float(256),\n",
      "      %layer3.1.bn1.running_var : Float(256),\n",
      "      %layer3.1.bn1.num_batches_tracked : Long(),\n",
      "      %layer3.1.conv2.weight : Float(256, 256, 3, 3),\n",
      "      %layer3.1.bn2.weight : Float(256),\n",
      "      %layer3.1.bn2.bias : Float(256),\n",
      "      %layer3.1.bn2.running_mean : Float(256),\n",
      "      %layer3.1.bn2.running_var : Float(256),\n",
      "      %layer3.1.bn2.num_batches_tracked : Long(),\n",
      "      %layer4.0.conv1.weight : Float(512, 256, 3, 3),\n",
      "      %layer4.0.bn1.weight : Float(512),\n",
      "      %layer4.0.bn1.bias : Float(512),\n",
      "      %layer4.0.bn1.running_mean : Float(512),\n",
      "      %layer4.0.bn1.running_var : Float(512),\n",
      "      %layer4.0.bn1.num_batches_tracked : Long(),\n",
      "      %layer4.0.conv2.weight : Float(512, 512, 3, 3),\n",
      "      %layer4.0.bn2.weight : Float(512),\n",
      "      %layer4.0.bn2.bias : Float(512),\n",
      "      %layer4.0.bn2.running_mean : Float(512),\n",
      "      %layer4.0.bn2.running_var : Float(512),\n",
      "      %layer4.0.bn2.num_batches_tracked : Long(),\n",
      "      %layer4.0.downsample.0.weight : Float(512, 256, 1, 1),\n",
      "      %layer4.0.downsample.1.weight : Float(512),\n",
      "      %layer4.0.downsample.1.bias : Float(512),\n",
      "      %layer4.0.downsample.1.running_mean : Float(512),\n",
      "      %layer4.0.downsample.1.running_var : Float(512),\n",
      "      %layer4.0.downsample.1.num_batches_tracked : Long(),\n",
      "      %layer4.1.conv1.weight : Float(512, 512, 3, 3),\n",
      "      %layer4.1.bn1.weight : Float(512),\n",
      "      %layer4.1.bn1.bias : Float(512),\n",
      "      %layer4.1.bn1.running_mean : Float(512),\n",
      "      %layer4.1.bn1.running_var : Float(512),\n",
      "      %layer4.1.bn1.num_batches_tracked : Long(),\n",
      "      %layer4.1.conv2.weight : Float(512, 512, 3, 3),\n",
      "      %layer4.1.bn2.weight : Float(512),\n",
      "      %layer4.1.bn2.bias : Float(512),\n",
      "      %layer4.1.bn2.running_mean : Float(512),\n",
      "      %layer4.1.bn2.running_var : Float(512),\n",
      "      %layer4.1.bn2.num_batches_tracked : Long(),\n",
      "      %fc.weight : Float(1000, 512),\n",
      "      %fc.bias : Float(1000)):\n",
      "  %123 : Float(10, 64, 112, 112) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[7, 7], pads=[3, 3, 3, 3], strides=[2, 2]](%0, %conv1.weight), scope: ResNet/Conv2d[conv1]\n",
      "  %124 : Float(10, 64, 112, 112) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%123, %bn1.weight, %bn1.bias, %bn1.running_mean, %bn1.running_var), scope: ResNet/BatchNorm2d[bn1]\n",
      "  %125 : Float(10, 64, 112, 112) = onnx::Relu(%124), scope: ResNet/ReLU[relu]\n",
      "  %126 : Float(10, 64, 56, 56) = onnx::MaxPool[kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[2, 2]](%125), scope: ResNet/MaxPool2d[maxpool]\n",
      "  %127 : Float(10, 64, 56, 56) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%126, %layer1.0.conv1.weight), scope: ResNet/Sequential[layer1]/BasicBlock[0]/Conv2d[conv1]\n",
      "  %128 : Float(10, 64, 56, 56) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%127, %layer1.0.bn1.weight, %layer1.0.bn1.bias, %layer1.0.bn1.running_mean, %layer1.0.bn1.running_var), scope: ResNet/Sequential[layer1]/BasicBlock[0]/BatchNorm2d[bn1]\n",
      "  %129 : Float(10, 64, 56, 56) = onnx::Relu(%128), scope: ResNet/Sequential[layer1]/BasicBlock[0]/ReLU[relu]\n",
      "  %130 : Float(10, 64, 56, 56) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%129, %layer1.0.conv2.weight), scope: ResNet/Sequential[layer1]/BasicBlock[0]/Conv2d[conv2]\n",
      "  %131 : Float(10, 64, 56, 56) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%130, %layer1.0.bn2.weight, %layer1.0.bn2.bias, %layer1.0.bn2.running_mean, %layer1.0.bn2.running_var), scope: ResNet/Sequential[layer1]/BasicBlock[0]/BatchNorm2d[bn2]\n",
      "  %132 : Float(10, 64, 56, 56) = onnx::Add(%131, %126), scope: ResNet/Sequential[layer1]/BasicBlock[0]\n",
      "  %133 : Float(10, 64, 56, 56) = onnx::Relu(%132), scope: ResNet/Sequential[layer1]/BasicBlock[0]/ReLU[relu]\n",
      "  %134 : Float(10, 64, 56, 56) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%133, %layer1.1.conv1.weight), scope: ResNet/Sequential[layer1]/BasicBlock[1]/Conv2d[conv1]\n",
      "  %135 : Float(10, 64, 56, 56) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%134, %layer1.1.bn1.weight, %layer1.1.bn1.bias, %layer1.1.bn1.running_mean, %layer1.1.bn1.running_var), scope: ResNet/Sequential[layer1]/BasicBlock[1]/BatchNorm2d[bn1]\n",
      "  %136 : Float(10, 64, 56, 56) = onnx::Relu(%135), scope: ResNet/Sequential[layer1]/BasicBlock[1]/ReLU[relu]\n",
      "  %137 : Float(10, 64, 56, 56) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%136, %layer1.1.conv2.weight), scope: ResNet/Sequential[layer1]/BasicBlock[1]/Conv2d[conv2]\n",
      "  %138 : Float(10, 64, 56, 56) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%137, %layer1.1.bn2.weight, %layer1.1.bn2.bias, %layer1.1.bn2.running_mean, %layer1.1.bn2.running_var), scope: ResNet/Sequential[layer1]/BasicBlock[1]/BatchNorm2d[bn2]\n",
      "  %139 : Float(10, 64, 56, 56) = onnx::Add(%138, %133), scope: ResNet/Sequential[layer1]/BasicBlock[1]\n",
      "  %140 : Float(10, 64, 56, 56) = onnx::Relu(%139), scope: ResNet/Sequential[layer1]/BasicBlock[1]/ReLU[relu]\n",
      "  %141 : Float(10, 128, 28, 28) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[2, 2]](%140, %layer2.0.conv1.weight), scope: ResNet/Sequential[layer2]/BasicBlock[0]/Conv2d[conv1]\n",
      "  %142 : Float(10, 128, 28, 28) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%141, %layer2.0.bn1.weight, %layer2.0.bn1.bias, %layer2.0.bn1.running_mean, %layer2.0.bn1.running_var), scope: ResNet/Sequential[layer2]/BasicBlock[0]/BatchNorm2d[bn1]\n",
      "  %143 : Float(10, 128, 28, 28) = onnx::Relu(%142), scope: ResNet/Sequential[layer2]/BasicBlock[0]/ReLU[relu]\n",
      "  %144 : Float(10, 128, 28, 28) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%143, %layer2.0.conv2.weight), scope: ResNet/Sequential[layer2]/BasicBlock[0]/Conv2d[conv2]\n",
      "  %145 : Float(10, 128, 28, 28) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%144, %layer2.0.bn2.weight, %layer2.0.bn2.bias, %layer2.0.bn2.running_mean, %layer2.0.bn2.running_var), scope: ResNet/Sequential[layer2]/BasicBlock[0]/BatchNorm2d[bn2]\n",
      "  %146 : Float(10, 128, 28, 28) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[2, 2]](%140, %layer2.0.downsample.0.weight), scope: ResNet/Sequential[layer2]/BasicBlock[0]/Sequential[downsample]/Conv2d[0]\n",
      "  %147 : Float(10, 128, 28, 28) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%146, %layer2.0.downsample.1.weight, %layer2.0.downsample.1.bias, %layer2.0.downsample.1.running_mean, %layer2.0.downsample.1.running_var), scope: ResNet/Sequential[layer2]/BasicBlock[0]/Sequential[downsample]/BatchNorm2d[1]\n",
      "  %148 : Float(10, 128, 28, 28) = onnx::Add(%145, %147), scope: ResNet/Sequential[layer2]/BasicBlock[0]\n",
      "  %149 : Float(10, 128, 28, 28) = onnx::Relu(%148), scope: ResNet/Sequential[layer2]/BasicBlock[0]/ReLU[relu]\n",
      "  %150 : Float(10, 128, 28, 28) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%149, %layer2.1.conv1.weight), scope: ResNet/Sequential[layer2]/BasicBlock[1]/Conv2d[conv1]\n",
      "  %151 : Float(10, 128, 28, 28) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%150, %layer2.1.bn1.weight, %layer2.1.bn1.bias, %layer2.1.bn1.running_mean, %layer2.1.bn1.running_var), scope: ResNet/Sequential[layer2]/BasicBlock[1]/BatchNorm2d[bn1]\n",
      "  %152 : Float(10, 128, 28, 28) = onnx::Relu(%151), scope: ResNet/Sequential[layer2]/BasicBlock[1]/ReLU[relu]\n",
      "  %153 : Float(10, 128, 28, 28) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%152, %layer2.1.conv2.weight), scope: ResNet/Sequential[layer2]/BasicBlock[1]/Conv2d[conv2]\n",
      "  %154 : Float(10, 128, 28, 28) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%153, %layer2.1.bn2.weight, %layer2.1.bn2.bias, %layer2.1.bn2.running_mean, %layer2.1.bn2.running_var), scope: ResNet/Sequential[layer2]/BasicBlock[1]/BatchNorm2d[bn2]\n",
      "  %155 : Float(10, 128, 28, 28) = onnx::Add(%154, %149), scope: ResNet/Sequential[layer2]/BasicBlock[1]\n",
      "  %156 : Float(10, 128, 28, 28) = onnx::Relu(%155), scope: ResNet/Sequential[layer2]/BasicBlock[1]/ReLU[relu]\n",
      "  %157 : Float(10, 256, 14, 14) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[2, 2]](%156, %layer3.0.conv1.weight), scope: ResNet/Sequential[layer3]/BasicBlock[0]/Conv2d[conv1]\n",
      "  %158 : Float(10, 256, 14, 14) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%157, %layer3.0.bn1.weight, %layer3.0.bn1.bias, %layer3.0.bn1.running_mean, %layer3.0.bn1.running_var), scope: ResNet/Sequential[layer3]/BasicBlock[0]/BatchNorm2d[bn1]\n",
      "  %159 : Float(10, 256, 14, 14) = onnx::Relu(%158), scope: ResNet/Sequential[layer3]/BasicBlock[0]/ReLU[relu]\n",
      "  %160 : Float(10, 256, 14, 14) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%159, %layer3.0.conv2.weight), scope: ResNet/Sequential[layer3]/BasicBlock[0]/Conv2d[conv2]\n",
      "  %161 : Float(10, 256, 14, 14) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%160, %layer3.0.bn2.weight, %layer3.0.bn2.bias, %layer3.0.bn2.running_mean, %layer3.0.bn2.running_var), scope: ResNet/Sequential[layer3]/BasicBlock[0]/BatchNorm2d[bn2]\n",
      "  %162 : Float(10, 256, 14, 14) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[2, 2]](%156, %layer3.0.downsample.0.weight), scope: ResNet/Sequential[layer3]/BasicBlock[0]/Sequential[downsample]/Conv2d[0]\n",
      "  %163 : Float(10, 256, 14, 14) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%162, %layer3.0.downsample.1.weight, %layer3.0.downsample.1.bias, %layer3.0.downsample.1.running_mean, %layer3.0.downsample.1.running_var), scope: ResNet/Sequential[layer3]/BasicBlock[0]/Sequential[downsample]/BatchNorm2d[1]\n",
      "  %164 : Float(10, 256, 14, 14) = onnx::Add(%161, %163), scope: ResNet/Sequential[layer3]/BasicBlock[0]\n",
      "  %165 : Float(10, 256, 14, 14) = onnx::Relu(%164), scope: ResNet/Sequential[layer3]/BasicBlock[0]/ReLU[relu]\n",
      "  %166 : Float(10, 256, 14, 14) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%165, %layer3.1.conv1.weight), scope: ResNet/Sequential[layer3]/BasicBlock[1]/Conv2d[conv1]\n",
      "  %167 : Float(10, 256, 14, 14) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%166, %layer3.1.bn1.weight, %layer3.1.bn1.bias, %layer3.1.bn1.running_mean, %layer3.1.bn1.running_var), scope: ResNet/Sequential[layer3]/BasicBlock[1]/BatchNorm2d[bn1]\n",
      "  %168 : Float(10, 256, 14, 14) = onnx::Relu(%167), scope: ResNet/Sequential[layer3]/BasicBlock[1]/ReLU[relu]\n",
      "  %169 : Float(10, 256, 14, 14) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%168, %layer3.1.conv2.weight), scope: ResNet/Sequential[layer3]/BasicBlock[1]/Conv2d[conv2]\n",
      "  %170 : Float(10, 256, 14, 14) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%169, %layer3.1.bn2.weight, %layer3.1.bn2.bias, %layer3.1.bn2.running_mean, %layer3.1.bn2.running_var), scope: ResNet/Sequential[layer3]/BasicBlock[1]/BatchNorm2d[bn2]\n",
      "  %171 : Float(10, 256, 14, 14) = onnx::Add(%170, %165), scope: ResNet/Sequential[layer3]/BasicBlock[1]\n",
      "  %172 : Float(10, 256, 14, 14) = onnx::Relu(%171), scope: ResNet/Sequential[layer3]/BasicBlock[1]/ReLU[relu]\n",
      "  %173 : Float(10, 512, 7, 7) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[2, 2]](%172, %layer4.0.conv1.weight), scope: ResNet/Sequential[layer4]/BasicBlock[0]/Conv2d[conv1]\n",
      "  %174 : Float(10, 512, 7, 7) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%173, %layer4.0.bn1.weight, %layer4.0.bn1.bias, %layer4.0.bn1.running_mean, %layer4.0.bn1.running_var), scope: ResNet/Sequential[layer4]/BasicBlock[0]/BatchNorm2d[bn1]\n",
      "  %175 : Float(10, 512, 7, 7) = onnx::Relu(%174), scope: ResNet/Sequential[layer4]/BasicBlock[0]/ReLU[relu]\n",
      "  %176 : Float(10, 512, 7, 7) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%175, %layer4.0.conv2.weight), scope: ResNet/Sequential[layer4]/BasicBlock[0]/Conv2d[conv2]\n",
      "  %177 : Float(10, 512, 7, 7) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%176, %layer4.0.bn2.weight, %layer4.0.bn2.bias, %layer4.0.bn2.running_mean, %layer4.0.bn2.running_var), scope: ResNet/Sequential[layer4]/BasicBlock[0]/BatchNorm2d[bn2]\n",
      "  %178 : Float(10, 512, 7, 7) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[2, 2]](%172, %layer4.0.downsample.0.weight), scope: ResNet/Sequential[layer4]/BasicBlock[0]/Sequential[downsample]/Conv2d[0]\n",
      "  %179 : Float(10, 512, 7, 7) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%178, %layer4.0.downsample.1.weight, %layer4.0.downsample.1.bias, %layer4.0.downsample.1.running_mean, %layer4.0.downsample.1.running_var), scope: ResNet/Sequential[layer4]/BasicBlock[0]/Sequential[downsample]/BatchNorm2d[1]\n",
      "  %180 : Float(10, 512, 7, 7) = onnx::Add(%177, %179), scope: ResNet/Sequential[layer4]/BasicBlock[0]\n",
      "  %181 : Float(10, 512, 7, 7) = onnx::Relu(%180), scope: ResNet/Sequential[layer4]/BasicBlock[0]/ReLU[relu]\n",
      "  %182 : Float(10, 512, 7, 7) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%181, %layer4.1.conv1.weight), scope: ResNet/Sequential[layer4]/BasicBlock[1]/Conv2d[conv1]\n",
      "  %183 : Float(10, 512, 7, 7) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%182, %layer4.1.bn1.weight, %layer4.1.bn1.bias, %layer4.1.bn1.running_mean, %layer4.1.bn1.running_var), scope: ResNet/Sequential[layer4]/BasicBlock[1]/BatchNorm2d[bn1]\n",
      "  %184 : Float(10, 512, 7, 7) = onnx::Relu(%183), scope: ResNet/Sequential[layer4]/BasicBlock[1]/ReLU[relu]\n",
      "  %185 : Float(10, 512, 7, 7) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%184, %layer4.1.conv2.weight), scope: ResNet/Sequential[layer4]/BasicBlock[1]/Conv2d[conv2]\n",
      "  %186 : Float(10, 512, 7, 7) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%185, %layer4.1.bn2.weight, %layer4.1.bn2.bias, %layer4.1.bn2.running_mean, %layer4.1.bn2.running_var), scope: ResNet/Sequential[layer4]/BasicBlock[1]/BatchNorm2d[bn2]\n",
      "  %187 : Float(10, 512, 7, 7) = onnx::Add(%186, %181), scope: ResNet/Sequential[layer4]/BasicBlock[1]\n",
      "  %188 : Float(10, 512, 7, 7) = onnx::Relu(%187), scope: ResNet/Sequential[layer4]/BasicBlock[1]/ReLU[relu]\n",
      "  %189 : Float(10, 512, 1, 1) = onnx::GlobalAveragePool(%188), scope: ResNet/AdaptiveAvgPool2d[avgpool]\n",
      "  %190 : Long() = onnx::Constant[value={0}](), scope: ResNet\n",
      "  %191 : Tensor = onnx::Shape(%189), scope: ResNet\n",
      "  %192 : Long() = onnx::Gather[axis=0](%191, %190), scope: ResNet\n",
      "  %193 : Long() = onnx::Constant[value={-1}](), scope: ResNet\n",
      "  %194 : Tensor = onnx::Unsqueeze[axes=[0]](%192)\n",
      "  %195 : Tensor = onnx::Unsqueeze[axes=[0]](%193)\n",
      "  %196 : Tensor = onnx::Concat[axis=0](%194, %195)\n",
      "  %197 : Float(10, 512) = onnx::Reshape(%189, %196), scope: ResNet\n",
      "  %198 : Float(10, 1000) = onnx::Gemm[alpha=1, beta=1, transB=1](%197, %fc.weight, %fc.bias), scope: ResNet/Linear[fc]\n",
      "  return (%198)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "torch_resnet18 = torchvision.models.resnet18()\n",
    "dummy_input = torch.randn(10, 3, 224, 224)\n",
    "torch.onnx.export(torch_resnet18, dummy_input, \"resnet.onnx\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Attribute momentum is ignored in relay.sym.batch_norm\n",
      "WARNING:root:Attribute momentum is ignored in relay.sym.batch_norm\n",
      "WARNING:root:Attribute momentum is ignored in relay.sym.batch_norm\n",
      "WARNING:root:Attribute momentum is ignored in relay.sym.batch_norm\n",
      "WARNING:root:Attribute momentum is ignored in relay.sym.batch_norm\n",
      "WARNING:root:Attribute momentum is ignored in relay.sym.batch_norm\n",
      "WARNING:root:Attribute momentum is ignored in relay.sym.batch_norm\n",
      "WARNING:root:Attribute momentum is ignored in relay.sym.batch_norm\n",
      "WARNING:root:Attribute momentum is ignored in relay.sym.batch_norm\n",
      "WARNING:root:Attribute momentum is ignored in relay.sym.batch_norm\n",
      "WARNING:root:Attribute momentum is ignored in relay.sym.batch_norm\n",
      "WARNING:root:Attribute momentum is ignored in relay.sym.batch_norm\n",
      "WARNING:root:Attribute momentum is ignored in relay.sym.batch_norm\n",
      "WARNING:root:Attribute momentum is ignored in relay.sym.batch_norm\n",
      "WARNING:root:Attribute momentum is ignored in relay.sym.batch_norm\n",
      "WARNING:root:Attribute momentum is ignored in relay.sym.batch_norm\n",
      "WARNING:root:Attribute momentum is ignored in relay.sym.batch_norm\n",
      "WARNING:root:Attribute momentum is ignored in relay.sym.batch_norm\n",
      "WARNING:root:Attribute momentum is ignored in relay.sym.batch_norm\n",
      "WARNING:root:Attribute momentum is ignored in relay.sym.batch_norm\n",
      "WARNING:root:Infering Reshape argument by precompute\n"
     ]
    },
    {
     "ename": "TVMError",
     "evalue": "Traceback (most recent call last):\n  [bt] (8) 9   libtvm.dylib                        0x000000011b88ffdd tvm::relay::backend::RelayBuildModule::GetFunction(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::shared_ptr<tvm::runtime::ModuleNode> const&)::'lambda1'(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const + 429\n  [bt] (7) 8   libtvm.dylib                        0x000000011b8901f2 tvm::relay::backend::RelayBuildModule::Build(tvm::relay::Function, tvm::Map<tvm::Integer, tvm::Target, void, void> const&, tvm::Target const&) + 130\n  [bt] (6) 7   libtvm.dylib                        0x000000011b8903f9 tvm::relay::backend::RelayBuildModule::BuildRelay(tvm::relay::Function, std::__1::unordered_map<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, tvm::runtime::NDArray, std::__1::hash<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > >, std::__1::equal_to<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > >, std::__1::allocator<std::__1::pair<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const, tvm::runtime::NDArray> > > const&) + 345\n  [bt] (5) 6   libtvm.dylib                        0x000000011b95e2ca tvm::relay::ModuleNode::FromExpr(tvm::relay::Expr const&, tvm::Map<tvm::relay::GlobalVar, tvm::relay::Function, void, void> const&) + 938\n  [bt] (4) 5   libtvm.dylib                        0x000000011b95cb17 tvm::relay::ModuleNode::Add(tvm::relay::GlobalVar const&, tvm::relay::Function const&, bool) + 151\n  [bt] (3) 4   libtvm.dylib                        0x000000011bc3b5b8 tvm::relay::InferType(tvm::relay::Function const&, tvm::relay::Module const&, tvm::relay::GlobalVar const&) + 472\n  [bt] (2) 3   libtvm.dylib                        0x000000011bc3a687 tvm::relay::TypeInferencer::Infer(tvm::relay::Expr) + 135\n  [bt] (1) 2   libtvm.dylib                        0x000000011b928e23 tvm::relay::ErrorReporter::RenderErrors(tvm::relay::Module const&, bool) + 5555\n  [bt] (0) 1   libtvm.dylib                        0x000000011b4df949 dmlc::LogMessageFatal::~LogMessageFatal() + 57\n  [bt] (8) 9   libtvm.dylib                        0x000000011b95cb17 tvm::relay::ModuleNode::Add(tvm::relay::GlobalVar const&, tvm::relay::Function const&, bool) + 151\n  [bt] (7) 8   libtvm.dylib                        0x000000011bc3b5b8 tvm::relay::InferType(tvm::relay::Function const&, tvm::relay::Module const&, tvm::relay::GlobalVar const&) + 472\n  [bt] (6) 7   libtvm.dylib                        0x000000011bc3a66b tvm::relay::TypeInferencer::Infer(tvm::relay::Expr) + 107\n  [bt] (5) 6   libtvm.dylib                        0x000000011bc5705a tvm::relay::TypeSolver::Solve() + 1114\n  [bt] (4) 5   libtvm.dylib                        0x000000011bc576f8 tvm::TypedEnvFunc<bool (tvm::Array<tvm::relay::Type, void> const&, int, tvm::Attrs const&, tvm::relay::TypeReporter const&)>::operator()(tvm::Array<tvm::relay::Type, void> const&, int, tvm::Attrs const&, tvm::relay::TypeReporter const&) const + 328\n  [bt] (3) 4   libtvm.dylib                        0x000000011b9b0ba9 std::__1::__function::__func<void tvm::runtime::TypedPackedFunc<bool (tvm::Array<tvm::relay::Type, void> const&, int, tvm::Attrs const&, tvm::relay::TypeReporter const&)>::AssignTypedLambda<bool (*)(tvm::Array<tvm::relay::Type, void> const&, int, tvm::Attrs const&, tvm::relay::TypeReporter const&)>(bool (*)(tvm::Array<tvm::relay::Type, void> const&, int, tvm::Attrs const&, tvm::relay::TypeReporter const&))::'lambda'(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*), std::__1::allocator<void tvm::runtime::TypedPackedFunc<bool (tvm::Array<tvm::relay::Type, void> const&, int, tvm::Attrs const&, tvm::relay::TypeReporter const&)>::AssignTypedLambda<bool (*)(tvm::Array<tvm::relay::Type, void> const&, int, tvm::Attrs const&, tvm::relay::TypeReporter const&)>(bool (*)(tvm::Array<tvm::relay::Type, void> const&, int, tvm::Attrs const&, tvm::relay::TypeReporter const&))::'lambda'(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)>, void (tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)>::operator()(tvm::runtime::TVMArgs&&, tvm::runtime::TVMRetValue*&&) + 137\n  [bt] (2) 3   libtvm.dylib                        0x000000011b9b0c4f void tvm::runtime::detail::unpack_call_dispatcher<bool, 0, 4, bool (*)(tvm::Array<tvm::relay::Type, void> const&, int, tvm::Attrs const&, tvm::relay::TypeReporter const&)>::run<tvm::runtime::TVMArgValue, tvm::runtime::TVMArgValue, tvm::runtime::TVMArgValue, tvm::runtime::TVMArgValue>(bool (* const&)(tvm::Array<tvm::relay::Type, void> const&, int, tvm::Attrs const&, tvm::relay::TypeReporter const&), tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*, tvm::runtime::TVMArgValue&&, tvm::runtime::TVMArgValue&&, tvm::runtime::TVMArgValue&&, tvm::runtime::TVMArgValue&&) + 95\n  [bt] (1) 2   libtvm.dylib                        0x000000011babe79e tvm::relay::ConcatenateRel(tvm::Array<tvm::relay::Type, void> const&, int, tvm::Attrs const&, tvm::relay::TypeReporter const&) + 1918\n  [bt] (0) 1   libtvm.dylib                        0x000000011b4df949 dmlc::LogMessageFatal::~LogMessageFatal() + 57\n  File \"/Users/jroesch/Git/tvm/src/relay/ir/error.cc\", line 132\nTVMError: \u001b[1m\nError(s) have occurred. We have annotated the program with them:\n\n\u001b[0m\u001b[1mIn `main`: \n\u001b[0mv0.0.1\nfn (%v0: Tensor[(10, 3, 224, 224), float32]) {\n  %0 = nn.conv2d(%v0, meta[relay.Constant][0], strides=[2, 2], padding=[3, 3], kernel_size=[7, 7])\n  %1 = nn.batch_norm(%0, meta[relay.Constant][1], meta[relay.Constant][2], meta[relay.Constant][3], meta[relay.Constant][4], epsilon=1e-05)\n  %2 = %1.0\n  %3 = nn.relu(%2)\n  %4 = nn.max_pool2d(%3, pool_size=[3, 3], strides=[2, 2], padding=[1, 1])\n  %5 = nn.conv2d(%4, meta[relay.Constant][5], padding=[1, 1], kernel_size=[3, 3])\n  %6 = nn.batch_norm(%5, meta[relay.Constant][6], meta[relay.Constant][7], meta[relay.Constant][8], meta[relay.Constant][9], epsilon=1e-05)\n  %7 = %6.0\n  %8 = nn.relu(%7)\n  %9 = nn.conv2d(%8, meta[relay.Constant][10], padding=[1, 1], kernel_size=[3, 3])\n  %10 = nn.batch_norm(%9, meta[relay.Constant][11], meta[relay.Constant][12], meta[relay.Constant][13], meta[relay.Constant][14], epsilon=1e-05)\n  %11 = %10.0\n  %12 = add(%11, %4)\n  %13 = nn.relu(%12)\n  %14 = nn.conv2d(%13, meta[relay.Constant][15], padding=[1, 1], kernel_size=[3, 3])\n  %15 = nn.batch_norm(%14, meta[relay.Constant][16], meta[relay.Constant][17], meta[relay.Constant][18], meta[relay.Constant][19], epsilon=1e-05)\n  %16 = %15.0\n  %17 = nn.relu(%16)\n  %18 = nn.conv2d(%17, meta[relay.Constant][20], padding=[1, 1], kernel_size=[3, 3])\n  %19 = nn.batch_norm(%18, meta[relay.Constant][21], meta[relay.Constant][22], meta[relay.Constant][23], meta[relay.Constant][24], epsilon=1e-05)\n  %20 = %19.0\n  %21 = add(%20, %13)\n  %22 = nn.relu(%21)\n  %23 = nn.conv2d(%22, meta[relay.Constant][25], strides=[2, 2], padding=[1, 1], kernel_size=[3, 3])\n  %24 = nn.batch_norm(%23, meta[relay.Constant][26], meta[relay.Constant][27], meta[relay.Constant][28], meta[relay.Constant][29], epsilon=1e-05)\n  %25 = %24.0\n  %26 = nn.relu(%25)\n  %27 = nn.conv2d(%26, meta[relay.Constant][30], padding=[1, 1], kernel_size=[3, 3])\n  %28 = nn.batch_norm(%27, meta[relay.Constant][31], meta[relay.Constant][32], meta[relay.Constant][33], meta[relay.Constant][34], epsilon=1e-05)\n  %29 = %28.0\n  %30 = nn.conv2d(%22, meta[relay.Constant][35], strides=[2, 2], kernel_size=[1, 1])\n  %31 = nn.batch_norm(%30, meta[relay.Constant][36], meta[relay.Constant][37], meta[relay.Constant][38], meta[relay.Constant][39], epsilon=1e-05)\n  %32 = %31.0\n  %33 = add(%29, %32)\n  %34 = nn.relu(%33)\n  %35 = nn.conv2d(%34, meta[relay.Constant][40], padding=[1, 1], kernel_size=[3, 3])\n  %36 = nn.batch_norm(%35, meta[relay.Constant][41], meta[relay.Constant][42], meta[relay.Constant][43], meta[relay.Constant][44], epsilon=1e-05)\n  %37 = %36.0\n  %38 = nn.relu(%37)\n  %39 = nn.conv2d(%38, meta[relay.Constant][45], padding=[1, 1], kernel_size=[3, 3])\n  %40 = nn.batch_norm(%39, meta[relay.Constant][46], meta[relay.Constant][47], meta[relay.Constant][48], meta[relay.Constant][49], epsilon=1e-05)\n  %41 = %40.0\n  %42 = add(%41, %34)\n  %43 = nn.relu(%42)\n  %44 = nn.conv2d(%43, meta[relay.Constant][50], strides=[2, 2], padding=[1, 1], kernel_size=[3, 3])\n  %45 = nn.batch_norm(%44, meta[relay.Constant][51], meta[relay.Constant][52], meta[relay.Constant][53], meta[relay.Constant][54], epsilon=1e-05)\n  %46 = %45.0\n  %47 = nn.relu(%46)\n  %48 = nn.conv2d(%47, meta[relay.Constant][55], padding=[1, 1], kernel_size=[3, 3])\n  %49 = nn.batch_norm(%48, meta[relay.Constant][56], meta[relay.Constant][57], meta[relay.Constant][58], meta[relay.Constant][59], epsilon=1e-05)\n  %50 = %49.0\n  %51 = nn.conv2d(%43, meta[relay.Constant][60], strides=[2, 2], kernel_size=[1, 1])\n  %52 = nn.batch_norm(%51, meta[relay.Constant][61], meta[relay.Constant][62], meta[relay.Constant][63], meta[relay.Constant][64], epsilon=1e-05)\n  %53 = %52.0\n  %54 = add(%50, %53)\n  %55 = nn.relu(%54)\n  %56 = nn.conv2d(%55, meta[relay.Constant][65], padding=[1, 1], kernel_size=[3, 3])\n  %57 = nn.batch_norm(%56, meta[relay.Constant][66], meta[relay.Constant][67], meta[relay.Constant][68], meta[relay.Constant][69], epsilon=1e-05)\n  %58 = %57.0\n  %59 = nn.relu(%58)\n  %60 = nn.conv2d(%59, meta[relay.Constant][70], padding=[1, 1], kernel_size=[3, 3])\n  %61 = nn.batch_norm(%60, meta[relay.Constant][71], meta[relay.Constant][72], meta[relay.Constant][73], meta[relay.Constant][74], epsilon=1e-05)\n  %62 = %61.0\n  %63 = add(%62, %55)\n  %64 = nn.relu(%63)\n  %65 = nn.conv2d(%64, meta[relay.Constant][75], strides=[2, 2], padding=[1, 1], kernel_size=[3, 3])\n  %66 = nn.batch_norm(%65, meta[relay.Constant][76], meta[relay.Constant][77], meta[relay.Constant][78], meta[relay.Constant][79], epsilon=1e-05)\n  %67 = %66.0\n  %68 = nn.relu(%67)\n  %69 = nn.conv2d(%68, meta[relay.Constant][80], padding=[1, 1], kernel_size=[3, 3])\n  %70 = nn.batch_norm(%69, meta[relay.Constant][81], meta[relay.Constant][82], meta[relay.Constant][83], meta[relay.Constant][84], epsilon=1e-05)\n  %71 = %70.0\n  %72 = nn.conv2d(%64, meta[relay.Constant][85], strides=[2, 2], kernel_size=[1, 1])\n  %73 = nn.batch_norm(%72, meta[relay.Constant][86], meta[relay.Constant][87], meta[relay.Constant][88], meta[relay.Constant][89], epsilon=1e-05)\n  %74 = %73.0\n  %75 = add(%71, %74)\n  %76 = nn.relu(%75)\n  %77 = nn.conv2d(%76, meta[relay.Constant][90], padding=[1, 1], kernel_size=[3, 3])\n  %78 = nn.batch_norm(%77, meta[relay.Constant][91], meta[relay.Constant][92], meta[relay.Constant][93], meta[relay.Constant][94], epsilon=1e-05)\n  %79 = %78.0\n  %80 = nn.relu(%79)\n  %81 = nn.conv2d(%80, meta[relay.Constant][95], padding=[1, 1], kernel_size=[3, 3])\n  %82 = nn.batch_norm(%81, meta[relay.Constant][96], meta[relay.Constant][97], meta[relay.Constant][98], meta[relay.Constant][99], epsilon=1e-05)\n  %83 = %82.0\n  %84 = add(%83, %76)\n  %85 = nn.relu(%84)\n  %86 = nn.global_avg_pool2d(%85)\n  %87 = shape_of(%86, dtype=\"int32\")\n  %88 = take(%87, int64(0), axis=0)\n  %89 = expand_dims(%88, axis=0)\n  %90 = expand_dims(int64(-1), axis=0)\n  %91 = (%89, %90)\n  concatenate(%91)\u001b[31man internal invariant was violated while typechecking your program [17:22:35] /Users/jroesch/Git/tvm/src/relay/op/tensor/transform.cc:204: Check failed: e_dtype == dtype (int64 vs. int32) : relay.concatenate requires all tensors have the same dtype\n; \u001b[39m\n}\n// meta data omitted. you can use show_meta_data=True to include meta data\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTVMError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-a586a9ffca5e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0monnx_resnet18\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0monnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'resnet.onnx'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrelay\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrontend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_onnx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0monnx_resnet18\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m \u001b[0;34m'0'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m224\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Git/tvm/python/tvm/relay/frontend/onnx.py\u001b[0m in \u001b[0;36mfrom_onnx\u001b[0;34m(model, shape, dtype)\u001b[0m\n\u001b[1;32m   1244\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1245\u001b[0m         \u001b[0mopset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1246\u001b[0;31m     \u001b[0msym\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_onnx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1247\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msym\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Git/tvm/python/tvm/relay/frontend/onnx.py\u001b[0m in \u001b[0;36mfrom_onnx\u001b[0;34m(self, graph, opset)\u001b[0m\n\u001b[1;32m   1072\u001b[0m                 \u001b[0mattr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tvm_custom'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'name'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1073\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1074\u001b[0;31m                 \u001b[0mop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_operator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1075\u001b[0m                 \u001b[0mnode_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fix_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1076\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_expr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTupleWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Git/tvm/python/tvm/relay/frontend/onnx.py\u001b[0m in \u001b[0;36m_convert_operator\u001b[0;34m(self, op_name, inputs, attrs, opset)\u001b[0m\n\u001b[1;32m   1178\u001b[0m             \u001b[0msym\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_relay_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mop_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconvert_map\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0msym\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mop_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m             raise NotImplementedError(\n",
      "\u001b[0;32m~/Git/tvm/python/tvm/relay/frontend/onnx.py\u001b[0m in \u001b[0;36m_impl_v1\u001b[0;34m(cls, inputs, attr, params)\u001b[0m\n\u001b[1;32m    415\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_expr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mir_pass\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfree_vars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelay\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt_level\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 417\u001b[0;31m                 \u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelay\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"llvm\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    418\u001b[0m             \u001b[0mctx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"llvm\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m             \u001b[0;32mfrom\u001b[0m \u001b[0mtvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgraph_runtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Git/tvm/python/tvm/relay/build_module.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(func, target, target_host, params)\u001b[0m\n\u001b[1;32m    194\u001b[0m         \u001b[0mbld_mod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBuildModule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m         graph_json, mod, params = bld_mod.build(func, target, target_host,\n\u001b[0;32m--> 196\u001b[0;31m                                                 params)\n\u001b[0m\u001b[1;32m    197\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_json\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Git/tvm/python/tvm/relay/build_module.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, func, target, target_host, params)\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0;31m# Build the function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_host\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m         \u001b[0;31m# Get artifacts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0mgraph_json\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Git/tvm/python/tvm/_ffi/_ctypes/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    207\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtcodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m                 ctypes.byref(ret_val), ctypes.byref(ret_tcode)) != 0:\n\u001b[0;32m--> 209\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mget_last_ffi_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m         \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m         \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTVMError\u001b[0m: Traceback (most recent call last):\n  [bt] (8) 9   libtvm.dylib                        0x000000011b88ffdd tvm::relay::backend::RelayBuildModule::GetFunction(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::shared_ptr<tvm::runtime::ModuleNode> const&)::'lambda1'(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const + 429\n  [bt] (7) 8   libtvm.dylib                        0x000000011b8901f2 tvm::relay::backend::RelayBuildModule::Build(tvm::relay::Function, tvm::Map<tvm::Integer, tvm::Target, void, void> const&, tvm::Target const&) + 130\n  [bt] (6) 7   libtvm.dylib                        0x000000011b8903f9 tvm::relay::backend::RelayBuildModule::BuildRelay(tvm::relay::Function, std::__1::unordered_map<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, tvm::runtime::NDArray, std::__1::hash<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > >, std::__1::equal_to<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > >, std::__1::allocator<std::__1::pair<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const, tvm::runtime::NDArray> > > const&) + 345\n  [bt] (5) 6   libtvm.dylib                        0x000000011b95e2ca tvm::relay::ModuleNode::FromExpr(tvm::relay::Expr const&, tvm::Map<tvm::relay::GlobalVar, tvm::relay::Function, void, void> const&) + 938\n  [bt] (4) 5   libtvm.dylib                        0x000000011b95cb17 tvm::relay::ModuleNode::Add(tvm::relay::GlobalVar const&, tvm::relay::Function const&, bool) + 151\n  [bt] (3) 4   libtvm.dylib                        0x000000011bc3b5b8 tvm::relay::InferType(tvm::relay::Function const&, tvm::relay::Module const&, tvm::relay::GlobalVar const&) + 472\n  [bt] (2) 3   libtvm.dylib                        0x000000011bc3a687 tvm::relay::TypeInferencer::Infer(tvm::relay::Expr) + 135\n  [bt] (1) 2   libtvm.dylib                        0x000000011b928e23 tvm::relay::ErrorReporter::RenderErrors(tvm::relay::Module const&, bool) + 5555\n  [bt] (0) 1   libtvm.dylib                        0x000000011b4df949 dmlc::LogMessageFatal::~LogMessageFatal() + 57\n  [bt] (8) 9   libtvm.dylib                        0x000000011b95cb17 tvm::relay::ModuleNode::Add(tvm::relay::GlobalVar const&, tvm::relay::Function const&, bool) + 151\n  [bt] (7) 8   libtvm.dylib                        0x000000011bc3b5b8 tvm::relay::InferType(tvm::relay::Function const&, tvm::relay::Module const&, tvm::relay::GlobalVar const&) + 472\n  [bt] (6) 7   libtvm.dylib                        0x000000011bc3a66b tvm::relay::TypeInferencer::Infer(tvm::relay::Expr) + 107\n  [bt] (5) 6   libtvm.dylib                        0x000000011bc5705a tvm::relay::TypeSolver::Solve() + 1114\n  [bt] (4) 5   libtvm.dylib                        0x000000011bc576f8 tvm::TypedEnvFunc<bool (tvm::Array<tvm::relay::Type, void> const&, int, tvm::Attrs const&, tvm::relay::TypeReporter const&)>::operator()(tvm::Array<tvm::relay::Type, void> const&, int, tvm::Attrs const&, tvm::relay::TypeReporter const&) const + 328\n  [bt] (3) 4   libtvm.dylib                        0x000000011b9b0ba9 std::__1::__function::__func<void tvm::runtime::TypedPackedFunc<bool (tvm::Array<tvm::relay::Type, void> const&, int, tvm::Attrs const&, tvm::relay::TypeReporter const&)>::AssignTypedLambda<bool (*)(tvm::Array<tvm::relay::Type, void> const&, int, tvm::Attrs const&, tvm::relay::TypeReporter const&)>(bool (*)(tvm::Array<tvm::relay::Type, void> const&, int, tvm::Attrs const&, tvm::relay::TypeReporter const&))::'lambda'(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*), std::__1::allocator<void tvm::runtime::TypedPackedFunc<bool (tvm::Array<tvm::relay::Type, void> const&, int, tvm::Attrs const&, tvm::relay::TypeReporter const&)>::AssignTypedLambda<bool (*)(tvm::Array<tvm::relay::Type, void> const&, int, tvm::Attrs const&, tvm::relay::TypeReporter const&)>(bool (*)(tvm::Array<tvm::relay::Type, void> const&, int, tvm::Attrs const&, tvm::relay::TypeReporter const&))::'lambda'(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)>, void (tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)>::operator()(tvm::runtime::TVMArgs&&, tvm::runtime::TVMRetValue*&&) + 137\n  [bt] (2) 3   libtvm.dylib                        0x000000011b9b0c4f void tvm::runtime::detail::unpack_call_dispatcher<bool, 0, 4, bool (*)(tvm::Array<tvm::relay::Type, void> const&, int, tvm::Attrs const&, tvm::relay::TypeReporter const&)>::run<tvm::runtime::TVMArgValue, tvm::runtime::TVMArgValue, tvm::runtime::TVMArgValue, tvm::runtime::TVMArgValue>(bool (* const&)(tvm::Array<tvm::relay::Type, void> const&, int, tvm::Attrs const&, tvm::relay::TypeReporter const&), tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*, tvm::runtime::TVMArgValue&&, tvm::runtime::TVMArgValue&&, tvm::runtime::TVMArgValue&&, tvm::runtime::TVMArgValue&&) + 95\n  [bt] (1) 2   libtvm.dylib                        0x000000011babe79e tvm::relay::ConcatenateRel(tvm::Array<tvm::relay::Type, void> const&, int, tvm::Attrs const&, tvm::relay::TypeReporter const&) + 1918\n  [bt] (0) 1   libtvm.dylib                        0x000000011b4df949 dmlc::LogMessageFatal::~LogMessageFatal() + 57\n  File \"/Users/jroesch/Git/tvm/src/relay/ir/error.cc\", line 132\nTVMError: \u001b[1m\nError(s) have occurred. We have annotated the program with them:\n\n\u001b[0m\u001b[1mIn `main`: \n\u001b[0mv0.0.1\nfn (%v0: Tensor[(10, 3, 224, 224), float32]) {\n  %0 = nn.conv2d(%v0, meta[relay.Constant][0], strides=[2, 2], padding=[3, 3], kernel_size=[7, 7])\n  %1 = nn.batch_norm(%0, meta[relay.Constant][1], meta[relay.Constant][2], meta[relay.Constant][3], meta[relay.Constant][4], epsilon=1e-05)\n  %2 = %1.0\n  %3 = nn.relu(%2)\n  %4 = nn.max_pool2d(%3, pool_size=[3, 3], strides=[2, 2], padding=[1, 1])\n  %5 = nn.conv2d(%4, meta[relay.Constant][5], padding=[1, 1], kernel_size=[3, 3])\n  %6 = nn.batch_norm(%5, meta[relay.Constant][6], meta[relay.Constant][7], meta[relay.Constant][8], meta[relay.Constant][9], epsilon=1e-05)\n  %7 = %6.0\n  %8 = nn.relu(%7)\n  %9 = nn.conv2d(%8, meta[relay.Constant][10], padding=[1, 1], kernel_size=[3, 3])\n  %10 = nn.batch_norm(%9, meta[relay.Constant][11], meta[relay.Constant][12], meta[relay.Constant][13], meta[relay.Constant][14], epsilon=1e-05)\n  %11 = %10.0\n  %12 = add(%11, %4)\n  %13 = nn.relu(%12)\n  %14 = nn.conv2d(%13, meta[relay.Constant][15], padding=[1, 1], kernel_size=[3, 3])\n  %15 = nn.batch_norm(%14, meta[relay.Constant][16], meta[relay.Constant][17], meta[relay.Constant][18], meta[relay.Constant][19], epsilon=1e-05)\n  %16 = %15.0\n  %17 = nn.relu(%16)\n  %18 = nn.conv2d(%17, meta[relay.Constant][20], padding=[1, 1], kernel_size=[3, 3])\n  %19 = nn.batch_norm(%18, meta[relay.Constant][21], meta[relay.Constant][22], meta[relay.Constant][23], meta[relay.Constant][24], epsilon=1e-05)\n  %20 = %19.0\n  %21 = add(%20, %13)\n  %22 = nn.relu(%21)\n  %23 = nn.conv2d(%22, meta[relay.Constant][25], strides=[2, 2], padding=[1, 1], kernel_size=[3, 3])\n  %24 = nn.batch_norm(%23, meta[relay.Constant][26], meta[relay.Constant][27], meta[relay.Constant][28], meta[relay.Constant][29], epsilon=1e-05)\n  %25 = %24.0\n  %26 = nn.relu(%25)\n  %27 = nn.conv2d(%26, meta[relay.Constant][30], padding=[1, 1], kernel_size=[3, 3])\n  %28 = nn.batch_norm(%27, meta[relay.Constant][31], meta[relay.Constant][32], meta[relay.Constant][33], meta[relay.Constant][34], epsilon=1e-05)\n  %29 = %28.0\n  %30 = nn.conv2d(%22, meta[relay.Constant][35], strides=[2, 2], kernel_size=[1, 1])\n  %31 = nn.batch_norm(%30, meta[relay.Constant][36], meta[relay.Constant][37], meta[relay.Constant][38], meta[relay.Constant][39], epsilon=1e-05)\n  %32 = %31.0\n  %33 = add(%29, %32)\n  %34 = nn.relu(%33)\n  %35 = nn.conv2d(%34, meta[relay.Constant][40], padding=[1, 1], kernel_size=[3, 3])\n  %36 = nn.batch_norm(%35, meta[relay.Constant][41], meta[relay.Constant][42], meta[relay.Constant][43], meta[relay.Constant][44], epsilon=1e-05)\n  %37 = %36.0\n  %38 = nn.relu(%37)\n  %39 = nn.conv2d(%38, meta[relay.Constant][45], padding=[1, 1], kernel_size=[3, 3])\n  %40 = nn.batch_norm(%39, meta[relay.Constant][46], meta[relay.Constant][47], meta[relay.Constant][48], meta[relay.Constant][49], epsilon=1e-05)\n  %41 = %40.0\n  %42 = add(%41, %34)\n  %43 = nn.relu(%42)\n  %44 = nn.conv2d(%43, meta[relay.Constant][50], strides=[2, 2], padding=[1, 1], kernel_size=[3, 3])\n  %45 = nn.batch_norm(%44, meta[relay.Constant][51], meta[relay.Constant][52], meta[relay.Constant][53], meta[relay.Constant][54], epsilon=1e-05)\n  %46 = %45.0\n  %47 = nn.relu(%46)\n  %48 = nn.conv2d(%47, meta[relay.Constant][55], padding=[1, 1], kernel_size=[3, 3])\n  %49 = nn.batch_norm(%48, meta[relay.Constant][56], meta[relay.Constant][57], meta[relay.Constant][58], meta[relay.Constant][59], epsilon=1e-05)\n  %50 = %49.0\n  %51 = nn.conv2d(%43, meta[relay.Constant][60], strides=[2, 2], kernel_size=[1, 1])\n  %52 = nn.batch_norm(%51, meta[relay.Constant][61], meta[relay.Constant][62], meta[relay.Constant][63], meta[relay.Constant][64], epsilon=1e-05)\n  %53 = %52.0\n  %54 = add(%50, %53)\n  %55 = nn.relu(%54)\n  %56 = nn.conv2d(%55, meta[relay.Constant][65], padding=[1, 1], kernel_size=[3, 3])\n  %57 = nn.batch_norm(%56, meta[relay.Constant][66], meta[relay.Constant][67], meta[relay.Constant][68], meta[relay.Constant][69], epsilon=1e-05)\n  %58 = %57.0\n  %59 = nn.relu(%58)\n  %60 = nn.conv2d(%59, meta[relay.Constant][70], padding=[1, 1], kernel_size=[3, 3])\n  %61 = nn.batch_norm(%60, meta[relay.Constant][71], meta[relay.Constant][72], meta[relay.Constant][73], meta[relay.Constant][74], epsilon=1e-05)\n  %62 = %61.0\n  %63 = add(%62, %55)\n  %64 = nn.relu(%63)\n  %65 = nn.conv2d(%64, meta[relay.Constant][75], strides=[2, 2], padding=[1, 1], kernel_size=[3, 3])\n  %66 = nn.batch_norm(%65, meta[relay.Constant][76], meta[relay.Constant][77], meta[relay.Constant][78], meta[relay.Constant][79], epsilon=1e-05)\n  %67 = %66.0\n  %68 = nn.relu(%67)\n  %69 = nn.conv2d(%68, meta[relay.Constant][80], padding=[1, 1], kernel_size=[3, 3])\n  %70 = nn.batch_norm(%69, meta[relay.Constant][81], meta[relay.Constant][82], meta[relay.Constant][83], meta[relay.Constant][84], epsilon=1e-05)\n  %71 = %70.0\n  %72 = nn.conv2d(%64, meta[relay.Constant][85], strides=[2, 2], kernel_size=[1, 1])\n  %73 = nn.batch_norm(%72, meta[relay.Constant][86], meta[relay.Constant][87], meta[relay.Constant][88], meta[relay.Constant][89], epsilon=1e-05)\n  %74 = %73.0\n  %75 = add(%71, %74)\n  %76 = nn.relu(%75)\n  %77 = nn.conv2d(%76, meta[relay.Constant][90], padding=[1, 1], kernel_size=[3, 3])\n  %78 = nn.batch_norm(%77, meta[relay.Constant][91], meta[relay.Constant][92], meta[relay.Constant][93], meta[relay.Constant][94], epsilon=1e-05)\n  %79 = %78.0\n  %80 = nn.relu(%79)\n  %81 = nn.conv2d(%80, meta[relay.Constant][95], padding=[1, 1], kernel_size=[3, 3])\n  %82 = nn.batch_norm(%81, meta[relay.Constant][96], meta[relay.Constant][97], meta[relay.Constant][98], meta[relay.Constant][99], epsilon=1e-05)\n  %83 = %82.0\n  %84 = add(%83, %76)\n  %85 = nn.relu(%84)\n  %86 = nn.global_avg_pool2d(%85)\n  %87 = shape_of(%86, dtype=\"int32\")\n  %88 = take(%87, int64(0), axis=0)\n  %89 = expand_dims(%88, axis=0)\n  %90 = expand_dims(int64(-1), axis=0)\n  %91 = (%89, %90)\n  concatenate(%91)\u001b[31man internal invariant was violated while typechecking your program [17:22:35] /Users/jroesch/Git/tvm/src/relay/op/tensor/transform.cc:204: Check failed: e_dtype == dtype (int64 vs. int32) : relay.concatenate requires all tensors have the same dtype\n; \u001b[39m\n}\n// meta data omitted. you can use show_meta_data=True to include meta data\n"
     ]
    }
   ],
   "source": [
    "onnx_resnet18 = onnx.load('resnet.onnx')\n",
    "func, params = relay.frontend.from_onnx(onnx_resnet18, shape={ '0': (10, 3, 224, 224) })\n",
    "print(func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Format\n",
    "\n",
    "Relay has a textual representation that can be used to write and print programs. The textual format is still being stablized but can still be of great use today. For example, instead of providing inscrutable graph representations of programs, we can produce human readable output by default.\n",
    "\n",
    "There are a few different ways to interact with the textual format. The first is to just print out a Relay expression as we have seen above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp, params = relay.testing.mlp.get_workload(1)\n",
    "print(mlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default the pretty printer renders the code without metadata. The pretty printer omits metadata because the metadata, which contains information such as constants, is often unreadable. For instance, imagine we perform an optimization, such as inlining the parameters into the program for further optimization. The metadata would include 100s of megabytes of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inline_parameters(expr, params):\n",
    "    param_map = dict((p.name_hint, p) for p in expr.params)\n",
    "    params = dict((param_map[k], relay.const(params[k])) for k in params)\n",
    "    new_body = relay.bind(expr.body, params)\n",
    "    return relay.Function(relay.ir_pass.free_vars(new_body), new_body)\n",
    "\n",
    "inline_mlp = inline_parameters(mlp, params)\n",
    "print(inline_mlp.astext(show_meta_data=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relay's pretty printer also allows users to attach debugging output and metadata to the IR. For example you can see the type information on the example above, but we can also customize the annotation process by passing a callback for annotating nodes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0 \n",
    "def ann(*args):\n",
    "    global i\n",
    "    i += 1\n",
    "    return f\" <expression: {i}>\"\n",
    "\n",
    "print(mlp.astext(show_meta_data=True, annotate=ann))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, an important part of the Relay text format is the ability to load Relay code \n",
    "like a normal programming language. We can use the Relay parser to parse code. We actually do this to define the Relay\n",
    "*prelude* the small standard library of utilities shipped in Relay. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Executing Relay\n",
    "\n",
    "Now that we have looked at how to write and manipulate a Relay program, we will show you how to run one. Relay has multiple execution mechanisms: a custom *debug interpreter* for Relay which can be used for experimentation and debugging, TVM's older graph runtime, the existing execution mechanism, and Relay VM. Relay VM is a newly designed execution mechanism with the goal to smoothly execute all of Relay efficiently. \n",
    "\n",
    "We provide a high level interface which imposes some wrapping overhead but enables quick experimentation with each API. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = relay.Module()\n",
    "debug_ex = relay.create_executor('debug', mod=mod)\n",
    "graph_ex = relay.create_executor('graph', mod=mod)\n",
    "vm_ex = relay.create_executor('vm', mod=mod)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each executor can be used to evaluate an expression given a Relay module, in this case we use an empty module, and will just evaluate the same expression, a MLP, using each one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_mlp = debug_ex.evaluate(mlp)\n",
    "graph_mlp = graph_ex.evaluate(mlp)\n",
    "vm_mlp = vm_ex.evaluate(mlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each one can be called like a normal Python function with the inputs passed as positional arguments and the parameters as keyword arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = numpy.random.rand(1, 1, 28, 28).astype('float32')\n",
    "print(\"Debug: \", debug_mlp(data, **params))\n",
    "print(\"Graph: \", graph_mlp(data, **params))\n",
    "print(\"VM: \", vm_mlp(data, **params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Virtual Machine\n",
    "\n",
    "The Relay virtual machine is worth highlighting, it is a brand new runtime mechanism for Relay which is beginning to stablize. We encourage new users to check it out and provide feedback on its design, perforomance, and more. The VM enables support for non-standard aspects of Relay including control-flow, closures, and data structures.\n",
    "\n",
    "For more details on the virtual machine see [here](https://github.com/dmlc/tvm/issues/2810)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pass Manager\n",
    "\n",
    "Relay has a flexible and configurable pass manager with an elegant API that be used to easily compose and schedule pass pipelines. We believe an easy-to-configure pipeline is important to enable intelligent exploration between a variety of \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet, params = relay.testing.resnet.get_workload()\n",
    "\n",
    "seq = _transform.Sequential([\n",
    "    relay.transform.InferType(),\n",
    "    relay.transform.FoldConstant(),\n",
    "    relay.transform.EliminateCommonSubexpr(),\n",
    "    relay.transform.AlterOpLayout()\n",
    "])\n",
    "\n",
    "mod = relay.Module({\"main\": before()})\n",
    "with relay.build_config(opt_level=3):\n",
    "    with tvm.target.create(\"llvm\"):\n",
    "        mod = seq(mod)\n",
    "\n",
    "    zz = mod[\"main\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizations\n",
    "\n",
    "Defining optimizations to transform your program is straight forward and easy to do in Relay.\n",
    "\n",
    "For example let's define a constant evaluator for Relay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantization\n",
    "\n",
    "Relay supports an automatic quantization framework which can be used to implement a variety of quantization schemes. You can find more details about it in our recent paper as well as it being used in action here.\n",
    "\n",
    "Below we will apply the default quantization scheme to ResNet-18."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet, params = relay.testing.resnet.get_workload()\n",
    "\n",
    "seq = _transform.Sequential([\n",
    "    relay.transform.InferType(),\n",
    "    relay.transform.FoldConstant(),\n",
    "    relay.transform.EliminateCommonSubexpr(),\n",
    "    relay.transform.AlterOpLayout()\n",
    "])\n",
    "\n",
    "mod = relay.Module({\"main\": before()})\n",
    "with relay.build_config(opt_level=3):\n",
    "    with tvm.target.create(\"llvm\"):\n",
    "        mod = seq(mod)\n",
    "    \n",
    "    zz = mod[\"main\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "puVpH87g0Nui"
   },
   "source": [
    "## Heterogeneous Execution\n",
    "\n",
    "Relay supports a high-level interface for scheduling computation across multiple heterogeneous devices. An interesting property of this pass is that it is not special. It is built using Relay's standard machinery for\n",
    "passes. \n",
    "\n",
    "We implement this by using an annotation to mark which computations we would like to schedule on each device, \n",
    "and a pass inserts all the appropriate calls to synchronize memory across devices. \n",
    "\n",
    "The below pass uses this machinery to schedule all convolutions onto the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 6276
    },
    "colab_type": "code",
    "id": "5sfd3Svu0Nui",
    "outputId": "bf451b82-ece9-429e-d2cf-395e1ee1b026"
   },
   "outputs": [],
   "source": [
    "class ScheduleConv2d(ExprMutator):\n",
    "    def __init__(self, device):\n",
    "        self.device = device\n",
    "        super().__init__()\n",
    "\n",
    "    def visit_call(self, expr):\n",
    "        visit = super().visit_call(expr)\n",
    "        if expr.op == tvm.relay.op.get(\"nn.conv2d\"):\n",
    "            return relay.annotation.on_device(visit, self.device)\n",
    "        else:\n",
    "            return visit\n",
    "\n",
    "def schedule_conv2d_on_gpu(expr):\n",
    "    sched = ScheduleConv2d(tvm.gpu(0))\n",
    "    return sched.visit(expr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can grab a model, we provide a few basic models in Relay's testing library. By default when printing a model we will see it rendered in Relay's textual format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can grab a model, we provide a few basic models in Relay's testing library.\n",
    "resnet, params = relay.testing.resnet.get_workload()\n",
    "print(resnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now run the customized pass we defined above to schedule individual convolutions on the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet = schedule_conv2d_on_gpu(resnet)\n",
    "print(resnet)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can later rewrite away the device annotations to insert the copies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet = relay.ir_pass.rewrite_annotated_ops(resnet, 0)\n",
    "print(resnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we will look at a couple case studies of what can be built using Relay. We will first look at how Relay is used as a backend in PyTorch integration, then how Relay can be used to compile a model down to traditional hardware, and finally how it can be used to support a custom accelerator, VTA, which we will dicuss in detail today."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ahead of time compilation\n",
    "\n",
    "An example of what can be built using Relay can be found with the [ahead of time compiler](https://github.com/uwsampl/relay-aot). \n",
    "\n",
    "We have already installed the compiler above, we can simply import it and use it below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmlp = aot.compile(mlp, relay.Module({}))\n",
    "print(cmlp(input, **params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch Integration\n",
    "\n",
    "Recently Facebook engineers have begun to integrate TVM into PyTorch. The are using PyTorch's JIT functionality to generate Relay code which is then optimized and deployed. This support is currently being integrated into mainline TVM, and installed from [here](https://github.com/pytorch/tvm). \n",
    "\n",
    "Initial results are promising and a recent writeup on the design can be found on our [blog](https://tvm.ai/2019/05/30/pytorch-frontend). \n",
    "\n",
    "![results](https://i.imgur.com/KfJ7oas.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def add(a, b, c):\n",
    "    return a + b + c\n",
    "\n",
    "# via tracing\n",
    "relay_graph = torch_tvm.to_relay(add, inputs)\n",
    "\n",
    "@torch.jit.script\n",
    "def mul(a, b, c):\n",
    "    return a * b * c\n",
    "\n",
    "# via script\n",
    "relay_graph = torch_tvm.to_relay(mul, inputs)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UPK5HmW30Nun"
   },
   "source": [
    "## VTA\n",
    "TODO TALK WITH THIERRY"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "02_TVM_Tutorial_Relay.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
